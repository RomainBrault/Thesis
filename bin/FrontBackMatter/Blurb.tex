%!TEX root = ../../ThesisRomainbrault.tex
% Blurb

\pagestyle{empty}

\pdfbookmark[-1]{Blurb}{blurb}

\setlength{\fboxsep}{5mm}
\areaset%
{\textwidth}% calculate requiered \textwidth
{\dimexpr\the\paperheight-0.cm\relax}% calculate requiered \textheight

\begin{textblock}{1} (1.5, 0.5)
    \includegraphics[scale=.75]{../gfx/STIC.png} %% Logo de Paris Saclay
\end{textblock}

\vspace*{\fill}

\begin{textblock}{11.5} (2, 1.5)
    \footnotesize
    \noindent\color{PSaclay}\fbox{%
    \parbox{\linewidth-2\fboxrule-2\fboxsep}{\normalcolor%
    \begin{center}
        \textsc{\mySubtitle}
    \end{center}
    \medskip
    \emph{Keywords:} Operator-Valued Kernels, Large Scale Learning, Random
    Fourier Features
    \begin{multicols}{2}
        \emph{Abstract:} Many problems in Machine Learning can be cast into
        vector-valued functions approximation. \emph{\acl{OVK}s} and
        vector-valued Reproducing Kernel Hilbert Spaces provide a theoretical
        and practical framework to address that issue, extending nicely the
        well-known setting of scalar-valued kernels.  However large scale
        applications are usually not affordable with these tools that require
        an important computational power along with a large memory capacity. In
        this thesis, we propose and study scalable methods to perform
        regression with \emph{\acl{OVK}s}. To achieve this goal, we extend
        Random Fourier Features, an approximation technique originally
        introduced for scalar-valued kernels, to \emph{\acl{OVK}s}. The idea is
        to take advantage of an approximated operator-valued feature map in
        order to come up with a linear model in a finite-dimensional space.
        \paragraph{}
        This thesis is structured as follows. First we develop a
        general framework devoted to the approximation of shift-invariant
        Mercer kernels on Locally Compact Abelian groups and study their
        properties along with the complexity of the algorithms based on them.
        Second we show theoretical guarantees by bounding the error due to the
        approximation, with high probability. Third, we study various
        applications of Operator Random Fourier Features (\acs{ORFF}) to
        different tasks of Machine learning such as multi-class classification,
        multi-task learning, time serie modeling, functional regression and
        anomaly detection. We also compare the proposed framework with other
        state of the art methods. Fourth, we conclude by drawing short-term and
        mid-term perspectives of this work.
    \end{multicols}}}
    \medskip
    \noindent\color{PSaclay}\fbox{%
    \parbox{\linewidth-2\fboxrule-2\fboxsep}{\normalcolor%
    \begin{center}
        \textsc{R\'egression \`A Noyaux \`A Valeurs Op\'erateurs Pour Grands
        Ensembles De Donn\'ees}
    \end{center}
    \vspace*{\fill}
    \emph{Mots clefs:} Noyaux \`a Valeurs Op\'erateurs, Passage \`a l'\'echelle,
    Random Fourier Features
    \begin{multicols}{2}
        \emph{R\'esum\'e:} De nombreuses probl\'ematiques d'ap\-pren\-tis\-sage
        artificiel peuvent \^etres mod\'elis\'es gr\^ace \`a des fonctions \`a
        valeurs vectorielles. Les \emph{noyaux \`a valeurs op\'erateurs} et
        leur espace de Hilbert \`a noyaux reproduisant \`a valeurs vectorielles
        associ\'es donnent un cadre th\'eorique et pratique pour apprendre de
        telles fonctions, \'etendant la litt\'erature existante des noyaux
        scalaires. Cependant, lorsque les donn\'ees sont nombreuses, ces
        m\'ethodes sont peu utilisables, ne passant pas \`a l'\'echelle, car
        elle n\'ecessite une quantit\'e de m\'emoire \'evoluant
        quadratiquement et un temps de calcul \'evoluant cubiquement vis \`a
        vis du nombre de donn\'ees, dans leur impl\'ementation la plus na\"\i
        ve. Afin de faire passer les \emph{noyaux \`a valeurs op\'erateurs} \`a
        l'\'echelle, nous \'etendons une technique d'approximation stochastique
        introduite dans le cadres des noyaux scalaires. L'id\'ee est de tirer
        parti d'une fonction de redescription caract\'etisant le \emph{noyau
        \`a valeurs op\'erateurs}, dont les fonctions associ\'ees vivent dans
        un espace de dimension infinie, afin d'obtenir un probl\`eme
        d'optimisation lin\'eaire de dimension finie.
        \paragraph{}
        Dans cette th\`ese nous d\'eveloppons dans un premier temps un cadre
        g\'en\'eral afin de permettre l'approximation de noyaux de Mercer
        d\'efinits sur des groupes commutatifs localement compacts et
        \'etudions leurs propri\'et\'es ainsi que la complexit\'e des
        algorithmes en d\'ecoulant. Dans un second temps nous montrons des
        garanties th\'eoriques en bornant l'erreur commise par l'approximation,
        avec grande probabilit\'e. Enfin, nous mettons en \'evidence plusieurs
        applications des Repr\'esentations Op\'erateurs Al\'eatoires de Fourier
        (\acs{ORFF}) telles que la classification multiple, l'apprentissage
        multi-t\^ache, la mod\'elisation de s\'eries temporelles, la
        r\'egression fonctionnelle et la d\'etection d'anomalies. Nous
        comparons \'egalement ce cadre avec d'autres m\'ethodes de la
        litt\'erature et concluons par des perspectives \`a moyen et long
        terme.
    \end{multicols}}}
\end{textblock}

\vspace*{\fill}

\begin{textblock}{13} (13.25,13.75)
    \includegraphics[height=2cm]{../gfx/logoEgrey.png}
\end{textblock}
