%!TEX root = ../../ThesisRomainBrault.tex

%------------------------------------------------------------------------------
\section{Generalization bound}
\label{sec:generalization_bound}
In this section, we are interested in finding a function
$f_*:\mathcal{X}\to\mathcal{Y}$, where $\mathcal{X}$ is a Polish space and
$\mathcal{Y}$ a separable Hilbert space such that for all $x_i$ in
$\mathcal{X}$ and all $y_i$ in $\mathcal{Y}$ that minimizes a criterion. In
statistical supervised learning, we consider a training set sequence
$\seq{s}=(x_i,y_i)_{i=1}^N\in(\mathcal{X}\times\mathcal{Y})^N$,
$N\in\mathbb{N^*}$ drawn \acs{iid}~from an unknown probability law
$\probability$. We suppose we are given a cost function
$c:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$, such that $c(f(x),y)$ returns
the error of the prediction $f(x)$ \acs{wrt}~the ground truth $y$. We define
the true risk the sum of the cost over all possible training examples drawn
from a latent probability law $\probability$,
\begin{dmath*}
    \risk{f}=\int_{\mathcal{X}\times\mathcal{Y}}L(x,f,y)d\probability(x,y)
    \hiderel{=}\int_{\mathcal{X}\times\mathcal{Y}}c(f(x), y)d\probability(x,y)
\end{dmath*}
Thus given a class of functions $\mathcal{F}$, the goal of a learning algorithm
is to find an optimal model $f_*$ that minimizes the true risk. Namely
\begin{dmath*}
    f_* = \argmin_{f\in\mathcal{F}} \risk{f}
    \hiderel{=}\argmin_{f\in\mathcal{F}}\int_{\mathcal{X} \times
    \mathcal{Y}}c(f(x), y)d\probability(x,y).
\end{dmath*}
Since in practice we do not have access to the joint probability law of (X,Y), we define its empirical counterpart as the empirical
mean estimate, where the sequence $\seq{s}=(x_i,y_i)_{i=1}^N$ is made of
$\mathcal{X}$-valued random vectors drawn \acs{iid}~from some law
$\probability$.
% Moreover we often add a regularization term on the norm of the functions in
% $\mathcal{F}$ to \say{reduce} the size of the class of function.
The empirical risk then reads
\begin{dmath*}
    \riskemp{f, \seq{s}}=\frac{1}{N}\sum_{i=1}^Nc(f(x_i),y_i) \condition{$(x_i
    , y_i)\sim \probability$ \acs{iid}.}
\end{dmath*}
As a result, in practice we seek a function $f_{\seq{s}}$ such that
\begin{dmath}
    \label{eq:argmin_empirical}
    f_{\seq{s}} = \argmin_{f\in\mathcal{F}} \riskemp{f,\seq{s}} \hiderel{=}
    \argmin_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^Nc\left(f(x_i), y_i\right).
\end{dmath}
The basic requirement for any learning algorithm is the generalization property: the empir-
ical error must be a good proxy of the expected error, that is the difference
between the two must be \say{small} when $N$ is large. A generalization
bound allows to study, for any $f\in\mathcal{F}$ the difference between its true risk $\risk{f}$
and its empirical risk, $\risk{f,\seq{s}}$. This quantifies the impact of having a limited
number of observations. Generalization (upper) bounds \cite{Vapnik1998} involve two components: one being the empirical risk and the other depends on the dataset size as well as some capacity notion that reflects the richness of the family of functions $\mathcal{F}$ considered. First generalization bounds proved by Vapnik and Chervonenkis involve the dimension of Vapnik-Chervonenkis dimension of $\mathcal{F}$.
In practise, generalization bounds are usually not enough tight to provide direct recommendations for choosing the appropriate family of functions to work with when $N$ is fixed.
However as the bias-variance dilemna, it suggests that when learning a function from a finite dataset,  it is necessary to control the richness of the functions family $\mathcal{F}$. 
In practise, 
a regularizer is added to the data-fitting term in order to maintain the solution $f_{\seq{s}}$ of
\cref{eq:argmin_empirical} in a ball of $\mathcal{F}$.  As a result if
$\mathcal{F}$ is a Banach space, it is common to find $f_{\seq{s}}$ such that
\begin{dmath*}
    f_{\seq{s}} = \argmin_{f\in\mathcal{F}} \riskemp{f,\seq{s}} +
    \lambda\norm{f}_{\mathcal{F}}^2.
\end{dmath*}
(Tychonov regularization) or
\begin{dmath*}
    f_{\seq{s}} = 
    \begin{cases}
        \argmin_{f\in\mathcal{F}} & \riskemp{f,\seq{s}} \\
        \text{subject to} & \norm{f}_{\mathcal{F}} < C \in \mathbb{R}_{>0}
    \end{cases}
\end{dmath*}
(Ivanov regularization) or 
\begin{dmath*}
    f_{\seq{s}} = 
    \begin{cases}
        \argmin_{f\in\mathcal{F}} & \riskemp{f,\seq{s}} \\
        \text{subject to} & \norm{f}_{\infty} < C \in \mathbb{R}_{>0}.
    \end{cases}
\end{dmath*}
\subsection{Generalization by bounding the function space complexity}
In the following we
consider functions living in a \acl{vv-RKHS}, with kernel $K$ (or
$\tildeK{\omega}$).
\begin{proposition}[\citet{bartlett2002rademacher, maurer2016vector}]
    \label{pr:generalization_rademacher}
    Suppose that $f\in\mathcal{H}_K$ a \acs{vv-RKHS} where
    \begin{dmath*}
        \sup_{x\in\mathcal{X}} \Tr[K(x,x)] < T
    \end{dmath*}
    and $\norm{f}_{\mathcal{H}_K}<B$. Moreover let $c:\mathcal{Y}\to[0, C]$ be
    a $L$-Lipschitz cost function and $\mathcal{Y}$ a separable Hilbert space.
    Then if we are given $N$ \acs{iid} random variables with values in
    $\mathcal{X}$ (training samples, noted $\seq{s}$), then we have with at
    least probability $1-\delta$, $\delta\in(0, 1)$ over the drawn training
    samples $\seq{s}$ that for any $f\in\mathcal{H}_K$,
    \begin{dmath}
        \risk{f} \le \riskemp{f, \seq{s}}  + 2\sqrt{\frac{2}{N}}\left(
        LBT^{1/2} + C\sqrt{\ln(2/\delta)}\right).
    \end{dmath}
    \label{pr:ovk_gen}
\end{proposition}
\begin{proof}
    This proof is due to \citet{maurer2016vector} generalizing the work of
    \citet[section 4.3]{bartlett2002rademacher}: we do not claim any
    originality for this proof. First let us introduce the notion of Rademacher
    complexity of a class of function $\mathcal{F}$. We recall that the
    probability mass function of a uniformly distributed Rademacher random
    variable is given for any $k\in\Set{-1,1}$ by
    \begin{dmath*}
        f(k)=
        \begin{cases}
            1/2 & \text{if $k=-1$} \\
            1/2 & \text{otherwise.}
        \end{cases}
    \end{dmath*}
    \begin{definition}[\citet{bartlett2002rademacher}]
        Let $\mathcal{X}$ be any set. Let $\epsilon_1,\hdots,\epsilon_N$ be $N$
        independent Rademacher random variables, identically uniformly
        distributed on $\{-1;1\}$. For any class of functions
        $F:~\mathcal{X}\to\mathbb{R}$, then for all $x_1, \hdots x_N\in
        \mathcal{X}$ the quantity
        \begin{dmath*}
            \mathcal{R}_N(F) \colonequals \expectation\left[ \sup_{f\in
            \mathcal{F}} \sum_{i=1}^N \epsilon_i f(x_i) \;\middle|\; x_1,
            \dots, x_N \right]
        \end{dmath*}
        is called Rademacher complexity of the class $\mathcal{F}$.
    \end{definition}
    In a few words the Rademacher complexity measures the richness of a class a
    function by its capacity to be correlated to noise. In generalization
    bounds, the Rademacher complexity of a class of function often involves a
    composition between a target function to be learn and a cost function, part
    of the risk we want to minimize. The idea is to bound the Rademacher
    complexity with a term that does not depends on the cost function, but only
    on the target function.
    \begin{proposition}[\citet{maurer2016vector}]
        \label{pr:radswap}
        Let $\mathcal{X}$ be any set, $x_1, \hdots, x_N$ in $\mathcal{X}$, let
        $\mathcal{F}$ be a class of function $f:~\mathcal{X}\to\mathcal{Y}$ and for i=1, \ldots, N, 
        each function $h_i:~\mathcal{Y}\to\mathbb{R}$ be a $L$-Lipschitz function, where
        $\mathcal{Y}$ is a separable Hilbert space endowed with
        euclidean inner product. Then
        \begin{dmath*}
            \expectation\left[ \sup_{f\in \mathcal{F}} \sum_{i=1}^N \epsilon_i
            h_i(f(x_i)) \;\middle|\; x_1, \dots, x_N \right] \le
            \sqrt{2}L\expectation\left[\sup_{f\in F}\sum_{i=1,k}^{i=N}
            \epsilon_{ik}f_k(x_i) \;\middle|\; x_1, \dots, x_N \right],
        \end{dmath*}
        where $\epsilon_{ik}$ is a doubly indexed independent Rademacher
        sequence and $f_k(x_i)$ is the $k$-th component of $f(x_i)$.
    \end{proposition}
    From now on, we consider functions $f\in\mathcal{H}_K$ a \acl{vv-RKHS}. Then
    there exists an induced feature-map $\Phi:~\mathcal{X}\to
    \mathcal{L}(\mathcal{Y}, \mathcal{H})$ such that for all $y,
    y'\in\mathcal{Y}$ the kernel is given by
    \begin{dmath*}
        \inner{y, K(x,z)y'} = \inner{\Phi_xy, \Phi_zy'}.
    \end{dmath*}
    We say that the feature space $\mathcal{H}$ is embedded into the RKHS
    $\mathcal{H}_K$ by means of the \emph{feature operator}
    $(W\theta)(x)\colonequals(\Phi_x^*\theta)$. Indeed $W$ defines a partial
    isometry between $\mathcal{H}$ and $\mathcal{H}_K$. Suppose that
    $\mathcal{Y}$ a is separable Hilbert space and let the class of
    $\mathcal{Y}$-valued functions $F$ be
    \begin{dmath*}
        \mathcal{F}=\Set{f | f:~x \mapsto (W \theta)(x), \enskip
        \norm{\theta}_{\mathcal{H}} < B } \hiderel{\subset} \mathcal{H}_K.
    \end{dmath*}
    Let $c_{y_i}=c(\cdot - y_i)$, for all $in\in\mathbb{N}_N$.  Then from
    \cref{pr:radswap} and if $K$ is trace class, we have
    \begin{dmath*}
        \expectation \sup_{\norm{\theta}_{\mathcal{H}}<B} \sum_{i=1}^N
        \epsilon_i c_{y_i}(\Phi_{x_i}^\adjoint \theta) 
        \le \sqrt{2}L\expectation \sup_{\norm{\theta}_{\mathcal{H}}<B}
        \sum_{i=1,k}^{i=N}\epsilon_{ik}\inner{\Phi_{x_i}^\adjoint \theta, e_k}
        = \sqrt{2}L\expectation \sup_{\norm{\theta}_{\mathcal{H}}<B}
        \inner*{\theta,
        \sum_{i=1,k}^{i=N}\epsilon_{ik}\Phi_{x_i}e_k}_{\mathcal{Y}}.
    \end{dmath*}
    Thus
    \begin{dmath}
        \label{eq:ker_bound}
        \expectation \sup_{\norm{\theta}_{\mathcal{H}}<B} \sum_{i=1}^N
        \epsilon_i c_{y_i}(\Phi_{x_i}^\adjoint \theta) 
        \le \sqrt{2}LB\expectation
        \norm{\sum_{i=1,k}^{i=N}\epsilon_{ik}\Phi_{x_i}e_k}_{\mathcal{Y}}
        \le \sqrt{2}LB\sqrt{\sum_{i=1,k}^{i=N}
        \norm{\Phi_{x_i}e_k}_{\mathcal{Y}}^2 }
        \le \sqrt{2}LB\sqrt{\sum_{i=1}^{N} \Tr\left[ K(x_i, x_i) \right] }
        \le \sqrt{2}LB\sqrt{N}\sqrt{\sup_{x\in\mathcal{X}}\Tr\left[ K(x,
        x)\right]}.
    \end{dmath}
    From \citet{maurer2016vector, bartlett2002rademacher},
    \begin{theorem}
        \label{th:gen_rad_bound} 
        Let $\mathcal{X}$ be any set, $\mathcal{F}$ a class of functions
        $f:~\mathcal{X}\to[0, C]$ and let $X_1, \hdots, X_N$ be a sequence of
        \acs{iid} random variables with value in $\mathcal{X}$. Then for
        $\delta \in (0, 1)$, with probability at least $1-\delta$, we have for
        all $f\in \mathcal{F}$ that
        \begin{equation}
            \expectation f(X) \le \frac{1}{N} \sum_{i=1}^Nf(X_i) +
            \frac{2}{N}\mathcal{R}_N(\mathcal{F}) +
            C\sqrt{\frac{8\ln(2/\delta)}{N}}
        \end{equation}
    \end{theorem}
    Conclude by pluging \cref{eq:ker_bound} in \cref{th:gen_rad_bound}.
\end{proof}
As an example, let us consider \cref{eq:learning_rkhs}, which is a solution of
the regularized empirical risk, and $\cref{alg:close_form}$ when $D\to\infty$.
We first list the following assumptions useful in the rest of the section. Let
$\seq{s}=(x_i, y_i)_{i=1}^N\in \mathcal{X}^N\times \mathcal{Y}^N$ be the
training samples.
\begin{assumption}\label{ass:bounded_norm}
    There exists a positive constant $\kappa\in\mathbb{R}_{\ge 0}$ such that
    \begin{dmath*}
        \max_{i\in\mathbb{N}^*_N} \norm{K(x_i,x_i)}_{\mathcal{Y}, \mathcal{Y}}
        < \kappa.
    \end{dmath*}
\end{assumption}

\begin{assumption}\label{ass:bounded_trace}
    There exists a positive constant $T\in\mathbb{R}_{\ge 0}$ such that
    \begin{dmath*}
        \max_{i\in\mathbb{N}^*_N} \Tr\left[ K(x_i,x_i)\right] < T.
    \end{dmath*}
\end{assumption}
\begin{assumption}\label{ass:bounded_outputs}
    There exists a positive constant $C\in\mathbb{R}_{\ge 0}$ such that
    \begin{dmath*}
        \max_{i\in\mathbb{N}^*_N}\norm{y_i}_{\mathcal{Y}} \le C.
    \end{dmath*}
\end{assumption}
\begin{assumption}\label{ass:bounded_loss}
    Given a Loss function $L$, there exists a positive constant $\xi
    \in\mathbb{R}_{\ge 0}$ such that for all $x\in\mathcal{X}$, for all
    $y\in\mathcal{Y}$ and for any $\seq{s}\in\mathcal{X}^N\times\mathcal{Y}^N$,
    \begin{dmath*}
        L(x, f_{\seq{s}}, y) \le \xi.
    \end{dmath*}
\end{assumption}
Under \cref{ass:bounded_outputs}, from \cref{rk:rkhs_bound}, we know that
$\norm{f}_{\mathcal{H}_K}\le \sqrt{\frac{2}{\lambda}}\sigma_y$, where
\begin{dmath*}
    \frac{1}{N}\sum_{i=1}^N\norm{y_i}_{\mathcal{Y}}^2 \le \sigma_y^2
    \hiderel{\le} C^2.
\end{dmath*}
Thus we straight away see that it is possible to choose
$B=\sqrt{\frac{2}{\lambda}} C$. Let $\kappa = \norm{K_e(e)}_{\mathcal{Y},
\mathcal{Y}}$ The Lipschitz constant of the least square loss
$c(f_{\seq{s}}(x), y)=\frac{1}{2}\norm{f_{\seq{s}}(x) - y}_{\mathcal{Y}}^2$
with respect to $f_{\seq{s}}(x)$ is
$L=\max\left(\sqrt{\frac{2\kappa}{\lambda}}C, C\right)$ and the loss takes
values in $\left[0, \frac{1}{2}L^2\right]$. Hence under assumption that
$\lambda < 2\kappa$ and \cref{ass:bounded_trace}, and
\cref{ass:bounded_outputs}, \cref{pr:ovk_gen} applies especially that  for any
$f_{\seq{s}}\in\mathcal{H}_K$, solution of \cref{alg:close_form},
\begin{dmath}
    \risk{f_{\seq{s}}} \le \riskemp{f_{\seq{s}}, \seq{s}}  +
    8\frac{C^2}{\lambda}\sqrt{\frac{\kappa}{N}}\left( T^{1/2} +
    \sqrt{\frac{\kappa\ln(1/\delta)}{2}}\right).
\end{dmath}
This bound is to be compared to the results of \citet{kadri2015operator} in the context of
$\beta$-stability. 
\subsection{Algorithm stability}
\label{sec:algorithm_stability}
The approach to generalization bounds presented in
\cref{pr:generalization_rademacher} is based on controlling the complexity of
the hypothesis space\mpar{Other methods using covering numbers
\citep{zhou2002covering, tewari1learning} or \acs{VC-dimension}
\citep{vapnik1992principles} have also been used as a proxy on the complexity
of the hypothesis space.} using Ra\-de\-ma\-cher complexity. On the other hand,
the idea of stability is that a reliable algorithm should not change its
solution too much if we modify slightly the training data. Given a training
sequence 
\begin{dmath*}
    \seq{s}=\left((x_1, y_1), \dots (x_N, y_N)\right) \hiderel{\in}
    (\mathcal{X}\times\mathcal{Y})^N,
\end{dmath*}
we note $\seq{s}^{\setminus i}$ the training sequence 
\begin{dmath*}
    \seq{s}^{\setminus i}=\left((x_1, y_1), \dots, (x_{i-1}, y_{i-1}), \dots,
    (x_{i+1}, y_{i+1}) (x_N, y_N)\right) \hiderel{\in}
    (\mathcal{X}\times\mathcal{Y})^N,
\end{dmath*} 
the subsequence of $\seq{s}$ from which we removed the $i$-th element.
\begin{definition}[Uniform stability {\citet[definition
6]{bousquet2002stability}}]
A learning algorithm $\seq{s}\mapsto f_{\seq{s}}$ has uniform stability $\beta$
with respect to the loss function $L$ if the following holds
\begin{dmath*}
    \forall i \in \mathbb{N}_N^*, \forall
    \seq{s}\hiderel{\in}\left(\mathcal{X}\times\mathcal{Y}\right)^N
    \sup_{x\in\mathcal{X}, y\in\mathcal{Y}} \abs{L(x, f_{\seq{s}}, y) - L(x,
    f_{\seq{s}^{\setminus i}}, y)} \hiderel{\le} \beta.
\end{dmath*}
\end{definition}
As shown by \citet{bousquet2002stability}, algorithm stability has direct link
with generalization. Indeed if an algorithm has $\beta$-stability, and a
\say{bounded} loss for all $x\in\mathcal{X}$ and $y\in\mathcal{Y}$
(\cref{ass:bounded_loss}), it is possible to exhibit a generalization bound.
\begin{theorem}[{\citet[theorem 12]{bousquet2002stability}}]
    Let $\seq{s}\mapsto f_{\seq{s}}$ be a learning algorithm with uniform
    stability $\beta$ with respect to a loss $L$ that satisfies
    \cref{ass:bounded_loss}. Then $\forall N\in\mathbb{N}^*$, $\forall \delta
    \in (0, 1)$, the following bound holds with probability at least $1-\delta$
    over the \acs{iid} drawn training smaples $\seq{s}$.
    \begin{dmath*}
        \risk{f_{\seq{S}}} \le \riskemp{f_{\seq{s}}, \seq{s}} + 2\beta +
        (4N\beta + \xi) \sqrt{\frac{\ln(1/\delta)}{2N}}.
    \end{dmath*}
\end{theorem}
In their original paper on learning function-valued output data,
\citet{kadri2015operator} showed that under \cref{ass:bounded_norm},
\cref{ass:bounded_outputs}, and provided that $K$ is weakly measurable, the
algorithm is $\beta$-stable with $\beta=$. Moreover \cref{ass:bounded_loss}
holds with $\xi$, where $\sigma=$.  Thus another generalization bound for
\cref{alg:close_form} is
\begin{dmath}
    \risk{f_{\seq{s}}} \le \riskemp{f_{\seq{s}}, \seq{s}}  + \frac{\kappa^2 C^2
    \left(1 + \frac{\kappa}{\sqrt{\lambda}} \right)^2 }{\lambda N} + C^2\left(
    1 + \frac{\kappa}{\sqrt{\lambda}} \right)^2\left( \frac{4\kappa^2}{\lambda}
    + 1 \right)\sqrt{\frac{\ln(1/\delta)}{2N}}.
\end{dmath}

%------------------------------------------------------------------------------
\section{Consistency of learning with ORFF}
In this section we are interested by measuring how  $\risk{\widetilde{f}_{\seq{s}}}$ is close to the smallest true risk achieved in the function class $\mathcal{F}$.
The quantity of interest is:
\begin{dmath*}
    \risk{\widetilde{f}_{\seq{s}}} - \min_{f\in\mathcal{F}} \risk{f}.
\end{dmath*}
In other words, we quantify the difference between risk of the optimal solution
belonging to a given class of function $\mathcal{F}$, and the risk given a
solution $f_{\seq{s}}$ returned by some learning algorithm.  Here to derive a consistency
result, we study an algorithm slightly different from \cref{alg:close_form}.
Given a loss function
$L:\mathcal{X}\times\mathcal{F}\times\mathcal{Y}\to\mathbb{R}_+$ and its
canonical cost function $c(f(x),y)\colonequals L(x, f, y)$ such that $c$ is
Lipschitz in its first argument. We consider learning with an \acs{ORFF}
$\tildePhi{\omega}(x):\mathcal{Y} \to \Vect_{j=1}^D \mathcal{Y}'$ thanks to the
algorithm
\begin{dmath}
    \label{eq:alg_generalization}
    \theta_{\seq{s}} = 
    \begin{cases}
        \argmin_{\theta\in\Vect_{j=1}^D\mathcal{Y}'} & \frac{1}{N} \sum_{i=1}^N
        c(\tildePhi{\omega}(x)^\adjoint \theta) \\
        \text{subject to} & \max_{j\in\mathbb{N}_D^*}
        \norm{\theta_j}_{\mathcal{Y}} \le \frac{B}{D}.
    \end{cases}
\end{dmath}
Then the associated output function return is
$\widetilde{f}_{\seq{s}}=\tildePhi{\omega}(\cdot)^\adjoint \theta_{\seq{s}}$.
We suppose that the operator $A(\omega)$ used in the construction of
$\tildePhi{\omega}(x)$ has bounded trace $\probability_{\rho,
\dual{\Haar}}$-almost everywhere.
\begin{proposition}
    \label{pr:consistency_algorithm_generalization}
    Let $\Phi_x=\pairing{x, \cdot}B(\cdot)$ be a Fourier feature such that
    there exists a constant $T\in\mathbb{R}_+$ such that 
    \begin{dmath*}
        \esssup_{\omega\in\dual{\mathcal{X}}} \Tr\left[A(\omega)\right] < T
    \end{dmath*}
    and a constant $u\in\mathbb{R}_+$ such that
    \begin{dmath*}
        \esssup_{\omega\in\dual{\mathcal{X}}} \norm{A(\omega)}^2_{\mathcal{Y},
        \mathcal{Y}} < u.
    \end{dmath*}
    where $A(\omega)=B(\omega)B(\omega)^\adjoint$.  Let $\rho$ be the density
    of a probability distribution with respect to the Haar measure $\Haar$ and
    define the set
    \begin{dmath*}
        \mathcal{F} = \Set{f | f: x\mapsto \int_{\dual{\mathcal{X}}}
        \Phi_x(\omega)\theta(\omega)^\adjoint d\dual{\Haar}(\omega), \enskip
        \norm{\theta(\omega)}_{\mathcal{Y}} < B\rho(\omega)}
        \hiderel{\subseteq} \mathcal{H}_K.
    \end{dmath*}
    Eventually let $c:\mathcal{Y}^2 \to [0, C]$ be a cost function
    $L$-Lipschitz in its first argument. Then for any $\delta\in(0, 1)$, given
    a training sequence $\seq{s}=(x_i, y_i)\in(\mathcal{X}\times\mathcal{Y})^N$
    drawn \acs{iid}, if $\widetilde{f}_{\seq{s}}$ is given by
    \cref{eq:alg_generalization} then we have
    \begin{dmath*}
        \risk{\widetilde{f}_{\seq{s}}} - \min_{f\in\mathcal{F}} \risk{f} \le
        \underbrace{4\sqrt{\frac{2}{N}}\left( LBT^{1/2} +
        C\sqrt{\ln(2/\delta)}\right)}_{\text{Estimation error.}} +
        \underbrace{\frac{uLB}{\sqrt{D}}\left(1 + \sqrt{2\ln(1/\delta)}
        \right)}_{\text{Approximation error.}}.
    \end{dmath*}
    with probability $1-2\delta$ over the training sequence and the random
    vectors $(\omega_j)_{j=1}^D$.
\end{proposition}
\begin{proof}
    We follow the proof idea of \citet{rahimi2009weighted} in the scalar case and adapt it to the vector-valued case
    in the light of the results of \citet{maurer2016vector}. We first define 
    the two following sets.
    \begin{dmath*}
        \mathcal{F} = \Set{f | f: x\mapsto \int_{\dual{\mathcal{X}}}
        \Phi_x(\omega)^\adjoint\theta(\omega)d\dual{\Haar}(\omega), \enskip
        \norm{\theta(\omega)}_{\mathcal{Y}} < B\rho(\omega)}
    \end{dmath*}
    and
    \begin{dmath*}
        \widetilde{\mathcal{F}} = \Set{f | f: x\mapsto \sum_{j=1}^D
        \Phi_x(\omega_j)^\adjoint \theta_j, \enskip \forall j\in
        \mathbb{N}_D^*, \norm{\theta_j}_{\mathcal{Y}} < \frac{B}{D} }.
    \end{dmath*}
    \begin{proposition}[Existence of an approximate function]
        \label{pr:existence_approx}
        Let $\mu$ be a measure on $\mathcal{X}$, and $f_*$ a function in
        $\mathcal{F}$. Moreover let $\esssup_{\omega\in\dual{\mathcal{X}}}
        \norm{B(\omega)}_{\mathcal{Y}, \mathcal{Y}}^2 \le u$. If
        $(\omega_j)_{j=1}^D$ are drawn \acs{iid} from a probability
        distribution of density $\rho$ \acs{wrt} $\dual{\Haar}$, then for any
        $\delta\in(0, 1)$, with probability at least $1-\delta$ over
        $(\omega_j)_{j=1}^D$, there exists a function $\widetilde{f}$ in
        $\widetilde{\mathcal{F}}$ such that
        \begin{dmath*}
            \sqrt{\int_{\mathcal{X}} \norm{\widetilde{f}(x) -
            f_*(x)}_{\mathcal{Y}}^2d\mu(x)} \le \frac{uB}{\sqrt{D}}\left( 1 +
            \sqrt{2\ln(1/\delta)} \right).
        \end{dmath*}
    \end{proposition}
    \begin{proof}
        Since $f_*\in\mathcal{F}$, we can write
        $f_*(x)=\int_{\mathcal{X}}\conj{\pairing{x,
        \omega}}B(\omega)\theta(\omega)d\dual{\Haar}(\omega)$. Construct the
        functions $f_j=\pairing{\cdot, \omega_j}B(\omega_j)\beta_j$ with
        $\beta_k\colonequals \frac{\theta(\omega_j)}{\rho(\omega)}$, so that
        $\expectation_{\rho, \dual{\Haar}} f_j = f_*$ pointwise. Let
        \begin{dmath*}
            \widetilde{f}(x) = \sum_{j=1}^D\Phi_x(\omega_j)^\adjoint
            \frac{\beta_j}{D}
        \end{dmath*}
        be the sample average of these functions. Then,
        $\widetilde{f}\in\widetilde{\mathcal{F}}$ because
        $\norm{\beta_j}_{\mathcal{Y}} / D < B / D$.  Also, under the inner
        product $\int_{\mathcal{X}} \inner{f(x), g(x)}_{\mathcal{Y}} d\mu(x)$,
        we have that
        \begin{dmath*}
            \norm{\conj{\pairing{\cdot,
            \omega_j}}B(\omega_j)\beta_j}_{L^2(\mathcal{X},\mu;\mathcal{Y})} \le
            \esssup_{\omega\in\dual{\mathcal{X}}}
            \norm{B(\omega)B(\omega)^\adjoint}^2_{\mathcal{Y},\mathcal{Y}} B.
        \end{dmath*}
        We introduce the following technical lemma of
        \citet{rahimi2009weighted} for concentration of random variable in
        Hilbert spaces (similar to \citet{pinelis1994optimum}).
        \begin{lemma}
            \label{lm:concentration_hilbert}
            Let $X_1, \dots, X_D$ be \acs{iid} random variables with values in
            a ball of radius $M$ centered at the origin in a Hilbert space
            $\mathcal{H}$. Denote the sample average $\bar{X} =
            \frac{1}{D}\sum_{j=1}^D X_j$. Then for any $\delta\in(0, 1)$ with
            probability $1 - \delta$,
            \begin{dmath*}
                \norm{\expectation\bar{X} - \bar{X}}_{\mathcal{Y}} \le
                \frac{M}{\sqrt{D}}\left(1+\sqrt{2\ln(1/\delta)}\right).
            \end{dmath*}
        \end{lemma}
        Eventually apply \cref{lm:concentration_hilbert} to $f_1, \dots, f_D$
        under the canonical inner product of the vector valued function space
        $L^2(\mathcal{X}, \mu; \mathcal{Y})$ to conclude the proof.
    \end{proof}
    \begin{proposition}[Bound on the approximation error]
        \label{pr:approximation_bound}
        Let $L(x, f, y)$ be a loss function and $c_{y}(f(x))=L(x, f, y)$ be
        a $L$-Lipschitz cost function for all $y\in\mathcal{Y}$. Let $f_*$ be a
        function in $\mathcal{F}$. Suppose there exists a constant
        $u\in\mathbb{R}_+$ such that
        \begin{dmath*}
            \esssup_{\omega\in\dual{\mathcal{X}}}
            \norm{A(\omega)}^2_{\mathcal{Y}, \mathcal{Y}} \le u.
        \end{dmath*}
        If $(\omega_j)_{j=1}^D$ are \acs{iid} random variables
        drawn from a probability distribution of density $\rho$, then for any
        $\delta\in(0, 1)$ there exits, with probability $1-\delta$ over
        $(\omega_j)_{j=1}^D$, a function $\widetilde{f} \in
        \widetilde{\mathcal{F}}$ such that
        \begin{dmath*}
            \risk{\widetilde{f}} \le \risk{f_*} + \frac{uLB}{\sqrt{D}}\left(1 +
            \sqrt{2\ln(1/\delta)} \right).
        \end{dmath*}
    \end{proposition}
    \begin{proof}
        Given any functions $f$ and $g$ in $\mathcal{F}$, the Lipschitz
        hypothesis on $c_{y_i}$ followed by the concavity of the square root
        (Jensen's inequality) gives
        \begin{dmath*}
            \risk{f} - \risk{g} = \expectation_{\mu} c_y(f(x)) - c_y(g(x))
            \le \expectation_{\mu} \abs{c_y(f(x)) - c_y(g(x))}
            \le L\expectation_{\mu} \norm{f(x) - g(x)}_{\mathcal{Y}}
            \le L \sqrt{\expectation_{\mu} \norm{f(x) - g(x)}^2_{\mathcal{Y}}}.
        \end{dmath*}
    \end{proof}
    apply \cref{pr:existence_approx} to conclude.
    \begin{proposition}[Bound on the estimation error]
        \label{pr:estimation_bound}
        Let $c_y:\mathcal{Y} \to [0; C]$ be a $L$-Lipschitz cost function
        for all $y\in\mathcal{Y}$. Let $(\omega_j)_{j=1}^D$ be $D$ fixed
        vectors in $\dual{\mathcal{X}}$. If $\seq{s}=(x_i,
        y_i)_{i=1}^N\in(\mathcal{X}\times\mathcal{Y})^N$ are \acs{iid} random
        variables then for all $\delta\in(0, 1)$, then it holds with
        probability $1-\delta$ for all $\widetilde{f} \in
        \widetilde{\mathcal{F}}$ that
        \begin{dmath*}
            \risk{\widetilde{f}} \le \riskemp{\widetilde{f}, \seq{s}} +
            2\sqrt{\frac{2}{N}}\left( LBT^{1/2} + C\sqrt{\ln(2/\delta)}\right).
        \end{dmath*}
        where $\Tr\left[ \widetilde{K}_e(e) \right] < T\in\mathbb{R}_+$.
    \end{proposition}
    \begin{proof}
        Since $\widetilde{f}\in\widetilde{\mathcal{F}}$ and forall
        $j\in\mathbb{N}^*_D$, $\norm{\theta_j}_{\mathcal{Y}} < B/D$, Thus
        $\norm{\theta}_{\Vect_{j=1}^D\mathcal{Y}} < B/\sqrt{D}$. Moreover, if
        we define $\tildePhi{\omega}(x)=\Vect_{j=1}^D\Phi_x(\omega_j)$, it
        gives birth to a \acs{RKHS} with kernel $D \Phi_x^\adjoint \Phi_z$ for
        all $x$, $z\in\mathcal{X}$. Thus with arguments similar to
        \cref{eq:ker_bound}, noticing that the terms in $\sqrt{D}$ cancels out,
        we obtain a bound on the Rademacher complexity
        \begin{dmath*}
            \mathcal{R}_N\left(\widetilde{\mathcal{F}}\right) \le
            \sqrt{2}BL\sqrt{N\Tr\left[ \widetilde{K}_e(e) \right]}.
        \end{dmath*}
        Eventually apply \cref{th:gen_rad_bound}.
    \end{proof}
    We are now ready to prove the main claim. Let $f_*$ be a minimizer of
    $\mathcal{R}$ over $\mathcal{F}$, $\widetilde{f}$ a minimizer of
    $\mathcal{R}_{\text{emp}}$ over $\widetilde{\mathcal{F}}$ and
    $\widetilde{f}_*$ a minimizer of $\mathcal{R}$ over
    $\widetilde{\mathcal{F}}$. Then
    \begin{dmath}
        \label{eq:risk_expansion}
        \risk{\widetilde{f}} - \risk{f_*} = \risk{\widetilde{f}} -
        \risk{\widetilde{f}_*} + \risk{\widetilde{f}_*} - \risk{f_*}.
    \end{dmath}
    The first difference in the right hand side of the equation is The
    estimation error. By \cref{pr:estimation_bound}, with probability
    $1-\delta$, $\risk{\widetilde{f}_*} - \riskemp{\widetilde{f}_*}\le
    \epsilon_{est}$ and simultaneously, $\risk{\widetilde{f}} -
    \riskemp{\widetilde{f}} \le \epsilon_{est}$. By optimality of
    $\widetilde{f}$, $\riskemp{\widetilde{f}} \le \risk{\widetilde{f}_*}$.
    Combining these facts, with probability $1 - \delta$,
    \begin{dmath*}
        \risk{\widetilde{f}} - \risk{\widetilde{f}_*} \le
        4\sqrt{\frac{2}{N}}\left( LBT^{1/2} + C\sqrt{\ln(2/\delta)}\right)
        \hiderel{=} 2\epsilon_{est}.
    \end{dmath*}
    Applying \cref{pr:approximation_bound} yields
    \begin{dmath*}
        \risk{\widetilde{f}_*} - \risk{f_*} \le \frac{uLB}{\sqrt{D}}\left(1 +
        \sqrt{2\ln(1/\delta)} \right) \hiderel{=} \epsilon_{app}.
    \end{dmath*}
    Conclude by the union bound with probability $1 - 2\delta$
    \cref{eq:risk_expansion} is bounded by above by $2\epsilon_{est} +
    \epsilon_{app}$. Notice that $\Tr\left[ \widetilde{K}_e(e)
    \right]=\frac{1}{D}\sum_{j=1}^D A(\omega_j)$. Thus if we have
    $\esssup_{x\in\dual{\mathcal{X}}} \Tr\left[A(\omega)\right] < \infty$,
    $\Tr\left[ \widetilde{K}_e(e) \right]\le\esssup_{x\in\dual{\mathcal{X}}}
    \Tr\left[ A(\omega) \right]$.
\end{proof}

\section{Discussion}
In this chapter we reviewed two ways of obtaining generalization bounds (see
\cref{sec:generalization_bound,sec:algorithm_stability}) for
\acsp{OVK} by bounding the function class complexity (\citet{maurer2016vector})
or using algorithm stability arguments (\citet{kadri2015operator}). Then we
used the results of \citet{maurer2016vector} to prove the consistency of the
algorithm obtained by minimizing \cref{eq:alg_generalization}, which is a
variant of \cref{alg:close_form}, where we replace the Tychonov regularizer by
a projection in a $\norm{\cdot}_{\infty}$ ball. This bound generalizes the work
of \citet{rahimi2009weighted} to vector-valued learning.
\paragraph{}
Notice that we cannot directly derive a consistency bound from
\cref{pr:consistency_algorithm_generalization} to \cref{alg:close_form}. Indeed
with arguments similar to \cref{rk:rkhs_bound}, we can show that
$\widetilde{f}_{\seq{s}}=\tildePhi{\omega}(x)^\adjoint \theta$ has a parameter
vector $\theta$ such that $\norm{\theta_j}_{\mathcal{Y}} <
\sqrt{\frac{2}{\lambda D}} \sigma_y$, where $\sigma_y^2=\frac{1}{N}\sum_{i=1}^N
\norm{y_i}^2_{\mathcal{Y}}$. Thus if $\widetilde{f}_{\seq{s}}$ is a solution of
\cref{alg:close_form}, we do not have
$\widetilde{f}_{\seq{s}}\in\widetilde{\mathcal{F}}$. \acs{ie} the Tichonov
regularization is not \say{powerfull} enough to guarantee that
$\widetilde{f}_{\seq{s}}$ belongs to $\widetilde{\mathcal{F}}$. One could argue
that we could choose $\lambda = O(\sqrt{D})$ to obtain consistency with
Tychonov regularization, however this makes little sense since in this case if
$D\to\infty$ then $\lambda \to \infty$ the \cref{alg:close_form} will always
return $\widetilde{f}_{\seq{s}}=0$. 
\paragraph{}
While the bound in \cref{pr:consistency_algorithm_generalization} shows the
consistency of learning with \acs{ORFF} it still has low and possibly
suboptimal rate. Moreover it does not allow to derive a number of features $D$
smaller than the number of data since both of them decrease the error in
$O(D^{-1/2})$ (respectively $O(N^{-1/2})$) as in the reference bound for
scalar-valued random features by \citet{rahimi2009weighted}. In the
scalar-valued kernel litterature, recent work of \citet{bach2015equivalence}
with much more involved analysis, gives simlar results to
\citet{rahimi2009weighted} in the case of Tichonov regularization.  Moreover it
suggests that the number of features $D$ to guarantee an error below some
constant is linked to the decrease rate of the eigenvalues of the Mercer
decomposition of scalar-valued kernel $k$. If the eigenvalues decrease in
$O(m^{-2s})$ then the error is in $O\left(\log(D)^s D^{-s}\right)$. Lastly the
new results of \citet{rudi2016generalization} shows that for scalar-valued
kernels, the kernel ridge regression algorithm (which is \cref{alg:close_form}
with $A = 1$) generalizes optimality with a number of features $D=O(\sqrt{N})$.
Thus the time complexity required for optimal generalization with \acsp{RFF} in
the case of kernel ridge regression is $O(ND^2)=O(N^{2})$ and the space
complexity is in $O(N^{1.5})$, if the random features are all stored and not
computed in an online fashion\mpar{See \cref{subsec:complexity}.}.

\chapterend
