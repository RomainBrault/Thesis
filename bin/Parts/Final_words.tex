%!TEX root = ../ThesisRomainbrault.tex

\chapter{Work in progress}
\label{ch:Perspectives}
\bigskip
\begin{justify}
    To conlude our work we present some work in progress. We show practical
    applications of operator-valued kernels acting on an infinite dimensional
    space $\mathcal{Y}$. We give two examples. Firsti we show how to generalize
    many quantile regression to learn a continuous function of the quantiles on
    the data. Second we apply the same methodology to the one-class SVM
    algorithm in order to learn a continuous function of all the level sets. We
    conclude by presenting Operalib, a python library developped during this
    thesis which aims at implementing \acs{OVK}-based algorithms in the spirit
    of Scikit-learn \citep{pedregosa2011scikit}.
\end{justify}
\minitoc
\include{Parts/Final_words/Perspectives}



\chapter{Conclusion}
\bigskip
\begin{justify}
    In machine learning, many algorithm focus on learning functions that model
    dependencies in a dataset, where the the ouputs are real numbers. However
    in numerous applications such as biology, economics, physics, etc.~the
    output data are not reals: they can be a collection of reals, present
    complex structures or can even be functions. To overcome this difficulty
    and take into account the structure of the data, a common approach is to
    see them as \emph{vectors} of some Hilbert space. From this observation, in
    this thesis, we took interrest in vector-valued functions. Looking at the
    litterature we focused on mathematical objects called \aclp{OVK} to learn
    such functions.
    \section{Contributions}
    \acsp{OVK} naturally extend the celebrated kernel method used to learn
    scalar-valued functions, to the case of learning vector-valued functions.
    Yet, although \acsp{OVK} are appealing from a theoretical aspect, these
    methods scale poorly in terms of computation time when the number of data
    is high. Indeed, to evaluate the value of function with an \acl{OVK}, it
    requires to evaluate an \acl{OVK} on all the point in the given dataset.
    Hence naive learning with kernels usually scales cubicly in time with the
    number of data. In the context of large-scale learning such scaling is not
    acceptable. Through this work we propose a methodology to tackle this
    difficulty.
    \paragraph{}
    Enlighted by the litterature on large-scale learning with
    \emph{scalar}-valued kernel, in particular the work of Rahimi and Recht
    \citep{Rahimi2007}, we propose to replace an \acs{OVK} by a random feature
    map that we called \acl{ORFF}. Our contribution start with the formal
    mathematical construction of this feature from an \acs{OVK}. Then we show
    that it is also possible to obtain a kernel from an \acs{ORFF}. Eventually
    we analyse the regularization properties in terms of \acl{FT} of
    $\mathcal{Y}$-Mercer kernels. Then we moved on giving a bound on the error
    due to the random approximation of the \acs{OVK} with high probability.
    We showed that it is possible to bound the error eventhough the \acs{ORFF}
    estimator of an \acs{OVK} is not a bounded random variable. Moreover we
    also give a bound when the dimension of the ouput data infinite.
    \paragraph{}
    After ensuring that an \acs{ORFF} is a good approximation of a kernel, we
    moved on giving a framework to learn with \aclp{OVK}. We showed that
    learning with a feature map is equivalent to learn with the reconstructed
    \acs{OVK} under some mild conditions. Then we focused on a efficient
    implementation of \acs{ORFF} by viewing them as linear operator rather than
    matrices and using matrix-free (iterative) solvers and conclued with some
    numerical experiments. Eventually we gave a generalization bound for
    \acs{ORFF} learning that suggest that the number of features sampled in an
    \acs{ORFF} should be proportional to the number of data. We concluded our
    contribution by applying the \acs{ORFF} framework to learning vector-valued
    time series.
    \section{Perspectives}
\end{justify}
\label{ch:conclusion}
\include{Parts/Final_words/Conclusions}
