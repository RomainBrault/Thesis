%----------------------------------------------------------------------------------------
\section{Motivations}
\label{sec:motivations}
Random Fourier Features have been proved useful to implement efficiently kernel methods in the scalar case, allowing to learn a linear model based on an approximated feature map. In this work, we are interested to construct approximated operator-valued feature maps to learn vector-valued functions. With an explicit (approximated) feature map, one converts the problem of learning a function $f$ in the vector-valued Reproducing Kernel Hilbert Space $\mathcal{H}_K$ into the learning of a linear model $\tilde{f}$ defined by:
 \begin{dmath*}
 \tilde{f}(x) = \tildePhi{1:D}(x)^* \theta,
 \end{dmath*}
 where $\Phi: \mathcal{X} \to \mathcal{L}(\mathcal{H},\mathcal{Y})$ and $\theta \in \mathcal{H}$. The methodology we propose works for operator-valued kernels defined on any \acf{LCA} group, noted ($\mathcal{X}, \groupop)$, for some operation noted $\groupop$. This allows us to use the general context of Pontryagin duality for \acl{FT} of functions on \acs{LCA} groups. Building upon a generalization of Bochner's theorem for operator-valued measures, an operator-valued kernel is seen as the \emph{\acl{FT}} of an operator-valued positive measure. From that result, we extend the principle of Random Fourier Feature for scalar-valued kernels and derive a general methodology to build Operator Random Fourier Feature when operator-valued kernels are shift-invariant according to the chosen group operation.

%----------------------------------------------------------------------------------------
\section{Construction}
\label{sec:construction}
We present a construction of \acf{ORFF} such that $f: x\mapsto \tildePhi{1:D}(x)^*\theta$ is a continuous function that maps an arbitrary \acs{LCA} group $\mathcal{X}$ as input space to an arbitrary output Hilbert space $\mathcal{Y}$. First we define a functional \emph{Fourier feature map}, and then propose a Monte-Carlo sampling from this feature map to construct an approximation of a shift-invariant $\mathcal{Y}$-Mercer kernel.
Then, we prove the convergence of the kernel approximation $\tilde{K}(x,z)=\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)$ with high probability on \emph{compact} subsets of the \acs{LCA} $\mathcal{X}$, when $\mathcal{Y}$ is \emph{finite dimensional}. Eventually we conclude with some numerical experiments.
\subsection{Theoretical study}
The following proposition of \citet{Zhang2012,Carmeli2010} extends Bochner's theorem to any shift-invariant $\mathcal{Y}$-Mercer kernel. 
\begin{proposition}[Operator-valued Bochner's theorem \citep{Zhang2012}]\label{eq:bochner-gen}
If a continuous function $K$ from $\mathcal{X} \times \mathcal{X}$ to $\mathcal{Y}$ is a shift-invariant $\mathcal{Y}$-Mercer kernel on $\mathcal{X}$, then there exists a unique positive operator-valued measure $M: \mathcal{B}(\mathcal{X}) \to \mathcal{L}_+(\mathcal{Y})$ such that for all $x$, $z \in \mathcal{X}$,
\begin{dmath}
K(x, z) = \int_{\dual{\mathcal{X}}} \conj{\pairing{x \groupop \inv{z}, \omega}} dM(\omega),
\end{dmath}
where $M$ belongs to the set of all the $\mathcal{L}_+(\mathcal{Y})$-valued measures of bounded variation on the $\sigma$-algebra of Borel subsets of $\dual{\mathcal{X}}$. Conversely, from any positive operator-valued measure $M$, a shift-invariant kernel $K$ can be defined by \cref{eq:bochner-gen}. 
\end{proposition}
Although this theorem is central to the spectral decomposition of shift-invariant $\mathcal{Y}$-Mercer \acs{OVK}, the following results proved by \citet{Carmeli2010} provides insights about this decomposition that are more relevant in practice. It first gives the necessary conditions to build shift-invariant $\mathcal{Y}$-Mercer kernel with  a pair $(A, \mu)$ where $A$ is an operator-valued function on $\dual{\mathcal{X}}$ and $\mu$ is a real-valued positive measure on $\dual{\mathcal{X}}$. Note that obviously such a pair is not unique ad the choice of this paper may have an impact on theoretical properties as well as practical computations.
Secondly it also states that any \acs{OVK} have such a spectral decomposition when $\mathcal{Y}$ is finite dimensional or $\mathcal{X}$.

\begin{proposition}[\citet{Carmeli2010}]\label{pr:mercer_kernel_bochner}
Let $\mu$ be a positive measure on $\mathcal{B}(\mathcal{\dual{\mathcal{X}}})$ and $A: \dual{\mathcal{X}}\to \mathcal{L}(\mathcal{Y})$ such that $\inner{A(.)y,y'}\in L^1(\mathcal{X},d\mu)$ for all $y,y'\in\mathcal{Y}$ and $A(\omega)\succcurlyeq 0$ for $\mu$-almost all $\omega$. Then, for all $\delta \in \mathcal{X}$ and for all $y, y' \in \mathcal{Y}$,
\begin{dmath}
\label{eq:AK0}
K_e(\delta)=\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta,\omega}}A(\omega)d\mu(\omega)
\end{dmath}
is the kernel signature of a shift-invariant $\mathcal{Y}$-Mercer kernel $K$ such that $K(x,z)=K_e(x \groupop \inv{z})$. The \acs{vv-RKHS} $\mathcal{H}_K$ is embed in $L^2(\dual{\mathcal{X}},d\mu;\mathcal{Y}')$ by mean of the feature operator
\begin{dmath}
\label{eq:feature_operator}
(Wg)(x)=\int_{\mathcal{\dual{X}}}\conj{\pairing{x,\omega}}B(\omega)g(\omega)d\mu(\omega),
\end{dmath}
Where $B(\omega)B(\omega)^\adjoint=A(\omega)$ and both integral converges in the weak sense. If $\mathcal{Y}$ is finite dimensional or $\mathcal{X}$ is compact, any shift-invariant kernel is of the above form for some pair $(A, d\mu)$. 
\end{proposition}
\paragraph{}
When $p=1$ one can always assume $A$ is reduced to the scalar $1$, $\mu$ is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (\cref{th:bochner-scalar}).
\paragraph{}
\Cref{pr:mercer_kernel_bochner} shows that a given pair $(A,d\mu)$ characterize an \acs{OVK}. Namely given a measure $d\mu$ and a function $A$ such that $\inner{A(.)y,y'}\in L^1(\mathcal{X},d\mu)$ for all $y,y'\in\mathcal{Y}$ and $A(\omega)\succcurlyeq 0$ for $\mu$-almost all $\omega$, it gives rise to an \acs{OVK}. Since $(A,d\mu)$ determine a unique kernel we can write $\mathcal{H}_{(A,d\mu)}{\scriptstyle\implies}\mathcal{H}_K$ where $K$ is defined as in \cref{eq:AK0}. However the converse is to true: Given a $\mathcal{Y}$-Mercer shift invariant \acl{OVK}, there exist infinitely many pairs $(A,d\mu)$ that characterize an \acs{OVK}.
\paragraph{}
The main difference between \cref{eq:bochner-gen} and \cref{pr:mercer_kernel_bochner} is that the first one characterize an \acs{OVK} by a unique \acf{POVM}, while the second one shows that the \acs{POVM} that uniquely characterize a $\mathcal{Y}$-Mercer \acs{OVK} has an operator-valued density with respect to a \emph{scalar} measure $\mu$; and that this operator-valued density is not unique.
\paragraph{}
Finally \cref{pr:mercer_kernel_bochner} does not provide any \emph{constructive} way to obtain the pair $(A,d\mu)$ that characterize an \acs{OVK}.
The following \cref{subsec:sufficient_conditions} is based on an other proposition of \citeauthor{carmeli2006vector} and show that if the kernel signature $K_e(\delta)$ of an $\acs{OVK}$ is in $L^1$ then it is possible to construct \emph{explicitly} a pair $(C,d\omega)$ from it. We show that we can always extract a scalar-valued \emph{probability} density function from $C$ such that we obtain a pair $(A,d\mu)$ where $\mu$ is a probability measure.

\subsection{Sufficient conditions of existence}
\label{subsec:sufficient_conditions}
While \cref{pr:mercer_kernel_bochner} gives some insights on how to build an approximation of a $\mathcal{Y}$-Mercer kernel, we need a theorem that provides an explicit construction of the pair $A(\omega), \mu(\omega)$ from the kernel signature. Proposition 14 in \citet{Carmeli2010} gives the solution, and also provide a sufficient condition for \cref{pr:mercer_kernel_bochner} to apply.
\begin{proposition}[\citet{Carmeli2010}]
\label{pr:inverse_ovk_Fourier_decomposition}
Let $K$ be a shift-invariant $\mathcal{Y}$-Mercer kernel. %
Suppose that $\forall z \in \mathcal{X}$ and $\forall y ,y' \in\mathcal{Y}$, $\inner{K_e(.)y,y'}\in L^1(\mathcal{X},dx)$ where $dx$ denotes the Haar measure on $\mathcal{X}$ endowed with the group law $\groupop$. %
Define $C$ such that for all $\omega \in \dual{\mathcal{X}}$ and for all $y$, $y'$ in $\mathcal{Y}$,
\begin{dmath}\label{eq:CK0}
\inner{y,C(\omega)y'} = \int_{\mathcal{X}} \pairing{\delta, \omega}\inner{y, K_e(\delta)y'}d\delta = \IFT{\inner{y, K_e(\cdot)y'}}(\omega)
\end{dmath}
Then
\begin{propenum}
\item $C(\omega)$ is a bounded non-negative operator for all $\omega \in \dual{\mathcal{X}}$,
\item $\inner{y, C(.)y'}\in L^1(\dual{\mathcal{X}},d\omega)$ for all $y,y'\in\mathcal{X}$,
\item for all $\delta\in\mathcal{X}$ and for all $y$, $y'$ in $\mathcal{Y}$,
\begin{dmath*}
\inner{y, K_e(\delta)y'}= \int_{\dual{\mathcal{X}}}\conj{\pairing{\delta,\omega}}\inner{y, C(\omega)y'}d\omega
=\FT{\inner{y, C(\cdot)y'}}(\delta).
\end{dmath*}
\end{propenum}
\end{proposition}
The following proposition allows to build a spectral decomposition of a shift-invariant $\mathcal{Y}$-Mercer kernel on a \acs{LCA} group $\mathcal{X}$ endowed with the group law $\groupop$ with respect to a scalar probability measure, by extracting a scalar probability density from $C$.
\paragraph{}
There have been a lot of confusion in the literature whether a kernel is the \acl{FT} or \acl{IFT} of a measure. However \cref{lm:fourier_shift_invariant} clarify the relation between the \acl{FT} and \acl{IFT} for a translation invariant \acl{OVK}. Notice that in the real scalar case the \acl{FT} and \acl{IFT} of a shift-invariant kernel are the same, while the difference is significant for \acs{OVK}.
\paragraph{}
The following lemma is a direct consequence of the definition of $C(\omega)$ as the \acl{FT} of the adjoint of $K_e$ and also helps simplifying the definition of \acs{ORFF}.
\begin{lemma}
\label{lm:C_characterization}
Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer kernel and let $\inner{y, C(\cdot)y'}=\IFT{\inner{y, K_e(\cdot)y'}}$ for all $y$. $y'\in\mathcal{Y}$. Then 
\begin{propenum}
\item \label{lm:C_characterization_1} $C(\omega)$ is self-adjoint and $C$ is even.
\item \label{lm:C_characterization_2} $\IFT{\inner{y, K_e(\cdot)y'}} = \FT{\inner{y, K_e(\cdot)y'}}$.
\item \label{lm:C_characterization_3} $K_e(\delta)$ is self-adjoint and $K_e$ is even.
\end{propenum}
\end{lemma}
\begin{proof}
For any function $f\in L^1(\mathcal{X},dx)$ define the flip operator $\mathcal{R}$ by
\begin{dmath*}
\mathcal{R}f(x) \colonequals f(\inv{x}).
\end{dmath*}
for any shift invariant $\mathcal{Y}$-Mercer kernel, we have for all $\delta\in\mathcal{X}$,  $K_e(\delta)=K_e(\inv{\delta})^\adjoint$. Indeed,
\begin{dmath*}
\mathcal{R}\inner{y,K_e(x\groupop \inv{z})y'}\hiderel{=}\inner{y,K_e(\inv{(x\groupop \inv{z})})y'}\hiderel{=}\inner{y,K_e(\inv{x}\groupop z)y'}=\inner{y,K_e(x\groupop \inv{z})^\adjoint y'}.
\end{dmath*}
\Cref{lm:C_characterization_1}: taking the Fourier transform yields,
\begin{dmath*}
\IFT{\inner{y, K_e(\cdot)y'}}=\mathcal{F}^{-1}\mathcal{R}\left[\inner{K_e(\cdot)y, y'}\right]
\hiderel{=}\mathcal{R}\inner{C(\cdot)y, y'}.
\end{dmath*}
Hence $C(\omega)=C(\inv{\omega})^\adjoint$.
\paragraph{}
Suppose that $\mathcal{Y}$ is a complex Hilbert space. Since for all $\omega\in\mathcal{\dual{X}}$ $C(\omega)$ is bounded and non-negative so $C(\omega)$ is self-adjoint. Besides we have $C(\omega)=C(\inv{\omega})^*$ to $C$ must be pair.
\paragraph{}
Suppose that $\mathcal{Y}$ is a real Hilbert space. Then we have the additional hypothesis that $K_e(\delta)=K_e(\delta)^\adjoint$. Taking the \acl{FT} yields that $C(\omega)=C(\omega)^\adjoint$. Since for any shift invariant $\mathcal{Y}$-Mercer kernel $C(\omega)=C(\inv{\omega})^\adjoint$ we also conclude that $C(\inv{\omega})=C(\omega)$.
\paragraph{}
\Cref{lm:C_characterization_2}: simply, for all $y$. $y'\in\mathcal{Y}$, $\inner{y, C(\inv{\omega})y'}=\inner{y, C(\omega)y'}$ thus $\IFT{\inner{y, C(\cdot)y'}}=\mathcal{F}\mathcal{R}\left[\inner{y, C(\cdot)y'}\right]=\FT{\inner{y, C(\cdot)y'}}$.
\paragraph{}
\Cref{lm:C_characterization_3}: from \cref{lm:C_characterization_2}, $\IFT{\inner{y, K_e(\cdot)y'}} = \mathcal{F}^{-1}\mathcal{R}{\inner{y, K_e(\cdot)y'}}$. By injectivity of the \acl{FT}, $K_e$ is even. Since $K_e(\delta)=K_e(\inv{\delta})^*$, we must have $K_e(\delta)=K_e(\delta)^*$. 
\end{proof}


\begin{proposition}[Sufficient condition for shift-invariant $\mathcal{Y}$-Mercer kernel spectral decomposition]
\label{pr:spectral}
Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer kernel on $\mathcal{X}$ endowed with the group law $\groupop$.
\paragraph{}
If for all $y$, $y' \in\mathcal{Y}$, $\inner{K_e(.)y,y'}\in L^1(\mathcal{X},dx)$ then there exists a positive measure $\mu$ with density $p_{\mu}$ on $\mathcal{B}(\mathcal{\dual{\mathcal{X}}})$ and $A:\dual{\mathcal{X}}\to \mathcal{L}_+(\mathcal{Y})$ an operator-valued function such that for all $y,$ $y'\in\mathcal{Y}$, $\inner{A(.)y,y'}\in L^1(\mathcal{X},d\mu)$ and
\begin{dmath*}
\inner{y, K_e(\delta)y'}=\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta, \omega}}\inner{y, A(\omega)y'}p_{\mu}(\omega)d\omega.
\end{dmath*}
where $\inner{y, A(\omega)y'}p_{\mu}(\omega) = \FT{\inner{y, K_e(\cdot)y'}}(\omega)$.
\end{proposition}
\begin{proof}
From \cref{pr:inverse_ovk_Fourier_decomposition} and \cref{lm:C_characterization}, by taking $\inner{y,C(\omega)y'} = \IFT{\inner{y, K_e(\cdot)y'}}(\omega)=\FT{\inner{y, K_e(\cdot)y'}}(\omega)$, we can write the following equality concerning the \acs{OVK} signature $K_e$. 
% Suppose that $\mu$ is absolutely continuous \wrt~$d\omega$. Then for all $\delta \in \mathcal{X}$ and for all $y,$ $y'$ in $\mathcal{Y}$
\begin{dmath*}
\inner{y, K_e(\cdot)y'}(\omega)=
\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta, \omega}}\inner{y, C(\omega)y'}d\omega
=\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta, \omega}}\inner*{y, \frac{C(\omega)}{p(\omega)}y'}p(\omega)d\omega
\end{dmath*}
Where $p$ is a function mapping the measured space $(\dual{\mathcal{X}}, \mathcal{B}(\dual{\mathcal{X}}), d\omega)$ to $\mathbb{R}$. It is always possible to choose $p(\omega)$ such that $\int_{\dual{\mathcal{X}}}p(\omega)d\omega=1$ in this case $p(\omega)$ is the density of a probability measure $\mu$. In this case we note $p(\omega)=p_{\mu}(\omega)$. Conclude by taking $A(\omega)=C(\omega)/p_\mu(\omega)$ and $d\mu(\omega)=p_\mu(\omega)d\omega$.
\end{proof}
\paragraph{}
In the case where $\mathcal{Y}=\mathbb{R}^p$, we rewrite \cref{pr:spectral} coefficient-wise by choosing an orthonormal basis $(e_1,\ldots, e_p)$ of $\mathcal{Y}$, such that for all $ i,j\inrange{1}{p}$,
\begin{dmath}
\label{eq:operator_identification_real}\inner{e_i,C(\omega)e_j}\hiderel{=}C(\omega)_{ij}\hiderel{=}A(\omega)_{ij}p_{\mu}(\omega)\hiderel{=}\FT{K_e(\delta)_{ij}}.
\end{dmath}
It follows that for all $i$, $j \inrange{1}{p}$,
\begin{dmath}\label{eq:matrix-exp}
K_e(x\groupop \inv{z})_{ij} \hiderel{=} \FT{A(\cdot)_{ij}p_\mu(\cdot)}
\end{dmath}

\begin{remark}
Note that although the \acl{IFT} of $K_e$ yields a unique operator-valued function $C(\cdot)$, the decomposition of $C(\omega)$ into $A(\omega)p_\mu(\omega)$ is again not unique. The choice of the decomposition may be justified by the computational cost or by the nature of the constants involved in the uniform convergence of the estimator.
\end{remark}

\subsection{Regularization property}
We have shown so far that it is always possible to construct a feature map that allows to approximate a shift-invariant $\mathcal{Y}$-Mercer kernel. However we could also propose a construction of such map by studying the regularization induced with respect to the \acl{FT} of a target function $f\in \mathcal{H}_K$. In other words, what is the norm in $L^2(\dual{\mathcal{X}}, d\omega, \mathcal{Y}')$ induced by $\norm{\cdot}_K$?
\begin{proposition}
Let $K$ be a shift-invariant $\mathcal{Y}$-Mercer Kernel such that for all $y$, $y'$ in $\mathcal{Y}$, $\inner{y, K_e(\cdot)y'}\in L^1(\mathcal{X}, dx)$.
\paragraph{}
Let $\inner{y, A(\omega)y'}p_\mu(\omega)\colonequals\FT{\inner{y, K_e(\cdot)^*y'}}(\omega)$ and let $f\in\mathcal{H}_K$. Then
\begin{dmath}
\norm{f}^2_K=\displaystyle\int_{\dual{\mathcal{X}}}\frac{\inner*{\FT{f}(\omega), A\left(\omega\right)^\dagger\FT{f}(\omega)}_{\mathcal{Y}}}{p_{\mu}(\omega)}d\omega.
\label{eq:reg_L2}
\end{dmath}
\label{pr:regularization}
\end{proposition}
\begin{proof}
We first show how the \acl{FT} relates to the feature operator. Since $\mathcal{H}_K$ is embed into $\mathcal{H}=L^2(\dual{\mathcal{X}}, \mu, \mathcal{Y})$ by mean of the feature operator $W$, we have:
\begin{dgroup*}
\begin{dmath*}
\FT{\IFT{f}}(x)\hiderel{=}\int_{\dual{\mathcal{X}}}\overline{\pairing{x,\omega}}\IFT{f}(\omega)d\omega = f(x)
\end{dmath*}
\begin{dmath*}
(Wg)(x)\hiderel{=}\int_{\dual{\mathcal{X}}}\overline{\pairing{x,\omega}}p_\mu(\omega)B(\omega)g(\omega)d\omega = f(x).
\end{dmath*}
\end{dgroup*}
By injectivity of the \acl{FT}, $\IFT{f}(\omega)=p_\mu(\omega)B(\omega)g(\omega)$ $\mu$-almost everywhere. From \cref{pr:feature_operator} we have
\begin{dmath*}
\norm{f}^2_{K} = \inf \Set{\norm{g}^2_{\mathcal{H}} | \forall g\hiderel{\in}\mathcal{H}, \enskip Wg\hiderel{=}f} = \inf \Set{\int_{\dual{\mathcal{X}}} \norm{g}^2_{\mathcal{Y}}d\mu | \forall g\hiderel{\in}\mathcal{H},\enskip \IFT{f}\hiderel{=}p_{\mu}(\cdot)B(\cdot)g(\cdot)}.
\end{dmath*}
The pseudo inverse of the operator $B(\omega)$ (noted $B(\omega)^\dagger$) is the unique solution of the system $\IFT{f}(\omega)=p_\mu(\omega)B(\omega)g(\omega)$ \wrt~$g(\omega)$ with minimal norm. Eventually,
\begin{dmath*}
\norm{f}^2_K = \displaystyle\int_{\dual{\mathcal{X}}} \frac{\norm{B(\omega)^\dagger\IFT{f}(\omega)}^2_{\mathcal{Y}}}{p_{\mu}(\omega)^2}d\mu(\omega)
\end{dmath*}
Using the fact that $\IFT{.}=\mathcal{F}\mathcal{R}[.]$ and $\mathcal{F}^2[.]=\mathcal{R}[.]$,
\begin{dmath*}
\norm{f}^2_K= \displaystyle\int_{\dual{\mathcal{X}}} \frac{\norm{\mathcal{R}\left[B(\cdot)p_\mu(\cdot)\right](\omega)^\dagger\FT{f}(\omega)}^2_{\mathcal{Y}}}{p_{\mu}(\omega)^2}d\omega
\end{dmath*}
Conclude the proof by taking $d\mu(\omega)=p_{\mu}(\omega)d\omega$ and rewriting the integral as an expectation and using the fact that $A(\omega)p_\mu(\omega)=C(\omega)=C(\inv{\omega})$.
\end{proof}
Note that if $K(x,z)=k(x,z)$ is a scalar kernel then for all $\omega$ in $\dual{\mathcal{X}}$, $A(\omega)=1$. Therefore we recover a well known results for kernels that is for any $f\in\mathcal{H}_k$ we have $\norm{f}_k=\int_{\dual{\mathcal{X}}}\FT{k_e}(\omega)^{-1}\FT{f}(\omega)^2d\omega$. We also note that the regularization property in $\mathcal{H}_K$ does not depends (as expected) on the decomposition of $A(\omega)$ into $B(\omega)B(\omega)^*$. Therefore the decomposition should be chosen such that it optimizes the computation cost. For instance if $A(\omega)\in\mathcal{L}(\mathbb{R}^p)$ has rank $r$, one could find an operator $B(\omega)\in\mathcal{L}(\mathbb{R}^p, \mathbb{R}^r)$ such that $A(\omega)=B(\omega)B(\omega)^*$.

\subsection{Functional Fourier feature map}
Let us introduce a functional feature map, we call here \emph{Fourier Feature map}, defined by the following proposition as a direct consequence of \cref{pr:mercer_kernel_bochner}.

\begin{proposition}[Fourier feature map]\label{pr:fourier_feature_map}
If there exist an operator-valued function $B:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y},\mathcal{Y}')$ such that for all $y$, $y'\in\mathcal{Y}$, $\inner{y, B(\omega)B(\omega)^*y'}=\inner{y, A(\omega)y'}$ $\mu$-almost everywhere and $\inner{y, A(\omega)y'}\in L^1(\dual{\mathcal{X}},d\mu)$ then the operator $\Phi_x$ defined for all $y$ in $\mathcal{Y}$ by
\begin{dmath}
\label{eq:feature_shiftinv_map}
(\Phi_x y)(\omega)=\pairing{x,\omega}B(\omega)^*y,
\end{dmath}
is \emph{a feature map}\mpar{\Ie~it satisfies for all $x$, $z \in \mathcal{X}$, $\Phi_x^*\Phi_z=K(x,z)$ where $K$ is a $\mathcal{Y}$-Mercer \acs{OVK}.} of some shift-invariant kernel $K$.
\end{proposition}
\begin{proof}
For all $y$, $y'\in \mathcal{Y}$ and $x$, $z\in\mathcal{X}$,
\begin{dmath*}
\inner{y, \Phi_x^*\Phi_z y'} = \inner{\Phi_x y, \Phi_z y'}_{L^2(\dual{\mathcal{X}},\mu,\mathcal{Y}')} \\
= \int_{\dual{\mathcal{X}}}\conj{\pairing{x,\omega}}\inner{y, B(\omega)\pairing{z,\omega}B(\omega)^\adjoint y'}d\mu(\omega) \\
= \int_{\dual{\mathcal{X}}}\conj{\pairing{x \groupop \myinv{z},\omega}}\inner{y B(\omega)B(\omega)^\adjoint y'}d\mu(\omega) \\
= \int_{\dual{\mathcal{X}}}\conj{\pairing{x \groupop \inv{z},\omega}}\inner{y,A(\omega)y'}d\mu(\omega),
\end{dmath*}
which defines a $\mathcal{Y}$-Mercer according to \cref{pr:mercer_kernel_bochner} of \citet{Carmeli2010}.
\end{proof}
With this notation notice that $\Phi: \mathcal{X} \to \mathcal{L}(\mathcal{Y}; L^2(\dual{\mathcal{X}}, \mu; \mathcal{Y}'))$ such that $\Phi_x\in \mathcal{L}(\mathcal{Y}; L^2(\dual{\mathcal{X}}, \mu; \mathcal{Y}'))$ where $\Phi_x\colonequals\Phi(x)$.

\subsection{Building Operator-valued Random Fourier Features}
Throughout the document, without loss of generality, we assume that $\int_{\mathcal{X}} d\mu(\omega)=1$ and thus $d\mu$ is a probability measure with density $p_{\mu}$. As shown in \cref{pr:fourier_feature_map} it is always possible to find a pair $(A(\omega), d\mu)$ such that $d\mu$ is a probability measure and $\expectation_\mu{\conj{\pairing{\delta,\omega}}A(\omega)}$.
\paragraph{}
Given a $\mathcal{Y}$-Mercer shift-invariant kernel $K$ on $\mathcal{X}$, an approximation of $K$ can be obtained using a decomposition $(A, \mu)$ and a plug-in Monte-Carlo estimator instead of the expectation. However, for efficient computations, as motivated in the introduction, we are interested in finding an approximated feature map more than a kernel approximation. Indeed, an approximated feature map will allow to build linear models in regression tasks. The following proposition provides the general form of an \acl{ORFF}.
\begin{proposition}\label{pr:ORFF-map}If one can find $B: \dual{\mathcal{X}} \to \mathcal{L}(\mathcal{Y}',\mathcal{Y})$ and a probability measure $\mu$ on $\mathcal{B}(\dual{\mathcal{X}})$, such that for all $y\in\mathcal{Y}$ and all $y'\in\mathcal{Y}'$, $\inner{y, B(\cdot)y'} \in L^2(\dual{\mathcal{X}}, d\mu)$, then the operator-valued function
\begin{equation}\label{eq:phitilde}
\tildePhi{1:D}(x)= \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*, \qquad \omega_j \sim \mu
\end{equation}
is an approximated feature map of an \acl{OVK}\mpar{\Ie~it satisfies $\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)\converges{\asurely}{D\to\infty}K(x,z)$ where $K$ is a $\mathcal{Y}$-Mercer \acs{OVK}.}.
\end{proposition}
\begin{proof}
Let $\omega_1, \ldots, \omega_D$ be $D$ \iid~random vectors following the law $\mu$. For all $x$, $z \in \mathcal{X}$ and all $y$, $y' \in \mathcal{Y}$, 
\begin{dmath*}
\inner*{\tildePhi{1:D}(x)y,\tildePhi{1:D}(z)y'}=\inner*{y,\left(\frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{z, \omega_j}B(\omega_j)^*\right)^* \left(\frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*\right)y'}
= \frac{1}{D} \sum_{j=1}^D \conj{\pairing{x\groupop \inv{z}, \omega_j}}A(\omega_j),
\end{dmath*}
where $A(\omega)=B(\omega)B(\omega)^*$. By assumption $\inner{y, A(\cdot)y'}\in L^1(\dual{\mathcal{X}},\mu)$ and $\omega_j$ are \iid~. Hence from the strong law of large numbers and \cref{pr:mercer_kernel_bochner}, 
\begin{dmath*}
\frac{1}{D} \sum_{j=1}^D \conj{\pairing{x\groupop\inv{z},\omega_j}}A(\omega_j)\converges{\asurely}{D\to\infty}\expectation_{\mu}[\conj{\pairing{x\groupop z^{-1},\omega_j}}A(\omega)]\hiderel{=}K_e(x\groupop\inv{z})
\end{dmath*}
in the weak operator topology.
\end{proof}
The approximate feature map proposed in \cref{pr:ORFF-map} has direct link with the functional feature map defined in \cref{pr:fourier_feature_map} since we have for all $y\in\mathcal{Y}$
\begin{dmath}
\tildePhi{1:D}(x)y = \frac{1}{\sqrt{D}} \Vect_{j=1}^D (\Phi_x y)(\omega_j) = \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*y, \qquad \omega_j \hiderel{\sim} \mu.
\end{dmath}
Therefore $\tildePhi{1:D}(x)$ can be seen as an \say{operator-valued vector} corresponding the \say{stacking} of $D$ \iid~operator-valued realization of $\Phi_x$, the functional feature map. In the same way we can define an approximate feature operator $\tildeW{D}$.
\begin{definition}[Random Fourier feature operator]
Let $\theta=\vect_{j=1}^D\theta_j\in\left(\mathcal{Y}'\right)^D$, where $\theta\in\mathcal{Y}'$. We call random Fourier feature operator the linear application $W:\left(\mathcal{Y}'\right)^D\to\mathcal{F}(\mathcal{X};\mathcal{Y})$ defined as follow.
\begin{dmath}
(\tildeW{D} \theta)(x) \hiderel{\colonequals} \tildePhi{1:D}(x)^*\theta\hiderel{=}\frac{1}{D}\sum_{j=1}^D \conj{\pairing{x,\omega_j}}B(\omega_j)\theta_j, \qquad \omega_j \hiderel{\sim} \mu.
\end{dmath}
\end{definition}
The approximate feature operator is useful to show the relations between the approximate feature map with the functional feature map defined in \cref{pr:fourier_feature_map}.
\begin{proposition} Let $g\in \mathcal{H}=L^2(\mathcal{\dual{X}},d\mu;\mathcal{Y})$ and let $\theta \colonequals \vect_{j=1}^Dg(\omega_j)$ where $\omega_j\sim \mu$. Then for all $g\in\mathcal{H}$,
\begin{propenum}
\item \label{pr:cv_feature_map_1} $\tildePhi{1:D}(x)^*\theta \converges{\asurely}{D\to\infty} \Phi_x^*g$,
\item \label{pr:cv_feature_map_2} $\norm{\theta}_{\mathcal{Y}}^2 \converges{\asurely}{D\to\infty} \norm{g}_{L^2\left(\dual{\mathcal{X}}, d\mu; \mathcal{Y}'\right)}^2$.
\end{propenum}
\end{proposition}
\begin{proof}
Proof of \cref{pr:cv_feature_map_1}: since $\omega_1, \hdots \omega_D$ are \iid~random vectors, for all $y\in \mathcal{Y}$ and for all $y'\in\mathcal{Y}'$, $\inner{y, B(\cdot)y'}\in L^2(\dual{\mathcal{X}},d\mu)$ and $g\in L^2(\dual{\mathcal{X}},d\mu);\mathcal{Y})$, from the strong law of large numbers
\begin{dmath*}
\tildePhi{1:D}(x)^*\theta=\frac{1}{D}\sum_{j=1}^D \conj{\pairing{x,\omega_j}}B(\omega_j)g(\omega_j), \qquad \omega_j \hiderel{\sim} \mu \\
\converges{\asurely}{D\to\infty} \int_{\dual{\mathcal{X}}}\conj{\pairing{x,\omega}}B(\omega)g(\omega)d\mu(\omega)
\hiderel{=} (Wg)(x) \hiderel{\colonequals} \Phi_x^*g.
\end{dmath*}
Proof of \cref{pr:cv_feature_map_2}: again, since $\omega_1, \hdots \omega_D$ are \iid~random vectors and $g\in L^2(\dual{\mathcal{X}},d\mu;\mathcal{Y})$, from the strong law of large numbers
\begin{dmath*}
\norm{\theta}^2_{\mathcal{Y}}=\sum_{j=1}^D\norm{g(\omega_j)}^2_{\mathcal{Y}}, \qquad \omega_j \hiderel{\sim} \mu \\
\converges{\asurely}{D\to\infty} \int_{\dual{\mathcal{X}}} \norm{g(\omega)}_{\mathcal{Y}}^2d\mu(\omega)
\colonequals \norm{g}_{L^2\left(\dual{\mathcal{X}}, d\mu; \mathcal{Y}'\right)}^2
\end{dmath*}
\end{proof}
Hence the sequence of function $\tildef{D}\colonequals \tildePhi{1:D}(\cdot)^*\theta$ converges almost surely to a function $f\in\mathcal{H}_{(A,d\mu)}{\scriptstyle\implies} \mathcal{H}_K$. Therefore, in light of \cref{pr:regularization}, it is possible to define an approximate feature map of an \acl{OVK} from its regularization properties in the \acs{vv-RKHS}.
Otherwise \cref{cr:ORFF-map-kernel} exhibit a construction of an \acs{ORFF} directly from an \acs{OVK}.
\begin{corollary}
\label{cr:ORFF-map-kernel}
If $K(x,z)$ is a shift-invariant $\mathcal{Y}$-Mercer kernel such that for all $y$, $y'\in\mathcal{Y}$, $\inner{y, K_e(\delta)y'}\in L^1(\mathcal{X},dx)$. Then
\begin{equation}
\tildePhi{1:D}(x)= \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*, \qquad \omega_j \sim \mu,
\end{equation}
where $\inner{y, B(\omega)B(\omega)^*y'}p_{\mu}(\omega)=\FT{\inner{y, K_e(\cdot)y'}}(\omega)$, is an approximated feature map of $K$.
\end{corollary}
\begin{proof}
Find $(A(\omega), d\mu)$ from \cref{pr:spectral} and apply \cref{pr:ORFF-map}.
\end{proof}

We write $\tildePhi{1:D}(x)^*\tildePhi{1:D}(x)\approx K(x,z)$ when $\tildePhi{1:D}(x)^*\tildePhi{1:D}(x)\converges{\asurely}{} K(x,z)$ in the weak operator topology when $D$ tends to infinity. With mild abuse of notation we say that $\tildePhi{1:D}(x)$ is an approximate feature map of $\Phi_x$ \ie~$\tildePhi{1:D}(x)\approx \Phi_x$, when for all $y\in\mathcal{Y}$, $\inner{y, K(x,z)y'}=\inner{\Phi_x y, \Phi_z y'}\approx \inner{\tildePhi{1:D}(x)y, \tildePhi{1:D}(x)y'}\colonequals \tilde{K}(x,z)$ where $\Phi_x$ is defined in the sense of \cref{pr:fourier_feature_map}. 

\begin{remark}
We find a decomposition such that for all $j=1, \ldots, D$, $A(\omega_j)=B(\omega_j)B(\omega_j)^*$ either by exhibiting an analytic closed-form or using a numerical decomposition. 
\end{remark}
\Cref{cr:ORFF-map-kernel} allows us to define \cref{alg:ORFF_construction} for constructing \acs{ORFF} from an operator valued kernel.
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\begin{center}
\begin{algorithm2e}[H]\label{alg:ORFF_construction}
	\SetAlgoLined
    \Input{$K(x, z)=K_e(\delta)$ a $\mathcal{Y}$-shift-invariant Mercer kernel such that $\forall y,y'\in\mathcal{Y},$ $\inner{y, K_e(\delta)y'}\in L^1(\mathbb{R}^d, dx)$ and $D$ the number of features.}
    \Output{A random feature $\tildePhi{1:D}(x)$ such that $\tildePhi{1:D}(x)^\adjoint \tildePhi{1:D}(z) \approx K(x,z)$}
    \BlankLine
	Define the pairing $\pairing{x, \omega}$ from the \acs{LCA} group $(\mathcal{X}, \groupop)$\;
	Find a decomposition $(B(\omega)$,$\density(\omega))$ such that $B(\omega)B(\omega)^*\density(\omega)=\IFT{K_e}(\omega)$\;
% 	Build an randomized feature map via Monte-Carlo sampling from the probability measure $\mu$ and the application $B$\;
	Draw $D$ random vectors $\omega_j$, $j=1, \hdots, D$ from the probability distribution $\mu$\;
   \Return $\tildePhi{1:D}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*$\;
   \caption{Construction of \acs{ORFF} from \acs{OVK}}
   \label{al:ORFF_construction}
\end{algorithm2e}
\end{center}

\subsection{Examples of Operator Random Fourier Feature maps}
We now give two examples of operator-valued random Fourier feature map when $\mathcal{Y}\subset\mathbb{R}^p$. First we introduce the general form of an approximated feature map for a matrix-valued kernel on the additive group $(\mathbb{R}^d,+)$.
\begin{example}[Matrix-valued kernel on the additive group]\label{ex:additive_group}
In the following, $K(x,z)=K_0(x-z)$ is a $\mathbb{R}^p$-Mercer matrix-valued kernel on $\mathcal{X}=\mathbb{R}^d$ invariant w.r.t. the group operation $+$. %
Then the function $\tildePhi{1:D}$ defined as follow is an \acl{ORFF} of $K_{0}$.
\begin{equation*}
\begin{aligned}
\tildePhi{1:D}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D\begin{pmatrix}\cos{\inner{x,\omega_j}}B(\omega_j)^* \\ \sin{\inner{x,\omega_j}}B(\omega_j)^*\end{pmatrix}, \enskip \omega_j \sim \mu.
\end{aligned}
\end{equation*}
\end{example}
\begin{proof}
The (Pontryagin) dual of $\mathcal{X}=\mathbb{R}^d$
is $\dual{\mathcal{X}}=\mathbb{R}^d$, and the duality pairing is $\pairing{x-z,\omega}=\exp(i\inner{x-z, \omega})$. The kernel approximation yields:
\begin{dmath*}
\tilde{K}(x,z)=\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)
= \frac{1}{D} \sum_{j=1}^D \begin{pmatrix} \cos{\inner{x,\omega_j}} & \sin{\inner{x,\omega_j}} \end{pmatrix}\begin{pmatrix}\cos{\inner{z,\omega_j}} \\ \sin{\inner{z,\omega_j}} \end{pmatrix} A(\omega_j)
= \frac{1}{D} \sum_{j=1}^D \cos{\inner{x-z,\omega_j}}A(\omega_j) \\
\converges{\asurely}{D\to\infty}\expectation_\mu\left[\cos{\inner{x-z,\omega}}A(\omega)\right]
\end{dmath*}
in the weak operator topology. Since for all $x\in\mathcal{X}$, $\sin(x, \cdot)$ is an odd function and $A(\cdot)p_\mu(\cdot)$ is even,
\begin{dmath*}
\expectation_\mu\left[\cos{\inner{x-z,\omega}}A(\omega)\right]=\expectation_\mu\left[\exp(-i\inner{x-z,\omega})A(\omega)\right]\hiderel{=}K(x,z).
\end{dmath*}
Hence $\tilde{K}(x,z)\converges{\asurely}{D\to\infty}K(x,z)$.
% which tends to $\expectation_{\mu}[\exp(-i\inner{x-z, \omega})A(\omega) ]=\expectation_{\mu}[\conj{\pairing{x-z,\omega}}A(\omega)]=K(x, z)$ when $D$ tends to infinity.
\end{proof}
The second example extends scalar-valued Random Fourier Features on the skewed multiplicative group described in \cref{eq:pairing-skewed}) to the operator-valued case.
\begin{example}[Matrix-valued kernel on the skewed multiplicative group]
In the following, $K(x,z)=K_{1-c}(x\odot z)$ is a $\mathbb{R}^p$-Mercer matrix-valued kernel on $\mathcal{X}=(-c;+\infty)^d$ invariant w.r.t. the group operation\mpar{The group operation $\odot$ is defined in \cref{sec:background}.} $\odot$. Then the function $\tildePhi{1:D}$ defined as follow is an \acl{ORFF} of $K_{1-c}$.
\begin{equation*}
\begin{aligned}
\tildePhi(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D\begin{pmatrix}\cos{\inner{\log(x+c),\omega_j}}B(\omega_j)^* \\ \sin{\inner{\log(x+c),\omega_j}}B(\omega_j)^*\end{pmatrix}, \enskip \omega_j \sim \mu.
\end{aligned}
\end{equation*}
\end{example}
\begin{proof}
The dual of $\mathcal{X}=(-c;+\infty)^d$
is $\dual{\mathcal{X}}=\mathbb{R}^d$, and the duality pairing is $\pairing{x \odot \myinv{z},\omega}=\exp(i\inner{\log(x\odot z^{-1}+c), \omega})$ (see \citet{Li2010}). Following the proof of \cref{ex:additive_group}, we have
\begin{equation*}
\begin{aligned}
\tilde{K}(x,z)&= \frac{1}{D} \sum\nolimits_{j=1}^D \cos{\inner*{\log\left(\frac{x+c}{z+c}\right), \omega_j}}A(\omega_j).
\end{aligned}
\end{equation*}
which converges almost surely to $\expectation_{\mu}[\exp(-i\inner*{\log(x\odot z^{-1}+c)})A(\omega) ]=\expectation_{\mu}[\conj{\pairing{x\odot z^{-1},\omega}}A(\omega)]=K(x, z)$ when $D$ tends to infinity.
\end{proof}

%----------------------------------------------------------------------------------------
\section{Learning with ORFF}
\label{sec:learning_with_operator-valued_random-fourier_features}
We now turn our attention to learning function with an ORFF model that approximate an OVK model. In particular we study the case of Ridge regression.
\subsection{Ridge regression}
Let $\mathcal{S} = \Set{(x_i,y_i) | \forall i\inrange{1}{N}, \forall x_i\in\mathcal{X}, \forall y_i\in\mathcal{Y}}$ be a collection of i.i.d training samples.
Given a local loss function $L: \mathcal{S}\to \mathbb{R}^+$, we are interested in finding a \emph{vector-valued function} $f_*:\mathcal{X}\to\mathcal{Y}$, that lives in a RKHS and minimize a tradeoff between a data fitting term $L$ and a regularization term to prevent from overfitting. Namely finding $f_*\in\mathcal{H}_K$ such that
\begin{equation}
f_* = \argmin_{f\in\mathcal{H}_K}  \frac{1}{2N}\displaystyle\sum_{i=1}^NL(f(x_i), y_i) + \frac{\lambda}{2}\norm{f}^2_{\mathcal{H}_K},
\label{eq:learning_rkhs}
\end{equation}
where $\lambda\in\mathbb{R}_+$ is a regularization parameter. Regression in \acl{vv-RKHS} has been well studied and a cornerstone of learning in \acs{vv-RKHS} is the representer theorem.
\begin{theorem}
Let $K$ be a $\mathcal{Y}$-Mercer Kernel
\end{theorem}
\chapterend

%----------------------------------------------------------------------------------------
\section{Uniform bound on the approximation}
\label{sec:uniform_bound_on_the_approximation}

%----------------------------------------------------------------------------------------
\section{Consistency and generalization bounds}
\label{sec:consistency and generalization bounds}

%----------------------------------------------------------------------------------------
\section{Conclusions}
\label{sec:conclusions}
