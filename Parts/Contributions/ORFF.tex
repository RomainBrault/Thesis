%----------------------------------------------------------------------------------------
\section{Motivations}
\label{sec:motivations}
Random Fourier Features have been proved useful to implement efficiently kernel methods in the scalar case, allowing to learn a linear model based on an approximated feature map. In this work, we are interested to construct approximated operator-valued feature maps to learn vector-valued functions. With an explicit (approximated) feature map, one converts the problem of learning a function $f$ in the vector-valued Reproducing Kernel Hilbert Space $\mathcal{H}_K$ into the learning of a linear model $\tilde{f}$ defined by:
 \begin{dmath*}
 \tilde{f}(x) = \tildePhi{1:D}(x)^* \theta,
 \end{dmath*}
 where $\Phi: \mathcal{X} \to \mathcal{L}(\mathcal{H},\mathcal{Y})$ and $\theta \in \mathcal{H}$. The methodology we propose works for operator-valued kernels defined on any \acf{LCA} group, noted ($\mathcal{X}, \groupop)$, for some operation noted $\groupop$. This allows us to use the general context of Pontryagin duality for \acl{FT} of functions on \acs{LCA} groups. Building upon a generalization of Bochner's theorem for operator-valued measures, an operator-valued kernel is seen as the \emph{\acl{FT}} of an operator-valued positive measure. From that result, we extend the principle of Random Fourier Feature for scalar-valued kernels and derive a general methodology to build Operator Random Fourier Feature when operator-valued kernels are shift-invariant according to the chosen group operation.

\clearpage
%----------------------------------------------------------------------------------------
\section{Construction}
\label{sec:construction}
We present a construction of \acf{ORFF} such that $f: x\mapsto \tildePhi{1:D}(x)^*\theta$ is a continuous function that maps an arbitrary \acs{LCA} group $\mathcal{X}$ as input space to an arbitrary output Hilbert space $\mathcal{Y}$. First we define a functional \emph{Fourier feature map}, and then propose a Monte-Carlo sampling from this feature map to construct an approximation of a shift-invariant $\mathcal{Y}$-Mercer kernel.
Then, we prove the convergence of the kernel approximation $\tilde{K}(x,z)=\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)$ with high probability on \emph{compact} subsets of the \acs{LCA} $\mathcal{X}$, when $\mathcal{Y}$ is \emph{finite dimensional}. Eventually we conclude with some numerical experiments.
\subsection{Theoretical study}
The following proposition of \citet{Zhang2012,Carmeli2010} extends Bochner's theorem to any shift-invariant $\mathcal{Y}$-Mercer kernel. 
\begin{proposition}[Operator-valued Bochner's theorem \citep{Zhang2012}]\label{eq:bochner-gen}
If a continuous function $K$ from $\mathcal{X} \times \mathcal{X}$ to $\mathcal{Y}$ is a shift-invariant $\mathcal{Y}$-Mercer kernel on $\mathcal{X}$, then there exists a unique positive operator-valued measure $M: \mathcal{B}(\mathcal{X}) \to \mathcal{L}_+(\mathcal{Y})$ such that for all $x$, $z \in \mathcal{X}$,
\begin{dmath}
K(x, z) = \int_{\dual{\mathcal{X}}} \conj{\pairing{x \groupop \inv{z}, \omega}} dM(\omega),
\end{dmath}
where $M$ belongs to the set of all the $\mathcal{L}_+(\mathcal{Y})$-valued measures of bounded variation on the $\sigma$-algebra of Borel subsets of $\dual{\mathcal{X}}$. Conversely, from any positive operator-valued measure $M$, a shift-invariant kernel $K$ can be defined by \cref{eq:bochner-gen}. 
\end{proposition}
Although this theorem is central to the spectral decomposition of shift-invariant $\mathcal{Y}$-Mercer \acs{OVK}, the following results proved by \citet{Carmeli2010} provides insights about this decomposition that are more relevant in practice. It first gives the necessary conditions to build shift-invariant $\mathcal{Y}$-Mercer kernel with  a pair $(A, \mu)$ where $A$ is an operator-valued function on $\dual{\mathcal{X}}$ and $\mu$ is a real-valued positive measure on $\dual{\mathcal{X}}$. Note that obviously such a pair is not unique ad the choice of this paper may have an impact on theoretical properties as well as practical computations.
Secondly it also states that any \acs{OVK} have such a spectral decomposition when $\mathcal{Y}$ is finite dimensional or $\mathcal{X}$.

\begin{proposition}[\citet{Carmeli2010}]\label{pr:mercer_kernel_bochner}
Let $\mu$ be a positive measure on $\mathcal{B}(\mathcal{\dual{\mathcal{X}}})$ and $A: \dual{\mathcal{X}}\to \mathcal{L}(\mathcal{Y})$ such that $\inner{A(.)y,y'}\in L^1(\mathcal{X},d\mu)$ for all $y,y'\in\mathcal{Y}$ and $A(\omega)\succcurlyeq 0$ for $\mu$-almost all $\omega$. Then, for all $\delta \in \mathcal{X}$ and for all $y, y' \in \mathcal{Y}$,
\begin{dmath}
\label{eq:AK0}
K_e(\delta)=\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta,\omega}}A(\omega)d\mu(\omega)
\end{dmath}
is the kernel signature of a shift-invariant $\mathcal{Y}$-Mercer kernel $K$ such that $K(x,z)=K_e(x \groupop \inv{z})$. The \acs{vv-RKHS} $\mathcal{H}_K$ is embed in $L^2(\dual{\mathcal{X}},d\mu;\mathcal{Y}')$ by mean of the feature operator
\begin{dmath}
\label{eq:feature_operator}
(Wg)(x)=\int_{\mathcal{\dual{X}}}\conj{\pairing{x,\omega}}B(\omega)g(\omega)d\mu(\omega),
\end{dmath}
Where $B(\omega)B(\omega)^\adjoint=A(\omega)$ and both integral converges in the weak sense. If $\mathcal{Y}$ is finite dimensional or $\mathcal{X}$ is compact, any shift-invariant kernel is of the above form for some pair $(A, d\mu)$. 
\end{proposition}
\paragraph{}
When $p=1$ one can always assume $A$ is reduced to the scalar $1$, $\mu$ is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (\cref{th:bochner-scalar}).
\paragraph{}
\Cref{pr:mercer_kernel_bochner} shows that a given pair $(A,d\mu)$ characterize an \acs{OVK}. Namely given a measure $d\mu$ and a function $A$ such that $\inner{A(.)y,y'}\in L^1(\mathcal{X},d\mu)$ for all $y,y'\in\mathcal{Y}$ and $A(\omega)\succcurlyeq 0$ for $\mu$-almost all $\omega$, it gives rise to an \acs{OVK}. Since $(A,d\mu)$ determine a unique kernel we can write $\mathcal{H}_{(A,d\mu)}{\scriptstyle\implies}\mathcal{H}_K$ where $K$ is defined as in \cref{eq:AK0}. However the converse is to true: Given a $\mathcal{Y}$-Mercer shift invariant \acl{OVK}, there exist infinitely many pairs $(A,d\mu)$ that characterize an \acs{OVK}.
\paragraph{}
The main difference between \cref{eq:bochner-gen} and \cref{pr:mercer_kernel_bochner} is that the first one characterize an \acs{OVK} by a unique \acf{POVM}, while the second one shows that the \acs{POVM} that uniquely characterize a $\mathcal{Y}$-Mercer \acs{OVK} has an operator-valued density with respect to a \emph{scalar} measure $\mu$; and that this operator-valued density is not unique.
\paragraph{}
Finally \cref{pr:mercer_kernel_bochner} does not provide any \emph{constructive} way to obtain the pair $(A,d\mu)$ that characterize an \acs{OVK}.
The following \cref{subsec:sufficient_conditions} is based on an other proposition of \citeauthor{carmeli2006vector} and show that if the kernel signature $K_e(\delta)$ of an $\acs{OVK}$ is in $L^1$ then it is possible to construct \emph{explicitly} a pair $(C,d\omega)$ from it. We show that we can always extract a scalar-valued \emph{probability} density function from $C$ such that we obtain a pair $(A,d\mu)$ where $\mu$ is a probability measure.

\subsection{Sufficient conditions of existence}
\label{subsec:sufficient_conditions}
While \cref{pr:mercer_kernel_bochner} gives some insights on how to build an approximation of a $\mathcal{Y}$-Mercer kernel, we need a theorem that provides an explicit construction of the pair $A(\omega), \mu(\omega)$ from the kernel signature. Proposition 14 in \citet{Carmeli2010} gives the solution, and also provide a sufficient condition for \cref{pr:mercer_kernel_bochner} to apply.
\begin{proposition}[\citet{Carmeli2010}]
\label{pr:inverse_ovk_Fourier_decomposition}
Let $K$ be a shift-invariant $\mathcal{Y}$-Mercer kernel. %
Suppose that $\forall z \in \mathcal{X}$ and $\forall y ,y' \in\mathcal{Y}$, $\inner{K_e(.)y,y'}\in L^1(\mathcal{X},dx)$ where $dx$ denotes the Haar measure on $\mathcal{X}$ endowed with the group law $\groupop$. %
Define $C$ such that for all $\omega \in \dual{\mathcal{X}}$ and for all $y$, $y'$ in $\mathcal{Y}$,
\begin{dmath}\label{eq:CK0}
\inner{y,C(\omega)y'} = \int_{\mathcal{X}} \pairing{\delta, \omega}\inner{y, K_e(\delta)y'}d\delta = \IFT{\inner{y, K_e(\cdot)y'}}(\omega)
\end{dmath}
Then
\begin{propenum}
\item $C(\omega)$ is a bounded non-negative operator for all $\omega \in \dual{\mathcal{X}}$,
\item $\inner{y, C(.)y'}\in L^1(\dual{\mathcal{X}},d\omega)$ for all $y,y'\in\mathcal{X}$,
\item for all $\delta\in\mathcal{X}$ and for all $y$, $y'$ in $\mathcal{Y}$,
\begin{dmath*}
\inner{y, K_e(\delta)y'}= \int_{\dual{\mathcal{X}}}\conj{\pairing{\delta,\omega}}\inner{y, C(\omega)y'}d\omega
=\FT{\inner{y, C(\cdot)y'}}(\delta).
\end{dmath*}
\end{propenum}
\end{proposition}
The following proposition allows to build a spectral decomposition of a shift-invariant $\mathcal{Y}$-Mercer kernel on a \acs{LCA} group $\mathcal{X}$ endowed with the group law $\groupop$ with respect to a scalar probability measure, by extracting a scalar probability density from $C$.
\paragraph{}
There have been a lot of confusion in the literature whether a kernel is the \acl{FT} or \acl{IFT} of a measure. However \cref{lm:fourier_shift_invariant} clarify the relation between the \acl{FT} and \acl{IFT} for a translation invariant \acl{OVK}. Notice that in the real scalar case the \acl{FT} and \acl{IFT} of a shift-invariant kernel are the same, while the difference is significant for \acs{OVK}.
\paragraph{}
The following lemma is a direct consequence of the definition of $C(\omega)$ as the \acl{FT} of the adjoint of $K_e$ and also helps simplifying the definition of \acs{ORFF}.
\begin{lemma}
\label{lm:C_characterization}
Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer kernel and let $\inner{y, C(\cdot)y'}=\IFT{\inner{y, K_e(\cdot)y'}}$ for all $y$. $y'\in\mathcal{Y}$. Then 
\begin{propenum}
\item \label{lm:C_characterization_1} $C(\omega)$ is self-adjoint and $C$ is even.
\item \label{lm:C_characterization_2} $\IFT{\inner{y, K_e(\cdot)y'}} = \FT{\inner{y, K_e(\cdot)y'}}$.
\item \label{lm:C_characterization_3} $K_e(\delta)$ is self-adjoint and $K_e$ is even.
\end{propenum}
\end{lemma}
\begin{proof}
For any function $f\in L^1(\mathcal{X},dx)$ define the flip operator $\mathcal{R}$ by
\begin{dmath*}
\mathcal{R}f(x) \colonequals f(\inv{x}).
\end{dmath*}
for any shift invariant $\mathcal{Y}$-Mercer kernel, we have for all $\delta\in\mathcal{X}$,  $K_e(\delta)=K_e(\inv{\delta})^\adjoint$. Indeed,
\begin{dmath*}
\mathcal{R}\inner{y,K_e(x\groupop \inv{z})y'}\hiderel{=}\inner{y,K_e(\inv{(x\groupop \inv{z})})y'}\hiderel{=}\inner{y,K_e(\inv{x}\groupop z)y'}=\inner{y,K_e(x\groupop \inv{z})^\adjoint y'}.
\end{dmath*}
\Cref{lm:C_characterization_1}: taking the Fourier transform yields,
\begin{dmath*}
\IFT{\inner{y, K_e(\cdot)y'}}=\mathcal{F}^{-1}\mathcal{R}\left[\inner{K_e(\cdot)y, y'}\right]
\hiderel{=}\mathcal{R}\inner{C(\cdot)y, y'}.
\end{dmath*}
Hence $C(\omega)=C(\inv{\omega})^\adjoint$.
\paragraph{}
Suppose that $\mathcal{Y}$ is a complex Hilbert space. Since for all $\omega\in\mathcal{\dual{X}}$ $C(\omega)$ is bounded and non-negative so $C(\omega)$ is self-adjoint. Besides we have $C(\omega)=C(\inv{\omega})^*$ to $C$ must be pair.
\paragraph{}
Suppose that $\mathcal{Y}$ is a real Hilbert space. Then we have the additional hypothesis that $K_e(\delta)=K_e(\delta)^\adjoint$. Taking the \acl{FT} yields that $C(\omega)=C(\omega)^\adjoint$. Since for any shift invariant $\mathcal{Y}$-Mercer kernel $C(\omega)=C(\inv{\omega})^\adjoint$ we also conclude that $C(\inv{\omega})=C(\omega)$.
\paragraph{}
\Cref{lm:C_characterization_2}: simply, for all $y$. $y'\in\mathcal{Y}$, $\inner{y, C(\inv{\omega})y'}=\inner{y, C(\omega)y'}$ thus $\IFT{\inner{y, C(\cdot)y'}}=\mathcal{F}\mathcal{R}\left[\inner{y, C(\cdot)y'}\right]=\FT{\inner{y, C(\cdot)y'}}$.
\paragraph{}
\Cref{lm:C_characterization_3}: from \cref{lm:C_characterization_2}, $\IFT{\inner{y, K_e(\cdot)y'}} = \mathcal{F}^{-1}\mathcal{R}{\inner{y, K_e(\cdot)y'}}$. By injectivity of the \acl{FT}, $K_e$ is even. Since $K_e(\delta)=K_e(\inv{\delta})^*$, we must have $K_e(\delta)=K_e(\delta)^*$. 
\end{proof}


\begin{proposition}[Shift-invariant $\mathcal{Y}$-Mercer kernel spectral decomposition]
\label{pr:spectral}
Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer kernel. If for all $y$, $y' \in\mathcal{Y}$, $\inner{K_e(.)y,y'}\in L^1(\mathcal{X},dx)$ then there exists a positive measure $\mu$ with density $p_{\mu}$ on $\mathcal{B}(\mathcal{\dual{\mathcal{X}}})$ and $A:\dual{\mathcal{X}}\to \mathcal{L}_+(\mathcal{Y})$ an operator-valued function such that for all $y,$ $y'\in\mathcal{Y}$, $\inner{A(.)y,y'}\in L^1(\dual{\mathcal{X}},d\mu)$ and
\begin{dmath*}
\inner{y, K_e(\delta)y'}=\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta, \omega}}\inner{y, A(\omega)y'}p_{\mu}(\omega)d\omega.
\end{dmath*}
where $\inner{y, A(\omega)y'}p_{\mu}(\omega) = \FT{\inner{y, K_e(\cdot)y'}}(\omega)$.
\end{proposition}
\begin{proof}
This is a simple consequence of \cref{pr:inverse_ovk_Fourier_decomposition} and \cref{lm:C_characterization}. By taking $\inner{y,C(\omega)y'} = \IFT{\inner{y, K_e(\cdot)y'}}(\omega)=\FT{\inner{y, K_e(\cdot)y'}}(\omega)$ we can write the following equality concerning the \acs{OVK} signature $K_e$. 
% Suppose that $\mu$ is absolutely continuous \wrt~$d\omega$. Then for all $\delta \in \mathcal{X}$ and for all $y,$ $y'$ in $\mathcal{Y}$
\begin{dmath*}
\inner{y, K_e(\cdot)y'}(\omega)=
\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta, \omega}}\inner{y, C(\omega)y'}d\omega
=\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta, \omega}}\inner*{y, \frac{C(\omega)}{p(\omega)}y'}p(\omega)d\omega
\end{dmath*}
Where $p$ is a function mapping the measured space $(\dual{\mathcal{X}}, \mathcal{B}(\dual{\mathcal{X}}), d\omega)$ to $\mathbb{R}$. It is always possible to choose $p(\omega)$ such that $\int_{\dual{\mathcal{X}}}p(\omega)d\omega=1$ in this case $p(\omega)$ is the density of a probability measure $\mu$. In this case we note $p(\omega)=p_{\mu}(\omega)$. Conclude by taking $A(\omega)=C(\omega)/p_\mu(\omega)$ and $d\mu(\omega)=p_\mu(\omega)d\omega$.
\end{proof}
\paragraph{}
In the case where $\mathcal{Y}=\mathbb{R}^p$, we rewrite \cref{pr:spectral} coefficient-wise by choosing an orthonormal basis $(e_1,\ldots, e_p)$ of $\mathcal{Y}$, such that for all $ i,j\inrange{1}{p}$,
\begin{dmath}
\label{eq:operator_identification_real}\inner{e_i,C(\omega)e_j}\hiderel{=}C(\omega)_{ij}\hiderel{=}A(\omega)_{ij}p_{\mu}(\omega)\hiderel{=}\FT{K_e(\delta)_{ij}}.
\end{dmath}
It follows that for all $i$, $j \inrange{1}{p}$,
\begin{dmath}\label{eq:matrix-exp}
K_e(x\groupop \inv{z})_{ij} \hiderel{=} \FT{A(\cdot)_{ij}p_\mu(\cdot)}
\end{dmath}

\begin{remark}
Note that although the \acl{IFT} of $K_e$ yields a unique operator-valued function $C(\cdot)$, the decomposition of $C(\omega)$ into $A(\omega)p_\mu(\omega)$ is again not unique. The choice of the decomposition may be justified by the computational cost or by the nature of the constants involved in the uniform convergence of the estimator.
\end{remark}
\begin{proposition}[Bounded shift-invariant $\mathcal{Y}$-Mercer kernel spectral decomposition]
Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer kernel. If for all $y$, $y' \in\mathcal{Y}$, $\inner{K_e(.)y,y'}\in L^1(\mathcal{X},dx)$ then for all $y$, $y'\in\mathcal{Y}$, $\inner{y, C(\cdot)y'}=\FT{\inner{y, K_e(\cdot)y'}}$ exists and
\begin{dmath}
\inner{y, K_e(\delta)y'}=\int_{\dual{\mathcal{X}}}\conj{\pairing{\delta, \omega}}\inner{y, A_p(\omega)y'}p_{\mu}(\omega)d\omega.
\end{dmath}
with
\begin{dgroup}
\begin{dmath}
c_p=\int_{\dual{\mathcal{X}}}\norm{C(\omega)}_p d\omega
\end{dmath}
\begin{dmath}
\label{eq:bounded_C}
A_p(\omega)=c_p\norm{C(\omega)}_p^{-1}C(\omega)
\end{dmath}
\begin{dmath}
p_{\mu}(\omega)=c_p^{-1}\norm{C(\omega)}_p.
\end{dmath}

\end{dgroup}
\label{eq:bounded_mu}
Moreover
\begin{propenum}
\item $A_p(\cdot)$ and $p_{\mu}$ are even functions.
\item $\sup_{\omega\in\dual{\mathcal{X}}}\norm{A_p(\omega)}_{\mathcal{Y},\mathcal{Y}}\le c_p$.
\item In particular $\sup_{\omega\in\dual{\mathcal{X}}}\norm{A_{\infty}(\omega)}_{\mathcal{Y},\mathcal{Y}} = c_{\infty} < \infty.$
\item For all $y$, $y'\in\mathcal{Y}$, $\inner{y, A_p(\omega)y'}\in L^1(\dual{\mathcal{X}}, d\mu)$.
\end{propenum}
\end{proposition}
\begin{proof} Apply \cref{pr:spectral} with $A_p(\omega)$ and $p_\mu(\omega)$ defined respectively as in \cref{eq:bounded_C} and \cref{eq:bounded_mu}. We see immediately that $\int_{\dual{\mathcal{X}}}p_{\mu}(\omega)d\omega=1$ and $A_p(\omega)p_{\mu}(\omega)=C(\omega)$.
\paragraph{}
The Schatten norms $\norm{\cdot}_p$ verifies $\norm{\cdot}_p\ge \norm{\cdot}_q \ge \norm{\cdot}_{\mathcal{Y},\mathcal{Y}}=\norm{\cdot}_{\infty}$ for all $p$, $q\in\mathbb{N}$ such that $p \le q$. Therefore 
\begin{dmath*}
\sup_{\omega\in\dual{\mathcal{X}}}\norm{A_p(\omega)}_{\mathcal{Y},\mathcal{Y}} = \sup_{\omega\in\dual{\mathcal{X}}}\norm{\frac{\int_{\dual{\mathcal{X}}}\norm{C(\omega)}_p d\omega}{\norm{C(\omega)}_p}C(\omega)}_{\mathcal{Y},\mathcal{Y}}
=\int_{\dual{\mathcal{X}}}\norm{C(\omega)}_p d\omega\sup_{\omega\in\dual{\mathcal{X}}}\frac{\norm{C(\omega)}_{\mathcal{Y},\mathcal{Y}}}{\norm{C(\omega)}_p}
\le \int_{\dual{\mathcal{X}}}\norm{C(\omega)}_pd\omega\hiderel{=}c_p.
\end{dmath*}
In particular if $p=+\infty$, $\sup_{\omega\in\dual{\mathcal{X}}}\norm{A_{\infty}(\omega)}_{\mathcal{Y},\mathcal{Y}}=c_{\infty}$. Since $\mathcal{Y}$ is separable and for all $y$, $y'\in\mathcal{Y}$, $\inner{y,C(\cdot)y'}\in L^1(\dual{\mathcal{X}},d\omega)$ we have $c_\infty=\int_{\dual{\mathcal{X}}}\norm{C(\omega)}_{\mathcal{Y},\mathcal{Y}} d\omega<+\infty$.
\end{proof}
\subsection{Examples of spectral decomposition}

\subsection{Regularization property}
We have shown so far that it is always possible to construct a feature map that allows to approximate a shift-invariant $\mathcal{Y}$-Mercer kernel. However we could also propose a construction of such map by studying the regularization induced with respect to the \acl{FT} of a target function $f\in \mathcal{H}_K$. In other words, what is the norm in $L^2(\dual{\mathcal{X}}, d\omega, \mathcal{Y}')$ induced by $\norm{\cdot}_K$?
\begin{proposition}
Let $K$ be a shift-invariant $\mathcal{Y}$-Mercer Kernel such that for all $y$, $y'$ in $\mathcal{Y}$, $\inner{y, K_e(\cdot)y'}\in L^1(\mathcal{X}, dx)$. Then for all $f\in\mathcal{H}_K$
\begin{dmath}
\norm{f}^2_K=\displaystyle\int_{\dual{\mathcal{X}}}\frac{\inner*{\FT{f}(\omega), A\left(\omega\right)^\dagger\FT{f}(\omega)}_{\mathcal{Y}}}{p_{\mu}(\omega)}d\omega.
\label{eq:reg_L2}
\end{dmath}
where $\inner{y, A(\omega)y'}p_\mu(\omega)\colonequals\FT{\inner{y, K_e(\cdot)y'}}(\omega)$. 
\label{pr:regularization}
\end{proposition}
\begin{proof}
We first show how the \acl{FT} relates to the feature operator. Since $\mathcal{H}_K$ is embed into $\mathcal{H}=L^2(\dual{\mathcal{X}}, \mu, \mathcal{Y})$ by mean of the feature operator $W$, we have:
\begin{dgroup*}
\begin{dmath*}
\FT{\IFT{f}}(x)\hiderel{=}\int_{\dual{\mathcal{X}}}\overline{\pairing{x,\omega}}\IFT{f}(\omega)d\omega = f(x)
\end{dmath*}
\begin{dmath*}
(Wg)(x)\hiderel{=}\int_{\dual{\mathcal{X}}}\overline{\pairing{x,\omega}}p_\mu(\omega)B(\omega)g(\omega)d\omega = f(x).
\end{dmath*}
\end{dgroup*}
By injectivity of the \acl{FT}, $\IFT{f}(\omega)=p_\mu(\omega)B(\omega)g(\omega)$ $\mu$-almost everywhere. From \cref{pr:feature_operator} we have
\begin{dmath*}
\norm{f}^2_{K} = \inf \Set{\norm{g}^2_{\mathcal{H}} | \forall g\hiderel{\in}\mathcal{H}, \enskip Wg\hiderel{=}f} = \inf \Set{\int_{\dual{\mathcal{X}}} \norm{g}^2_{\mathcal{Y}}d\mu | \forall g\hiderel{\in}\mathcal{H},\enskip \IFT{f}\hiderel{=}p_{\mu}(\cdot)B(\cdot)g(\cdot)}.
\end{dmath*}
The pseudo inverse of the operator $B(\omega)$ (noted $B(\omega)^\dagger$) is the unique solution of the system $\IFT{f}(\omega)=p_\mu(\omega)B(\omega)g(\omega)$ \wrt~$g(\omega)$ with minimal norm. Eventually,
\begin{dmath*}
\norm{f}^2_K = \displaystyle\int_{\dual{\mathcal{X}}} \frac{\norm{B(\omega)^\dagger\IFT{f}(\omega)}^2_{\mathcal{Y}}}{p_{\mu}(\omega)^2}d\mu(\omega)
\end{dmath*}
Using the fact that $\IFT{.}=\mathcal{F}\mathcal{R}[.]$ and $\mathcal{F}^2[.]=\mathcal{R}[.]$,
\begin{dmath*}
\norm{f}^2_K= \displaystyle\int_{\dual{\mathcal{X}}} \frac{\norm{\mathcal{R}\left[B(\cdot)p_\mu(\cdot)\right](\omega)^\dagger\FT{f}(\omega)}^2_{\mathcal{Y}}}{p_{\mu}(\omega)^2}d\omega
\end{dmath*}
Conclude the proof by taking $d\mu(\omega)=p_{\mu}(\omega)d\omega$ and rewriting the integral as an expectation and using the fact that $A(\omega)p_\mu(\omega)=C(\omega)=C(\inv{\omega})$.
\end{proof}
Note that if $K(x,z)=k(x,z)$ is a scalar kernel then for all $\omega$ in $\dual{\mathcal{X}}$, $A(\omega)=1$. Therefore we recover a well known results for kernels that is for any $f\in\mathcal{H}_k$ we have $\norm{f}_k=\int_{\dual{\mathcal{X}}}\FT{k_e}(\omega)^{-1}\FT{f}(\omega)^2d\omega$. We also note that the regularization property in $\mathcal{H}_K$ does not depends (as expected) on the decomposition of $A(\omega)$ into $B(\omega)B(\omega)^*$. Therefore the decomposition should be chosen such that it optimizes the computation cost. For instance if $A(\omega)\in\mathcal{L}(\mathbb{R}^p)$ has rank $r$, one could find an operator $B(\omega)\in\mathcal{L}(\mathbb{R}^p, \mathbb{R}^r)$ such that $A(\omega)=B(\omega)B(\omega)^*$.

\subsection{Functional Fourier feature map}
Let us introduce a functional feature map, we call here \emph{Fourier Feature map}, defined by the following proposition as a direct consequence of \cref{pr:mercer_kernel_bochner}.

\begin{proposition}[Fourier feature map]\label{pr:fourier_feature_map}
If there exist an operator-valued function $B:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y},\mathcal{Y}')$ such that for all $y$, $y'\in\mathcal{Y}$, $\inner{y, B(\omega)B(\omega)^*y'}=\inner{y, A(\omega)y'}$ $\mu$-almost everywhere and $\inner{y, A(\omega)y'}\in L^1(\dual{\mathcal{X}},d\mu)$ then the operator $\Phi_x$ defined for all $y$ in $\mathcal{Y}$ by
\begin{dmath}
\label{eq:feature_shiftinv_map}
(\Phi_x y)(\omega)=\pairing{x,\omega}B(\omega)^*y,
\end{dmath}
is \emph{a feature map}\mpar{\Ie~it satisfies for all $x$, $z \in \mathcal{X}$, $\Phi_x^*\Phi_z=K(x,z)$ where $K$ is a $\mathcal{Y}$-Mercer \acs{OVK}.} of some shift-invariant kernel $K$.
\end{proposition}
\begin{proof}
For all $y$, $y'\in \mathcal{Y}$ and $x$, $z\in\mathcal{X}$,
\begin{dmath*}
\inner{y, \Phi_x^*\Phi_z y'} = \inner{\Phi_x y, \Phi_z y'}_{L^2(\dual{\mathcal{X}},\mu,\mathcal{Y}')} \\
= \int_{\dual{\mathcal{X}}}\conj{\pairing{x,\omega}}\inner{y, B(\omega)\pairing{z,\omega}B(\omega)^\adjoint y'}d\mu(\omega) \\
= \int_{\dual{\mathcal{X}}}\conj{\pairing{x \groupop \myinv{z},\omega}}\inner{y B(\omega)B(\omega)^\adjoint y'}d\mu(\omega) \\
= \int_{\dual{\mathcal{X}}}\conj{\pairing{x \groupop \inv{z},\omega}}\inner{y,A(\omega)y'}d\mu(\omega),
\end{dmath*}
which defines a $\mathcal{Y}$-Mercer according to \cref{pr:mercer_kernel_bochner} of \citet{Carmeli2010}.
\end{proof}
With this notation notice that $\Phi: \mathcal{X} \to \mathcal{L}(\mathcal{Y}; L^2(\dual{\mathcal{X}}, \mu; \mathcal{Y}'))$ such that $\Phi_x\in \mathcal{L}(\mathcal{Y}; L^2(\dual{\mathcal{X}}, \mu; \mathcal{Y}'))$ where $\Phi_x\colonequals\Phi(x)$.

\subsection{Building Operator-valued Random Fourier Features}
Throughout the document, without loss of generality, we assume that $\int_{\mathcal{X}} d\mu(\omega)=1$ and thus $d\mu$ is a probability measure with density $p_{\mu}$. As shown in \cref{pr:fourier_feature_map} it is always possible to find a pair $(A(\omega), d\mu)$ such that $d\mu$ is a probability measure and $\expectation_\mu{\conj{\pairing{\delta,\omega}}A(\omega)}$.
\paragraph{}
Given a $\mathcal{Y}$-Mercer shift-invariant kernel $K$ on $\mathcal{X}$, an approximation of $K$ can be obtained using a decomposition $(A, \mu)$ and a plug-in Monte-Carlo estimator instead of the expectation. However, for efficient computations, as motivated in the introduction, we are interested in finding an approximated feature map more than a kernel approximation. Indeed, an approximated feature map will allow to build linear models in regression tasks. The following proposition provides the general form of an \acl{ORFF}.
\begin{proposition}\label{pr:ORFF-map}If one can find $B: \dual{\mathcal{X}} \to \mathcal{L}(\mathcal{Y}',\mathcal{Y})$ and a probability measure $\mu$ on $\mathcal{B}(\dual{\mathcal{X}})$, such that for all $y\in\mathcal{Y}$ and all $y'\in\mathcal{Y}'$, $\inner{y, B(\cdot)y'} \in L^2(\dual{\mathcal{X}}, d\mu)$, then the operator-valued function
\begin{equation}\label{eq:phitilde}
\tildePhi{1:D}(x)= \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*, \qquad \omega_j \sim \mu
\end{equation}
is an approximated feature map of an \acl{OVK}\mpar{\Ie~it satisfies $\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)\converges{\asurely}{D\to\infty}K(x,z)$ where $K$ is a $\mathcal{Y}$-Mercer \acs{OVK}.}.
\end{proposition}
\begin{proof}
Let $\omega_1, \ldots, \omega_D$ be $D$ \iid~random vectors following the law $\mu$. For all $x$, $z \in \mathcal{X}$ and all $y$, $y' \in \mathcal{Y}$, 
\begin{dmath*}
\inner*{\tildePhi{1:D}(x)y,\tildePhi{1:D}(z)y'}=\inner*{y,\left(\frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{z, \omega_j}B(\omega_j)^*\right)^* \left(\frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*\right)y'}
= \frac{1}{D} \sum_{j=1}^D \conj{\pairing{x\groupop \inv{z}, \omega_j}}A(\omega_j),
\end{dmath*}
where $A(\omega)=B(\omega)B(\omega)^*$. By assumption $\inner{y, A(\cdot)y'}\in L^1(\dual{\mathcal{X}},\mu)$ and $\omega_j$ are \iid~. Hence from the strong law of large numbers and \cref{pr:mercer_kernel_bochner}, 
\begin{dmath*}
\frac{1}{D} \sum_{j=1}^D \conj{\pairing{x\groupop\inv{z},\omega_j}}A(\omega_j)\converges{\asurely}{D\to\infty}\expectation_{\mu}[\conj{\pairing{x\groupop z^{-1},\omega_j}}A(\omega)]\hiderel{=}K_e(x\groupop\inv{z})
\end{dmath*}
in the weak operator topology.
\end{proof}
The approximate feature map proposed in \cref{pr:ORFF-map} has direct link with the functional feature map defined in \cref{pr:fourier_feature_map} since we have for all $y\in\mathcal{Y}$
\begin{dmath}
\tildePhi{1:D}(x)y = \frac{1}{\sqrt{D}} \Vect_{j=1}^D (\Phi_x y)(\omega_j) = \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*y, \qquad \omega_j \hiderel{\sim} \mu.
\end{dmath}
Therefore $\tildePhi{1:D}(x)$ can be seen as an \say{operator-valued vector} corresponding the \say{stacking} of $D$ \iid~operator-valued realization of $\Phi_x$, the functional feature map. In the same way we can define an approximate feature operator $\tildeW{D}$.
\begin{definition}[Random Fourier feature operator]
Let $\theta=\vect_{j=1}^D\theta_j\in\left(\mathcal{Y}'\right)^D$, where $\theta\in\mathcal{Y}'$. We call random Fourier feature operator the linear application $\tildeW{D}:\left(\mathcal{Y}'\right)^D\to\mathcal{F}(\mathcal{X};\mathcal{Y})$ defined as follow.
\begin{dmath}
(\tildeW{D} \theta)(x) \hiderel{\colonequals} \tildePhi{1:D}(x)^*\theta\hiderel{=}\frac{1}{D}\sum_{j=1}^D \conj{\pairing{x,\omega_j}}B(\omega_j)\theta_j, \qquad \omega_j \hiderel{\sim} \mu.
\end{dmath}
\end{definition}
The approximate feature operator is useful to show the relations between the approximate feature map with the functional feature map defined in \cref{pr:fourier_feature_map}.
\begin{proposition} 
\label{pr:phitilde_phi_rel}
Let $g\in \mathcal{H}=L^2(\mathcal{\dual{X}},d\mu;\mathcal{Y}')$ and let $\theta \colonequals \vect_{j=1}^Dg(\omega_j)$ and define $\mathcal{H}^D(\mathcal{Y}')\colonequals\Vect_{j=1}^D\mathcal{Y}'$, where $\omega_j\sim \mu$. Then for all $g\in\mathcal{H}$,
\begin{propenum}
\item \label{pr:cv_feature_map_1} $\tildePhi{1:D}(x)^*\theta \converges{\asurely}{D\to\infty} \Phi_x^*g$,
\item \label{pr:cv_feature_map_2} $\frac{1}{D}\norm{\theta}_{\mathcal{H}^D(\mathcal{Y}')}^2 \converges{\asurely}{D\to\infty} \norm{g}_{L^2\left(\dual{\mathcal{X}}, d\mu; \mathcal{Y}'\right)}^2$.
\end{propenum}
\end{proposition}
\begin{proof}
Proof of \cref{pr:cv_feature_map_1}: since $\omega_1, \hdots \omega_D$ are \iid~random vectors, for all $y\in \mathcal{Y}$ and for all $y'\in\mathcal{Y}'$, $\inner{y, B(\cdot)y'}\in L^2(\dual{\mathcal{X}},d\mu)$ and $g\in L^2(\dual{\mathcal{X}},d\mu;\mathcal{Y}')$, from the strong law of large numbers
\begin{dmath*}
\tildePhi{1:D}(x)^*\theta=\frac{1}{D}\sum_{j=1}^D \conj{\pairing{x,\omega_j}}B(\omega_j)g(\omega_j), \qquad \omega_j \hiderel{\sim} \mu \\
\converges{\asurely}{D\to\infty} \int_{\dual{\mathcal{X}}}\conj{\pairing{x,\omega}}B(\omega)g(\omega)d\mu(\omega)
\hiderel{=} (Wg)(x) \hiderel{\colonequals} \Phi_x^*g.
\end{dmath*}
Proof of \cref{pr:cv_feature_map_2}: again, since $\omega_1, \hdots \omega_D$ are \iid~random vectors and $g\in L^2(\dual{\mathcal{X}},d\mu;\mathcal{Y}')$, from the strong law of large numbers
\begin{dmath*}
\frac{1}{D}\norm{\theta}^2_{\mathcal{H}^D(\mathcal{Y}')}=\frac{1}{D}\sum_{j=1}^D\norm{g(\omega_j)}^2_{\mathcal{Y}'}, \qquad \omega_j \hiderel{\sim} \mu \\
\converges{\asurely}{D\to\infty} \int_{\dual{\mathcal{X}}} \norm{g(\omega)}_{\mathcal{Y}'}^2d\mu(\omega)
\colonequals \norm{g}_{L^2\left(\dual{\mathcal{X}}, d\mu; \mathcal{Y}'\right)}^2
\end{dmath*}
\end{proof}
Hence the sequence of function $\tildef{D}\colonequals \tildePhi{1:D}(\cdot)^*\theta$ converges almost surely to a function $f\in\mathcal{H}_{(A,d\mu)}{\scriptstyle\implies} \mathcal{H}_K$. Therefore, in light of \cref{pr:regularization}, it is possible to define an approximate feature map of an \acl{OVK} from its regularization properties in the \acs{vv-RKHS}.
Otherwise \cref{cr:ORFF-map-kernel} exhibit a construction of an \acs{ORFF} directly from an \acs{OVK}.
\begin{corollary}
\label{cr:ORFF-map-kernel}
If $K(x,z)$ is a shift-invariant $\mathcal{Y}$-Mercer kernel such that for all $y$, $y'\in\mathcal{Y}$, $\inner{y, K_e(\delta)y'}\in L^1(\mathcal{X},dx)$. Then
\begin{equation}
\tildePhi{1:D}(x)= \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*, \qquad \omega_j \sim \mu,
\end{equation}
where $\inner{y, B(\omega)B(\omega)^*y'}p_{\mu}(\omega)=\FT{\inner{y, K_e(\cdot)y'}}(\omega)$, is an approximated feature map of $K$.
\end{corollary}
\begin{proof}
Find $(A(\omega), d\mu)$ from \cref{pr:spectral} and apply \cref{pr:ORFF-map}.
\end{proof}

We write $\tildePhi{1:D}(x)^*\tildePhi{1:D}(x)\approx K(x,z)$ when $\tildePhi{1:D}(x)^*\tildePhi{1:D}(x)\converges{\asurely}{} K(x,z)$ in the weak operator topology when $D$ tends to infinity. With mild abuse of notation we say that $\tildePhi{1:D}(x)$ is an approximate feature map of $\Phi_x$ \ie~$\tildePhi{1:D}(x)\approx \Phi_x$, when for all $y\in\mathcal{Y}$, $\inner{y, K(x,z)y'}=\inner{\Phi_x y, \Phi_z y'}\approx \inner{\tildePhi{1:D}(x)y, \tildePhi{1:D}(x)y'}\colonequals \tilde{K}(x,z)$ where $\Phi_x$ is defined in the sense of \cref{pr:fourier_feature_map}. 

\begin{remark}
We find a decomposition such that for all $j=1, \ldots, D$, $A(\omega_j)=B(\omega_j)B(\omega_j)^*$ either by exhibiting an analytic closed-form or using a numerical decomposition. 
\end{remark}
\Cref{cr:ORFF-map-kernel} allows us to define \cref{alg:ORFF_construction} for constructing \acs{ORFF} from an operator valued kernel.
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\begin{center}
\begin{algorithm2e}[H]\label{alg:ORFF_construction}
	\SetAlgoLined
    \Input{$K(x, z)=K_e(\delta)$ a $\mathcal{Y}$-shift-invariant Mercer kernel such that $\forall y,y'\in\mathcal{Y},$ $\inner{y, K_e(\delta)y'}\in L^1(\mathbb{R}^d, dx)$ and $D$ the number of features.}
    \Output{A random feature $\tildePhi{1:D}(x)$ such that $\tildePhi{1:D}(x)^\adjoint \tildePhi{1:D}(z) \approx K(x,z)$}
    \BlankLine
	Define the pairing $\pairing{x, \omega}$ from the \acs{LCA} group $(\mathcal{X}, \groupop)$\;
	Find a decomposition $(B(\omega)$,$\density(\omega))$ such that $B(\omega)B(\omega)^*\density(\omega)=\IFT{K_e}(\omega)$\;
% 	Build an randomized feature map via Monte-Carlo sampling from the probability measure $\mu$ and the application $B$\;
	Draw $D$ random vectors $\omega_j$, $j=1, \hdots, D$ from the probability distribution $\mu$\;
   \Return $\tildePhi{1:D}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x, \omega_j}B(\omega_j)^*$\;
   \caption{Construction of \acs{ORFF} from \acs{OVK}}
   \label{al:ORFF_construction}
\end{algorithm2e}
\end{center}

\subsection{Examples of Operator Random Fourier Feature maps}
We now give two examples of operator-valued random Fourier feature map when $\mathcal{Y}\subset\mathbb{R}^p$. First we introduce the general form of an approximated feature map for a matrix-valued kernel on the additive group $(\mathbb{R}^d,+)$.
\begin{example}[Matrix-valued kernel on the additive group]\label{ex:additive_group}
In the following, $K(x,z)=K_0(x-z)$ is a $\mathbb{R}^p$-Mercer matrix-valued kernel on $\mathcal{X}=\mathbb{R}^d$ invariant w.r.t. the group operation $+$. %
Then the function $\tildePhi{1:D}$ defined as follow is an \acl{ORFF} of $K_{0}$.
\begin{equation*}
\begin{aligned}
\tildePhi{1:D}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D\begin{pmatrix}\cos{\inner{x,\omega_j}}B(\omega_j)^* \\ \sin{\inner{x,\omega_j}}B(\omega_j)^*\end{pmatrix}, \enskip \omega_j \sim \mu.
\end{aligned}
\end{equation*}
\end{example}
\begin{proof}
The (Pontryagin) dual of $\mathcal{X}=\mathbb{R}^d$
is $\dual{\mathcal{X}}=\mathbb{R}^d$, and the duality pairing is $\pairing{x-z,\omega}=\exp(i\inner{x-z, \omega})$. The kernel approximation yields:
\begin{dmath*}
\tilde{K}(x,z)=\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)
= \frac{1}{D} \sum_{j=1}^D \begin{pmatrix} \cos{\inner{x,\omega_j}} & \sin{\inner{x,\omega_j}} \end{pmatrix}\begin{pmatrix}\cos{\inner{z,\omega_j}} \\ \sin{\inner{z,\omega_j}} \end{pmatrix} A(\omega_j)
= \frac{1}{D} \sum_{j=1}^D \cos{\inner{x-z,\omega_j}}A(\omega_j) \\
\converges{\asurely}{D\to\infty}\expectation_\mu\left[\cos{\inner{x-z,\omega}}A(\omega)\right]
\end{dmath*}
in the weak operator topology. Since for all $x\in\mathcal{X}$, $\sin(x, \cdot)$ is an odd function and $A(\cdot)p_\mu(\cdot)$ is even,
\begin{dmath*}
\expectation_\mu\left[\cos{\inner{x-z,\omega}}A(\omega)\right]=\expectation_\mu\left[\exp(-i\inner{x-z,\omega})A(\omega)\right]\hiderel{=}K(x,z).
\end{dmath*}
Hence $\tilde{K}(x,z)\converges{\asurely}{D\to\infty}K(x,z)$.
% which tends to $\expectation_{\mu}[\exp(-i\inner{x-z, \omega})A(\omega) ]=\expectation_{\mu}[\conj{\pairing{x-z,\omega}}A(\omega)]=K(x, z)$ when $D$ tends to infinity.
\end{proof}
The second example extends scalar-valued Random Fourier Features on the skewed multiplicative group described in \cref{eq:pairing-skewed}) to the operator-valued case.
\begin{example}[Matrix-valued kernel on the skewed multiplicative group]
In the following, $K(x,z)=K_{1-c}(x\odot z)$ is a $\mathbb{R}^p$-Mercer matrix-valued kernel on $\mathcal{X}=(-c;+\infty)^d$ invariant w.r.t. the group operation\mpar{The group operation $\odot$ is defined in \cref{sec:background}.} $\odot$. Then the function $\tildePhi{1:D}$ defined as follow is an \acl{ORFF} of $K_{1-c}$.
\begin{equation*}
\begin{aligned}
\tildePhi(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D\begin{pmatrix}\cos{\inner{\log(x+c),\omega_j}}B(\omega_j)^* \\ \sin{\inner{\log(x+c),\omega_j}}B(\omega_j)^*\end{pmatrix}, \enskip \omega_j \sim \mu.
\end{aligned}
\end{equation*}
\end{example}
\begin{proof}
The dual of $\mathcal{X}=(-c;+\infty)^d$
is $\dual{\mathcal{X}}=\mathbb{R}^d$, and the duality pairing is $\pairing{x \odot \myinv{z},\omega}=\exp(i\inner{\log(x\odot z^{-1}+c), \omega})$ (see \citet{Li2010}). Following the proof of \cref{ex:additive_group}, we have
\begin{equation*}
\begin{aligned}
\tilde{K}(x,z)&= \frac{1}{D} \sum\nolimits_{j=1}^D \cos{\inner*{\log\left(\frac{x+c}{z+c}\right), \omega_j}}A(\omega_j).
\end{aligned}
\end{equation*}
which converges almost surely to $\expectation_{\mu}[\exp(-i\inner*{\log(x\odot z^{-1}+c)})A(\omega) ]=\expectation_{\mu}[\conj{\pairing{x\odot z^{-1},\omega}}A(\omega)]=K(x, z)$ when $D$ tends to infinity.
\end{proof}

\clearpage
%----------------------------------------------------------------------------------------
\section{Learning with ORFF}
\label{sec:learning_with_operator-valued_random-fourier_features}
We now turn our attention to learning function with an ORFF model that approximate an OVK model. In particular we study the case of Ridge regression.
\subsection{Ridge regression}
Let $\mathcal{S} = \Set{(x_i,y_i) | \forall i\inrange{1}{N}, \forall x_i\in\mathcal{X}, \forall y_i\in\mathcal{Y}}$ be a collection of i.i.d training samples.
Given a local loss function $L: \mathcal{X}\times\mathcal{F}\times\mathcal{Y}\to \mathbb{R}_+$ such that $L$ is convex in $f$, we are interested in finding a \emph{vector-valued function} $f_*:\mathcal{X}\to\mathcal{Y}$, that lives in a RKHS and minimize a tradeoff between a data fitting term $L$ and a regularization term to prevent from overfitting. Namely finding $f_*\in\mathcal{H}_K$ such that
\begin{dmath}
f_* = \argmin_{f\in\mathcal{H}_K}  \frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, f, y_i) + \frac{\lambda}{2}\norm{f}^2_{K},
\label{eq:learning_rkhs}
\end{dmath}
where $\lambda\in\mathbb{R}_+$ is a regularization\mpar{Tychonov regularization.} parameter. A common choice of data fitting term for regression is $L:(x_i, f, y_i) \mapsto \norm{f(x_i)-y_i}_{\mathcal{Y}}^2$.
\begin{remark}
\label{rk:rkhs_bound}
If for all $x_i\in\mathcal{X}$ and all $y_i\in\mathcal{Y}$ we have $L(x_i,0,y_i)\le 2\sigma_y^2$, then $\lambda\norm{f_*}_K\le\sigma_y^2$. Indeed, since $\mathcal{H}_K$ is a Hilbert space, $0\in\mathcal{H}_K$. Moreover $L$ is convex with respect to $f$, thus
\begin{dmath*}
\frac{\lambda}{2}\norm{f_*}^2_{K} \le \frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, f_*, y_i) + \frac{\lambda}{2}\norm{f_*}^2_{K}
\le \frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, 0, y_i) \hiderel{\le} \sigma_y^2
\end{dmath*}
\end{remark}
We now introduce a corollary from \citet{kurdila2006convex} showing that \cref{eq:learning_rkhs} attains a unique mimimizer.
\begin{corollary}
\label{cor:unique_minimizer}
Let $\mathcal{H}$ be a Hilbert Space and $J:\mathcal{H}\to \mathbb{R}$ is a strongly lower semi-continuous convex and coercive function. Then $J$ is bounded from below and attains a minimizer \cite{kadri2015operator}.
\end{corollary}
This is easily verified for Ridge regression. Define
\begin{dmath*}
J_\lambda(f)=\frac{1}{2N}\sum_{i=1}^N\norm{f(x_i)-y_i}_{\mathcal{Y}}^2+\frac{\lambda}{2}\norm{f}_K^2,
\end{dmath*}
where $f\in\mathcal{H}_K$ and $\lambda\in\mathbb{R}_{>0}$. $J_\lambda$ is continuous and strongly convex. Additionally $J_\lambda$ is coercive since $\norm{f}_K$ is coercive, $\lambda>0$, and all the summands of $J_\lambda$ are positive. Hence for all positive $\lambda$, $f_*=\argmin_{f\in\mathcal{H}_K}J_\lambda(f)$ exists, is unique and attained.
\paragraph{}
Regression in \acl{vv-RKHS} has been well studied and a cornerstone of learning in \acs{vv-RKHS} is the representer theorem\mpar{Sometimes referred to as minimal norm interpolation theorem}.
\begin{theorem}[Representer]
Let $K$ be a $\mathcal{Y}$-Mercer Kernel and $\mathcal{H}_K$ its corresponding \acl{vv-RKHS}. The solution $f_*\in\mathcal{H}_K$ of the regularized optimization problem 
\begin{dmath}
\label{eq:argmin_RKHS}
f_* = \argmin_{f\in\mathcal{H}_K}  \frac{1}{2N}\displaystyle\sum_{i=1}^N\norm{f(x_i) - y_i}_{\mathcal{Y}}^2 + \frac{\lambda}{2}\norm{f}^2_{K},
\label{eq:learning_rkhs}
\end{dmath}
has the form $f_*=\sum_{i=1}^NK(\cdot,x_i)u_i$, where $u_i\in\mathcal{Y}$.
\end{theorem}
This theorem was first introduced by \citet{Wahba90} in the case where $\mathcal{Y}=\mathbb{R}$. The extension to an arbitrary Hilbert space $\mathcal{Y}$ has been proved by many authors \citep{Brouard2011,kadri2015operator,Micchelli2005}. We present here the proof proposed by \citet{kadri2015operator}.
\begin{proof}
Let $J(f)=\sum_{i=1}^N\norm{f(x_i) - y_i}_{\mathcal{Y}}^2 + \frac{\lambda}{2}\norm{f}^2_{K}$ be the functional to minimize. We use the notion of Frechet derivative, noted $D_F$. This is the strongest notion of derivative in a normed vector space, see \citet{kurdila2006convex}.
$f_*$ denotes the optimal vector in $\mathcal{H}_K$ such§ that $f_*=\argmin_{f\in\mathcal{H}_K}J(f){\scriptstyle\implies}D_F J(f_*)=0$. To compute the Frechet derivative $D_F$, we use the Gateaux derivative $D_G$ of $J$ with respect to $f\in\mathcal{H}_K$ in the direction $h\in\mathcal{H}_K$. The Gateaux derivative is defined by
\begin{dmath*}
D_G J(f, h) = \lim_{t\to 0} \frac{J(f+th)-J(f)}{t}.
\end{dmath*}
Let $G_i(f)=\norm{f(x_i)-y_i}_{\mathcal{Y}}^2$. Then $J(f)=\frac{1}{2N}\sum_{i=1}^NG_i(f)+\frac{\lambda}{2}\norm{f}_{K}^2$. Moreover the directional Gateaux derivative obey $D_GJ(f, h)=\inner{D_GJ(f),h}_{K}$. Thus
\begin{enumerate}
\item Regularization term:
\begin{dmath}
\label{enum:Gateaux_diff_reg}
\lim_{t\to 0}\frac{\norm{f+th}^2_{K}-\norm{f}_{K}^2}{t}=2\inner{f,h}{\scriptstyle\implies}D_G\norm{f}_{K}^2\hiderel{=}2f.
\end{dmath}
\item Data fitting term:
\begin{dmath}
\label{enum:Gateaux_diff_fit}
\lim_{t\to 0}\frac{\norm{y_i-f(x_i)+th(x_i)}_{\mathcal{Y}}^2-\norm{y_i-f(x_i)}_{\mathcal{Y}}^2}{t} = -2\inner{y_i-f(x_i),h(x_i)}_{\mathcal{Y}} \hiderel{=} -2\inner{K_{x_i}(y_i-f(x_i)),h}_{K}\\{\scriptstyle\implies}D_G G_i(f)\hiderel{=}-2K_{x_i}(y_i-f(x_i)).
\end{dmath}
\end{enumerate}
Since $K$ is Mercer, $f$ is continuous. Applying corollary 4.1.1 in \citet{kurdila2006convex} shows that $J$ is Frechet differentiable and $\forall f\in\mathcal{H}_K$, $D_F J(f)=D_G J(f)$. Thus if $f_*=\argmin_{f\in\mathcal{H}_K}J(f)$ then $D_F J(f)=0$ so $D_G J(f)=0$. Eventually use \cref{enum:Gateaux_diff_reg} and \cref{enum:Gateaux_diff_fit} so that
\begin{dmath*}
D_G J(f_*)= \lambda f_*-\frac{1}{N}\sum_{i=1}^NK_{x_i}(y_i-f_*(x_i))\hiderel{=}0
\end{dmath*}
Let $u_i=\frac{(y_i-f_*(x_i)}{\lambda N}\in\mathcal{Y}$ to conclude the proof.
\end{proof}
The representer theorem show that minimizing a functional in a \acs{vv-RKHS} yields a solution which depends on all the points in the training set. Assuming that for all $x_i$, $x\in\mathcal{X}$ and for all $u_i\in\mathcal{Y}$ it takes time $O(P)$, to compute $K(x_i, x)u_i$, making a prediction using the representer theorem take $O(NP)$. Obviously If $\mathcal{Y}=\mathbb{R}^p$, Then $P=O(p^2)$ thus making a prediction cost $O(Np^2)$ operations.
\paragraph{}
Instead learning a model $f$ that depends on all the points of the training set, we would like to learn a parametric model of the form
$\tildef{1:D}(x)=\tildePhi{1:D}(x)^*\theta$, where $\theta$ lives in some redescription space\mpar{If $\mathcal{Y}=\mathbb{R}$ then $\mathcal{Y}'=\mathbb{R}$ thus $\theta=\mathbb{R}^D$ endowed with the euclidean inner product.} $\mathcal{H}^D\left(\mathcal{Y}'\right)\colonequals\Vect_{j=1}^D\mathcal{Y}'$. We are interested in finding a parameter vector $\theta_*$ such that
\begin{dmath}
\label{eq:argmin_applied}
\theta_*=\argmin_{\theta\in\mathcal{H}^D\left(\mathcal{Y}'\right)} \frac{1}{2N}\sum_{i=1}^N\norm{\tildePhi{1:D}(x_i)^\adjoint \theta-y_i}_{\mathcal{Y}}^2 + \frac{\lambda}{2D}\norm{\theta}^2_{\mathcal{H}^D\left(\mathcal{Y}'\right)}
\end{dmath}
Since $\theta\in\mathcal{H}^D\left(\mathcal{Y}'\right)$ lives in a different space than $f\in\mathcal{H}_K$ is makes no sense to show the convergence of $\theta_*$ to $f_*$. However we can verify that $\tildef{1:D}$ converges to $f\in\mathcal{H}_K$ pointwise. We first introduce the following lemma and assumptions. First we characterize extremum estimators. Let $\mathcal{H}$ be any Hilbert space.
\begin{assumption}[EE]
\label{ass:ee}
The parameter $\tilde{g}_*\in\mathcal{H}$ is an extremum estimator of $Q_D$ if $\tilde{g}_*=\argmin_{g\in\mathcal{H}} Q_D(g)$.
\end{assumption}
To derive consistency we must ensure that the stochastic objective $Q_D$ function converges uniformly-weakly to some target function $Q_0$. Let $B(g,\epsilon)$ be the open ball in $\mathcal{H}$ with radius $\epsilon$ and $\mathcal{H}\setminus{B(g,\epsilon)}$ denotes its complement in $\mathcal{H}$.
\begin{assumption}[U-SCON]
\label{ass:uwcon}
A \emph{stochastic} function $Q_D$ converges uniformly-strongly to some \emph{non-stochastic} function $Q_0$ if \begin{dmath*}
\sup_{g\in\mathcal{H}}\abs{Q_D(g)-Q_0(g)}\converges{\asurely}{D\to\infty}0
\end{dmath*}
\end{assumption}
This is equivalent to say that $Q_D$ converges to $Q_0$ in probability in the uniform norm topology. Eventually the mimizers of $Q$ must be uniquely identifiable.
\begin{assumption}[ID] 
\label{ass:id}
A function has unique identifiable minimizer if there exist $g_*\in\mathcal{H}$ such that for all $\epsilon\ge 0$, $\inf_{g\in \mathcal{H}\setminus{B(g_*,\epsilon)}} Q(g)>Q(g_*)$.
\end{assumption}
Note that if $Q$ is a strictly convex function of $g$, then \cref{ass:id} holds. These three assumptions combined yields the consistency of an estimator.
\begin{proposition} If
\cref{ass:ee}, \cref{ass:uwcon} and \cref{ass:id} hold then $\tilde{g}_* \converges{\asurely}{D\to\infty} g_*.$
\end{proposition}
\begin{proof}

\end{proof}
We are now ready to prove the consistency of ORFF with respect to the minimization in the RKHS, that is if we replace the model $f(x)=\sum_{i=1}^NK(\cdot, x_i)c_i$ by a model $f(x)=\tildePhi{1:D}(x)^\adjoint \theta$ and find the minimizer of the ridge regression problem, then when $D$ tends to infinity, both model return the \say{same} function.
\begin{proposition}[Consitensy of ORFF \wrt~\acs{vv-RKHS}] Let $K$ be a $\mathcal{Y}$-Mercer Kernel. If $\theta_*\in\mathcal{H}^D\left(\mathcal{Y}'\right)$ is the minimizer of \cref{eq:argmin_applied} and $f_*\in\mathcal{H}_K$ is the minimizer of \cref{eq:argmin_RKHS}. Then 
\begin{dmath}
\tildePhi{1:D}(x)^\adjoint\theta_* \converges{\asurely}{D\to\infty}f_*(x),
\end{dmath}
for all $x\in\mathcal{X}$.
\end{proposition}
\begin{proof}
% Let $(Wg)(x)=\Phi_x^\adjoint g$ be the feature operator associated to $\Phi$ and $\tildePhi{1:D}(x)y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D(\Phi_x y)(\omega_j)$, where $\omega_j$ are \iid~random vectors following the law $\mu$. Since $W$ is a partial isometry

% We have that for all $f\in\mathcal{H_K}$, there exists a $g\in\mathcal{H}$ such that $(W^\adjoint f)(x)=g(x)=\Phi(x)^\adjoint g$ where $W$ is a partial isometry with initial subspace $(\Ker W)^\perp\subset\mathcal{H}$ and final subspace $\mathcal{H}_K$. 

% \begin{dmath}
% \label{eq:argmin_HS}
% g_*=\argmin_{g\in(\Ker W)^\perp\subset\mathcal{H}} \frac{1}{2N}\sum_{i=1}^N\norm{(Wg)(x_i)-y_i}_{\mathcal{Y}}^2 + \frac{\lambda}{2}\norm{g}^2_{\mathcal{H}}
% \end{dmath}

Define
\begin{dmath}
Q_0(g)=\frac{1}{2N}\sum_{i=1}^N\norm{(Wg)(x_i)-y_i}_{\mathcal{Y}}^2 + \frac{\lambda}{2}\norm{g}^2_{\mathcal{H}}.
\end{dmath}
Moreover let $\tildePhi{1:D}(x_i)y=\Vect_{j=1}^D(\Phi_{x_i}y)(\omega_j)$ for all $y\in\mathcal{Y}$, and $\theta_g=\vect_{j=1}^Dg(\omega_j)$, where $\omega_j$ are \iid~random vectors following a law $\mu$. We define the empirical estimator of $Q_0$ as
\begin{dmath}
Q_D(g)=\frac{1}{2N}\sum_{i=1}^N\norm{\tildePhi{1:D}(x_i)^\adjoint \theta_g-y_i}_{\mathcal{Y}}^2 + \frac{\lambda}{2D}\norm{\theta_g}^2_{\mathcal{H}^D\left(\mathcal{Y}'\right)}
\end{dmath}
Let $g_*=\argmin_{g\in\mathcal{H}}Q_0(g)$ and $\tilde{g}_*=\argmin_{g\in\mathcal{H}}Q_D(g)$ thus $\tilde{g}_*$ is an extremum estimator (\cref{ass:ee} holds). Notice that since $W$ is a bounded linear application, $Q_0$ is continuous strongly convex and coercive. By \cref{cor:unique_minimizer} attains an identifiable unique minimizer (\cref{ass:id} holds).

Since W is bounded, $\Ker W$ is closed, so that we can perform the decomposition $\mathcal{H}=(\Ker W)^\perp\oplus \Ker W$. Then clearly by linearity of $W$ and the fact that for all $g\in\Ker W$, $Wg=0$,
\begin{dmath*}
g_*=\argmin_{g\in\mathcal{H}}Q_0(g)
\hiderel{=}\argmin_{g\in(\Ker W)^\perp\oplus \Ker W}Q_0(g)
\hiderel{=}\argmin_{g\in(\Ker W)^\perp}Q_0(g).
\end{dmath*}
Besides with similar arguments as in \cref{rk:rkhs_bound} we deduce that $\lambda\norm{g_*}_{\mathcal{H}}^2$ $\le$ $2\sigma^2_y$. As a result $g_*\in\mathcal{H}_\lambda\colonequals\Set{g\in(\Ker W)^\perp|\lambda\norm{g}_{\mathcal{H}}^2\le 2\sigma^2_y}$. Eventually
\begin{dmath*}
g_*=\argmin_{g\in\mathcal{H}}Q_0(g)\hiderel{=}\argmin_{g\in\mathcal{H}_\lambda}Q_0(g)
\end{dmath*}
% \begin{enumerate}
% \item By assumption $\sup_{y\in\mathcal{Y}} \norm{y}\le \sigma_y$ thus from \cref{rk:rkhs_bound} we have that $\lambda\norm{f_*}^2_K$ is bounded by $2\sigma_y^2$ and so is $\lambda\norm{g_*}_{\mathcal{H}}^2$. We note $\mathcal{H}_K^{\lambda}=\Set{f\in\mathcal{H}_K|\lambda\norm{f}^2_K\le 2\sigma_y}$ and $\mathcal{H}^\lambda=\Set{g\in(\Ker W)^\perp|\lambda\norm{g}^2_{\mathcal{H}}\le 2\sigma_y}$.
% Then 
% \begin{dmath*}
% f_*=\argmin_{f\in\mathcal{H}_K} \frac{1}{2N}\displaystyle\sum_{i=1}^N\norm{f(x_i) - y_i}_{\mathcal{Y}}^2 + \frac{\lambda}{2}\norm{f}^2_{\mathcal{H}_K}
% =\argmin_{f\in\mathcal{H}_K^\lambda} \frac{1}{2N}\displaystyle\sum_{i=1}^N\norm{f(x_i) - y_i}_{\mathcal{Y}}^2 + \frac{\lambda}{2}\norm{f}^2_{\mathcal{H}_K}
% \end{dmath*}
% and $g_*=\argmin_{g\in(Ker W)^\perp} Q_0(g) 
% \hiderel{=}\argmin_{g\in\mathcal{H}^\lambda} Q_0(g)$.
% Besides the inclusion $\iota_K:\mathcal{H}_K\hookrightarrow\mathcal{C}(\mathcal{X};\mathcal{Y})$ is compact, thus the closed set $\mathcal{H}_K^{\lambda}=\Set{f\in\mathcal{H}_K|\lambda\norm{f}^2_K\le 2\sigma_y}$ is compact with respect to the compact-open topology. $W$ is a partial isometry with initial space $(\Ker W)^\perp$, the restriction $W|_{(\Ker W)^\perp}:(\Ker W)^\perp\to\mathcal{H}_K$ is an isometry. Thus the set $\mathcal{H}^\lambda=\Set{g\in(\Ker W)^\perp|\lambda\norm{g}^2_{\mathcal{H}}\le 2\sigma_y}=(W|_{(\Ker W)^\perp})^\adjoint \mathcal{H}_K^{\lambda}$ is compact.
% \item It is easy to note that $Q_0$ is continuous and measureable. Besides since $f_*$ is unique and $W|_{(\Ker W)^\perp}$ is an isometry, $Q_0$ has unique minimizer $g_*$ such that $W|_{(\Ker W)^\perp}g_*=f_*$.
% \item Suppose that $\norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}}$ is bounded for all $\omega\in\dual{\mathcal{X}}$ as a consequence $\sup_{g\in\mathcal{H}^\lambda} \abs{Q_D(g)-Q_0(g)}$ is bounded thus from the strong law of large numbers $Q_D(g)\converges{\asurely}{D\to\infty}Q_0(g)$ uniformly.
% \end{enumerate}
% As a result, the extremum estimator $Q_D$ is consistent 
% \begin{dmath}
% \label{eq:consitency1}
% \tilde{g}_* \converges{\proba}{D\to\infty} g_*
% \end{dmath}
% Let $\theta_{g,*}=\vect_{j=1}^D\tilde{g}_*(\omega_j)$. since $\tilde{g}_*$ minimizes $Q_D$, $\theta_{g,*}$ minimizes \cref{eq:argmin_applied}. Besides from \cref{pr:phitilde_phi_rel} we have
% \begin{dmath}
% \tilde{W}\theta_{g,*} \converges{\asurely}{D\to\infty} W\tilde{g}_*.
% \end{dmath}
% Finally \cref{eq:consitency1} shows that $W\tilde{g}_*\converges{\proba}{D\to\infty}Wg_*=f_*$, hence
% \begin{dmath*}
% \tildePhi{1:D}\theta_{g,*}\hiderel{=}\tilde{W}\theta_{g,*}\converges{\proba}{D\to\infty} f_*\hiderel{\in}\mathcal{H}_K.
% \end{dmath*}
% with $\theta_{g,*}=\theta_*$ defined in \cref{eq:argmin_RKHS}.
\end{proof}

\clearpage

%----------------------------------------------------------------------------------------
% \section{Consistency and generalization bounds}
% \label{sec:consistency and generalization bounds}
% \subsection{Operator-valued kernels}
% In this section are interested in finding a minimizer $f_*:\mathcal{X}\to\mathcal{Y}$, where $\mathcal{X}$ is a Polish space and $\mathcal{Y}$ a separable Hilbert space such that for all $x_i$ in $\mathcal{X}$ and all $y_i$ in $\mathcal{Y}$,
% \begin{dmath}
% f_* = \argmin_{f\in\mathcal{H}_K} \sum_{i=1}^NL_{y_i}(f(x_i)) \hiderel{=} \argmin_{f\in\mathcal{H}_K} \mathcal{R}_{emp}(f, L),
% \label{eq:pbOVK}
% \end{dmath}
% where $\mathcal{H}_K$ is a \acs{vv-RKHS} and $\forall y_i \in \mathcal{Y}$, $L_{y_i}$ a $L$-Lipschitz cost function. How does a function trained as in \cref{eq:pbOVK} generalizes on a test set?
% \begin{proposition} Suppose that $f\in\mathcal{H}_K$ a \acs{vv-RKHS} where $\sup_{x\in\mathcal{X}} \Tr[K(x,x)] < T$ and $\norm{f}_{\mathcal{H}_K}<B$. Moreover let $L:\mathcal{Y}\times \mathcal{Y}\to[0, C]$ be a $L$-Lipschitz cost function. Then if we are given $N$ training points, we have with at least probability $1-2\delta$ that
% \begin{dmath}
% \mathcal{R}_{true}(f, L) \le \mathcal{R}_{emp}(f, L)  + 2\sqrt{\frac{2}{N}}\left( LBT^{1/2} + \frac{3C}{4}\sqrt{\ln(1/\delta)}\right).
% \end{dmath}
% \label{pr:ovk_gen}
% \end{proposition}
% \begin{proof}
% This proof is due to Mauer \mpar{See \url{https://arxiv.org/pdf/1605.00251.pdf}}. We do not claim any originality for this proof. First let us introduce the notion of Rade\-macher complexity of a class of function $F$.
% \begin{definition}
% Let $\mathcal{X}$ be any set. Let $\epsilon_1$, $\hdots$, $\epsilon_N$ be $N$ independent Rade\-macher random variables, identically uniformly distributed on $\{-1;1\}$. For any class of functions $F:~\mathcal{X}\to\mathbb{R}$, then for all $x_1, \hdots x_N\in \mathcal{X}$ the quantity
% \begin{dmath}
% \mathcal{R}_N(F) \colonequals \expectation\left[ \sup_{f\in F} \sum_{i=1}^N \epsilon_i f(x_i) \right]
% \end{dmath}
% is called Rademacher complexity of the class $F$.
% \end{definition}
% In generalization bounds the Rademacher complexity of a class of function often involves a composition between a target function to be learn and a cost function, part of the risk we want to minimize. The idea is to bound the Rademacher complexity with a term that does not depends on the cost function, but only on the target function. Such a bound has be recently proposed by Mauer:
% \begin{proposition}
% \label{pr:radswap}
% Let $\mathcal{X}$ be any set, $x_1, \hdots, x_N$ in $\mathcal{X}$, let $F$ be a class of function $f:~\mathcal{X}\to\mathcal{Y}$ and $h_i:~\mathcal{Y}\to\mathbb{R}$ be a $L$-Lipschitz function, where $\mathcal{Y}$ is a second countable Hilbert space endowed with euclidean inner product. Then
% \begin{dmath}
% \expectation \sup_{f\in F} \sum_{i=1}^N \epsilon_i h_i(x_i) \le \sqrt{2}L\sup_{f\in F}\sum_{i=1, k}^{i=N} \epsilon_{ik}f_k(x_i),
% \end{dmath}
% where $\epsilon_{ik}$ is a doubly indexed independent Rademacher sequence and $f_k(x_i)$ is the $k$-th component of $f(x_i)$
% \end{proposition}
% From now on we consider functions $f\in\mathcal{H}_K$ a vv-RKHS. Then there exists an induced feature-map $\Phi:~\mathcal{X}\to \mathcal{L}(\mathcal{Y}, \mathcal{H})$ such that for all $y, y'\in\mathcal{Y}$ the kernel is given by
% \begin{dmath}
% \inner{y, K(x,z)y'} = \inner{\Phi_xy, \Phi_zy'}.
% \end{dmath}
% We say that the feature space $\mathcal{H}$ is embed into the RKHS $\mathcal{H}_K$ by mean of the \emph{feature operator} $(W\theta)(x):=(\Phi_x^*\theta)$. Indeed $W$ defines a partial isometry between $\mathcal{H}$ and $\mathcal{H}_K$. Remember that $\mathcal{Y}$ a is supposed to be a separable Hilbert space so it has a countable basis and let the class of $\mathcal{Y}$-valued functions $F$ be
% \begin{dmath}
% F=\Set{ f | f: x \mapsto \Phi_x^* \theta, \enskip \norm{\theta} < B }\hiderel{\subset} \mathcal{H}_K.
% \end{dmath}
% Then from \cref{pr:radswap} and if $K$ is trace class, we have
% \begin{dmath}
% \label{eq:ker_bound}
% \expectation \sup_{\norm{\theta}<B} \sum_{i=1}^N \epsilon_i L_{y_i}(\Phi_{x_i}^* \theta) \le \sqrt{2}L\expectation \sup_{\norm{\theta}<B} \sum_{i=1,k}^{i=N}\epsilon_{ik}\inner{\Phi_{x_i} \theta, e_k}
% = \sqrt{2}L\expectation \sup_{\norm{\theta}<B} \inner*{ \theta, \sum_{i,k}^{i=N}\epsilon_{ik}\Phi_{x_i}e_k} 
% \le \sqrt{2}LB\expectation \norm{\sum_{i=1,k}^{i=N}\epsilon_{ik}\Phi_{x_i}e_k} 
% \le \sqrt{2}LB\sqrt{\sum_{i=1,k}^{i=N} \norm{\Phi_{x_i}e_k}^2 }
% \le \sqrt{2}LB\sqrt{\sum_{i=1}^{i=N} \Tr\left[ K(x_i, x_i) \right] }.
% \end{dmath}
% From \url{http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf}:
% \begin{theorem}
% \label{th:gen_rad_bound}
% Let $\mathcal{X}$ be any set, $F$ a class of functions $f:\mathcal{X}\to[0, C]$ and let $X_1, \hdots, X_N$ be a sequence of iid random variable with value in $\mathcal{X}$. Then for $\delta > 0$, with probability at least $1-\delta$, we have for all $f\in F$ that
% \begin{dmath*}
% \expectation f(X) \le \frac{1}{N} \sum_{i=1}^Nf(X_i) + \frac{2}{N}\mathcal{R}_N(F) + C\sqrt{\frac{9\ln(2/\delta)}{2N}}
% \end{dmath*}
% \end{theorem}
% Conclude by plugin \cref{eq:ker_bound} in \cref{th:gen_rad_bound}.
% \end{proof}

% \subsection{Operator-valued Random Fourier Features}
% Suppose we want to learn an approximate function such that for all $x\in\mathcal{X}$, $\tilde{f}_*(x) \approx f_*(x)$, where $f_*$ is defined as in \cref{eq:pbOVK}. In this section we suppose that $K$ is a shift invariant $\mathcal{Y}$-Mercer kernel on the \acs{LCA} group $(\mathcal{X},\star)$. Brault et al. showed that such a Kernel can be written for all $y$, $y'$ in $\mathcal{Y}$
% \begin{dmath*}
% \inner{\tilde{\Phi}(x)y, \tilde{\Phi}(z)y'}\approx \inner{y, K(x,z)y'}
% \end{dmath*}
% where 
% \begin{dmath*}
% \tilde{\Phi}(x)=\Vect_{j=1}^D \exp\left( -i\inner{x, \omega_j}B(\omega_j) \right), \enskip \omega_j \hiderel{\sim} \mu
% \label{eq:ORFF}
% \end{dmath*}
% such that $\inner{y, B(\omega)B(\omega)^*y'}\frac{d\mu}{d\omega}=\FT{\inner{y, K(\cdot y')})}(\omega)$. We are interested in solving \cref{eq:pbOVK} with the feature map defined from the kernel approximation defined in \cref{eq:ORFF}. Namely:
% \begin{dmath*}
% f_* = \argmin_{f\in\tilde{F}} \sum_{i=1}^NL_{y_i}(\tilde{f}(x_i))
% \hiderel{=} \argmin_{f\in\tilde{F}} \mathcal{R}_{emp}(\tilde{f}, L)
% = \argmin_{\theta\in\mathcal{Y}^D} \sum_{i=1}^NL_{y_i}(\tilde{\Phi}(x_i)^*\theta).
% \end{dmath*}
% \begin{proposition} Suppose that $\Tr[\tilde{K}_e(0)] \le T$ and $\norm{\theta}_2<\frac{B}{D}$. Let $L:\mathcal{Y}\times \mathcal{Y}\to[0, C]$ be a $L$-Lipschitz cost function. Then if we are given $N$ training points, we have with at least probability $1-2\delta$ that
% \begin{dmath*}
% \mathcal{R}_{true}(\tilde{f}, L) - \min_{f\in F}\mathcal{R}_{true}(f, L) \le O\left(\underbrace{\frac{1}{\sqrt{N}}\left(LBT^{1/2}+\frac{3C}{4}\sqrt{\ln\left(\frac{1}{\delta}\right)}\right)}_{\text{estimation error}}+\underbrace{\frac{LB}{\sqrt{D}}\left(1+\sqrt{\ln\left(\frac{1}{\delta}\right)}\right)}_{\text{approximation error}}\right)
% \end{dmath*}
% \label{pr:ovk_gen}
% \end{proposition}



% \begin{proof}
% We follow the proof of \url{https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf}. It is an adaptation of the original proof in the light of the recent results of Mauer. We first define the two following sets:
% \begin{dmath}
% F=\Set{ f | f:\enskip x \mapsto \Phi_x^* \theta, \enskip \norm{\theta(\omega)} < Bp_\mu(\omega) }\hiderel{\subset} \mathcal{H}_K.
% \end{dmath}
% and
% \begin{dmath}
% \tilde{F}=\Set{ f | f: x \mapsto \tilde{\Phi}_x^* \theta, \enskip \norm{\theta_j} < \frac{B}{D}}\hiderel{\subset} \mathcal{H}.
% \end{dmath}
% \begin{proposition}[Existence of approximation function] Let $\nu$ be a measure on $\mathcal{X}$, and $f_*$ a function in $F$. If $\omega_1, \hdots, \omega_D$ are drawn i.i.d. from $\mu$ then with probability at least $1 - \delta$, there exists a function $\tilde{f}\in \tilde{F}$ such that
% \begin{dmath}
% \sqrt{\int_{\mathcal{X}}\norm{f(x)-\tilde{f}(x)}^2_{\mathcal{Y}}d\mu(x)}\le \frac{B}{\sqrt{D}}\left(1 + \sqrt{2\log(1/\delta)} \right)
% \end{dmath}
% \label{pr:existence_app}
% \end{proposition}

% We use the following lemma of Rahimi and Recht:
% \begin{lemma}
% \label{lm:concentration_hilbert}
% Let $X_1, \hdots X_D$ be random variables with value in a ball $\mathcal{H}$ with radius $M$ centered around the origin in a Hilbert space. Denote the sample average $\bar{X}=\frac{1}{D}\sum_{j=1}^DX_j$. Then with probability $1-\delta$,
% \begin{dmath}
% \norm{\bar{X}-\expectation\bar{X}}\le \frac{M}{\sqrt{D}}\left(1 + \sqrt{2\log(1/\delta)} \right).
% \end{dmath}
% \end{lemma}
% \begin{proof}
% See \url{https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf}, lemma 4 of the appendix.
% \end{proof}
% Since $f_*\in F$, we can write $f_*(x)=\int_{\omega\in\dual{\mathcal{X}}}\pairing{x,\omega}A(\omega)\theta(\omega)d\omega$. Construct the functions $f_j=\pairing{\cdot, \omega_j}A(\omega_j)\theta_j$, $j=1,\dots, D$ with $\theta_j:=\theta(\omega_j)/p_\mu(\omega_j)$. Let $\tilde{f}=\frac{1}{D}\sum_{j=1}^Df_j$ be the sample average of these functions. Then $\tilde{f}\in \tilde{F}$ because $\norm{\theta_j}/D\le B/D$. Also under the inner product $\inner{f,g}=\int_{\mathcal{X}} f(x)g(x)d\mu(x)$ we have that $\norm{\pairing{\cdot, \omega_j}A(\omega_j)\theta_j}\le B$. The claim follow by applying \cref{lm:concentration_hilbert} to $f_1, \hdots f_D$ under this inner product.
% \end{proof}

% \begin{proposition}[Bound on the approximation error] Suppose that $f\in F$ and $L_{y_i}$ is a $L$-Lipschitz cost function. With probability $1-\delta$ there exist a function $\tilde{f}\in \tilde{F}$ such that
% \begin{dmath}
% \mathcal{R}(\tilde{f}, L_{y_i})\le\mathcal{R}(f, L_{y_i}) + \frac{LB}{\sqrt{D}}\left(1 + \sqrt{2\log(1/\delta)} \right)
% \end{dmath}
% \label{pr:bound_app_error}
% \end{proposition}
% \begin{proof} For any two functions $f$ and $g$, the Lipschitz condition on $L_{y_i}$ followed by the concavity of square root gives
% \begin{dmath}
% \mathcal{R}(f, L) - \mathcal{R}(f, L) = \expectation \left[L(f(x)) - L(g(x))\right]
% \le \expectation \norm{L(f(x)) - L(g(x))}
% \le L \expectation \norm{f(x) - g(x)}
% \le L \sqrt{\expectation \norm{f(x) - g(x)}^2}
% \end{dmath}
% Eventually apply \cref{pr:existence_app} to conclude.
% \end{proof}
% \begin{proposition}[Bound on the estimation error] 
% \label{pr:bound_est_error}
% Suppose $L: \mathcal{Y}\times\mathcal{Y}\to [0; C]$ is $L$-Lipschitz. Let $\omega_1, \hdots, \omega_D$ be fixed. If $\{(x_i, y_i)\}\subset(\mathcal{X}\times \mathcal{Y})^N$ are drawn i.i.d. from a fixed distribution, then for $\delta>0$ with probability $1-\delta$ over the dataset we have
% \begin{dmath}
% \forall\tilde{f}\in\mathcal{F}, \enskip \mathcal{R}_{true}(\tilde{f}, L) \le \mathcal{R}_{emp}(\tilde{f}, L) + \frac{1}{\sqrt{N}}\left( BL\sqrt{2\Tr\left[ \tilde{K}_e(0) \right]}+C\sqrt{\frac{9\ln(2/\delta)}{2}}\right)
% \end{dmath}
% \end{proposition}
% \begin{proof}
% Apply \cref{th:gen_rad_bound}. We get
% \begin{equation}
% \mathcal{R}_{true}(f, L) \le \mathcal{R}_{emp}(f, L) + \frac{2}{N}\mathcal{R}_N(F) + C\sqrt{\frac{9\ln(2/\delta)}{2N}}.
% \label{eq:est_bound}
% \end{dmath}
% Since $f\in F$, $\forall j\inrange{1}{D},\enskip\norm{\theta_j}_\mathcal{Y}<\frac{B}{D}$; and recall that $\tilde{\Phi}(x)=\vect_{j=1}^D\Phi_x(\omega_j)$. Now with similar arguments as in \cref{eq:ker_bound} we have
% \begin{dmath}
% \mathcal{R}_N(F) \le \sqrt{2}BL\sqrt{N\Tr\left[K(0)\right]}
% \end{dmath}
% Plugin back in \cref{eq:est_bound} yields the desired result.
% \end{proof}

\clearpage
%----------------------------------------------------------------------------------------
\section{Conclusions}
\label{sec:conclusions}

\chapterend
