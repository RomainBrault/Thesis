% Colophon (a brief description of publication or production notes relevant to the edition)

\pagestyle{empty}

\pdfbookmark[0]{Colophon}{colophon}

\begin{center}
\textbf{\spacedlowsmallcaps{\mySubtitle}}
\end{center}

\footnotesize
\paragraph{Abstact:} In this thesis we study scalable methods to perform regression with \emph{\acl{OVK}s}. In the context of scalar-kernel learning, a kernel can be seen a a similarity measure between two data point. A solution of the learning problem has the form of a linear combination of theses similarities with respect to unknown weights to be found in order to have the best \say{fit} of the data. When dealing with \acl{OVK}s, the evalution of the kernel is no longer a scalar similarity, but a function acting on vectors. A solution is then a linear combination of operators with respect to vector weights.

Although \acl{OVK}s generalize strictly scalar-valued kernels, they have the disadvantage of being expensive in memory and time. Thus these methods seems not to be fitted to the current applications where data are often massive, and we want to process all of them to obtain the best possible model. To overcome this scalability issue, we replace the \acl{OVK} by an equivalent stochastic approximation of the kernel called feature map. We featurize the input data with a stochastic function such that an inner product on two featurized data is equivalent to the \acl{OVK} evaluation.

First we generalize the construction of these stochastic feature map --based on a generalization of Bochner's theorem for shift-invariant Mercer kernels-- and study their properties along with the complexity of the algorithms based on them. Then we show theoretical guarantees by bounding the error due to the approximation with high probabiity. We conclude by application to functional data analysis, multitask learning, time serie regression and compare the proposed framework with other state of the art methods.

\paragraph{R\'esum\'e:} Dans cette th\`ese nous nous int\'eressons au passage \`a l'\'echelle des m\'ethodes de r\'egression \`a \emph{noyaux \`a valeurs op\'erateurs}. Dans le contexte des noyaux scalaires, un noyau peut \^etre vu comme un mesure de similarit\'e entre deux points. Une solution d'un probl\`eme d'apprentissage prend alors g\'en\'eralement la forme d\ une combinaison lin\'eaire de ces similarit\'es o\`u les pond\'erations sont les inconnus \`a d\'eterminer afin de trouver un mod\`ele correspondant le \say{mieux} au donn\'ees. Dans le cadre des noyaux \`a noyaux \`a valeurs op\'erateurs l'\'evaluation d'un noyau entre deux points n'est plus une similarit\'e scalaire, mais une fonction agissant sur des vecteurs. Une solution s'exprime alors comme une combinaison lin\'eaire d'op\'erateurs vis \`a vis de poids vectoriels.

Bien que les noyaux \`a valeurs op\'erateurs g\'en\'eralisent strictement leurs confr\`eres sca\-lai\-res, ceux-ci on le d\'esavantage d'\^etre des m\'ethodes couteuses en m\'emoire et en temps de calcul. Dans le contexte actuel, o\`u les donn\'ees sont nombreuses, il est int\'eressant de pouvoir traiter celles-ci dans leur totalit\'e afin d'obtenir de meilleurs r\'esultats. Pour ce faire nous rempla\c cons les noyaux \`a valeur op\'erateurs par une construction --stochastique-- \'equivalente appel\'e fonction de redescription.

Dans un premier temps nous g\'en\'eralision le principe de constrution de ces fonctions de redescription --fond\'e sur une g\'en\'eralisation du th\'eor\`eme de Bochner pour les noyaux de Mercer invariants par translation-- et \'etudions leurs propri\'et\'es ainsi que la complexit\'e des algorithmes en d\'ecoulant. Nous nous int\'eressons ensuite \`a donner des garanties th\'eoriques en montrant que l\'erreur commise par l'approximation stochastique est contr\^lable avec forte probabilit\'e. Nous concluons par plusieurs exemples applications \`a l'apprentissage de donn\'ees fonctionelles, multit\^ache, semi-supervis\'e, et l'apprentissage de s\'eries temporelles tout en comparant notre approche \`a d'autres m\'ethodes de l\'etat de l'art.

\section*{Colophon}
This document was typeset using the typographical look-and-feel \texttt{cla\-ssic\-the\-sis} developed by Andr\'e Miede. The style was inspired by Robert Bringhurst's seminal book on typography ``\emph{The Elements of Typographic Style}''. \texttt{cla\-ssic\-the\-sis} is \href{https://bitbucket.org/amiede/classicthesis/}{available} for both \LaTeX\ and \mLyX.

\bigskip

\noindent\finalVersionString
