%!TEX root = ../../ThesisRomainbrault.tex

\section{About statistical learning}
\label{sec:about_statistical_learning}
We place ourself in the context of supervised learning. In this context we
suppose we are given a sequence of experiments composed of events and their
respective outcome. We note $x_i\in\mathcal{X}$ the $i$-th event and
$y_i\in\mathcal{Y}$ its respective outcome and we suppose that an experiment
$(x_i, y_i)$ occurs with probability $\probability(x_i, y_i)$. We call the
sequence of all the experiments available \say{training data} and a realization
of an experiment is refered to as data and is noted $\seq{s}=(x_i,
y_i)_{i=1}^N$ where $N$ is the number of data. To simplify the problem further
we suppose that all the events follow the same probability distribution and
each experiment is idependent from all the others. We say that the experiments
are \acf{iid}.
\paragraph{}
In machine learning we are often interested in finding the most simple function
$f$ belonging to a class of function $\mathcal{F}$ that is able to find the
best the relation between an event $x\in\mathcal{X}$ and an outcome
$y\in\mathcal{Y}$ from the training data. To do so we suppose we are given a
loss function $L:\mathcal{X}\times\mathcal{F}\times\mathcal{Y} \to
\mathbb{R}_+$ that evaluates the capacity of a function $f$ to predict the
outcome $y$ from an event $x$. 
%A learning algorithm is a function $\seq{s}, L,
%\mathcal{F} \mapsto f_{\seq{s}}$ is a function that takes as an imput some
%training data $\seq{s}$, a loss function $L$ and a class of function
%$\mathcal{F}$ and return a function $f_{\seq{s}}$ that minimize a criterion.
In statistical learning the goal is often to find the model that minimize the
average loss over all the training samples. This aforemention quantity is
called empirical risk.
\begin{dmath*}
    \riskemp{f, \seq{s}} \frac{1}{N}\sum_{i=1}^N L(x_i, f, y_i)
\end{dmath*}
Since the training data are \acs{iid} the celebrated strong law of large
numbers tells us that for any given function $f$ in $\mathcal{F}$ converges
almost surely to a quantity called true risk (or simply risk).
\begin{dmath*}
    \risk{f} = \int_{\mathcal{X}\times\mathcal{Y}} L(x, f, y)
    d\probability(x, y)
\end{dmath*}
Intuitively the empirical risk measure the performances of a model on the
training data, while the risk measure the performances of a model with respect
to all the possible experiments (event the one not present in the training
set).
\paragraph{}
We call learning algorithm a function $S$ that takes a class of function
$\mathcal{F}$, a loss $L$ and some training data $\seq{s}$ and return a
function $f_{\seq{s}}$ in $\mathcal{F}$. Although the convergence of
the empirical risk to the true risk is guaranteed by the strong law of large
number, we usually require that a learning algorithm generalizes well. That is
the empirical risk must converges to the true risk uniformly for all
$f_{\seq{s}}$ returned by a learning algorithm. In other words we want 
that given a class of function $\mathcal{F}$ and a loss $L$, the bound
\begin{dmath*}
    \risk{f_{\seq{s}}} \le \riskemp{f_{\seq{s}}, \seq{s}} + C(\delta, \seq{s},
    L, \set{f_{\seq{s}}}\subseteq\mathcal{F})
\end{dmath*}
holds with probability $1-\delta$, for all $\delta\in(0, 1)$ and $C(\delta,
\seq{s}, \mathcal{F}) \to 0$ when the number of training data $N$ in $\seq{s}$
goes to infinity. This type of bound
\paragraph{}
There is a crutial difference between the strong law of large numbers and the
generalization property of a learning algorithm. The strong law of large number
holds \emph{after} a model $f$ has been selected and fixed in $\mathcal{F}$.
Thus minimizing the empirical risk doesn't yield \emph{ipso facto} a model that
minimize the true risk (which measure the adequation of the model on unseen
data). This can be illustrated by an intuitive example adapted from
\citet[page 64]{cornuejols2011apprentissage} and the infinite monkey theorem.
\begin{figure}
    \centering\includegraphics[width=\textwidth]{./gfx/infinite_monkey.jpg}
    \caption{Borel's strong law of large numbers.}
\end{figure}
\begin{example}
    Suppose we have a recruiter (a learning algorithm) whose task is to select 
    the best students from a pool of candidates (the class of functions).
    Given ten students the recruiter make them pass a test with $N$ questions.
    If the exam is well constructed and there are enough questions the
    recruiter should be able to retrieve the best student.
    \paragraph{}
    Now suppose that ten millions monkeys $\gg N$ take the test and answer
    randomly to the questions. Then with high probability a monkey will score
    better or as well as the best student (strong law of large number). Can we
    say then that the recruiter has identified the best
    student?
    \paragraph{}
    Intuitively we see that when the capacity of the class of function grows
    (the number of students and random monkeys), the performance of the best
    element \emph{a posteriori} (minimizing the empirical risk) is not linked
    to the future performance (minimizing the true risk).
    \paragraph{}
    On the contrary the generalization property ensure that the difference
    between the empirical risk and the true risk is controled because the bound
    does not depend on a single fixed model, but on the whole class of
    functions. In this case if there are too many random monkey, $C(\delta,
    \seq{s}, L, \mathcal{F})$ will blow-up, giving a poor generalization
    property.
\end{example}
\paragraph{}
A slightly stronger requirement is the consistency of learning algorithm.
Given a loss function $L$ and a class of function $\mathcal{F}$ there exists an
optimal solution that minimizes the true risk.
\begin{dmath*}
    f_* = \argmin_{f\in\mathcal{F}} \risk{f}.
\end{dmath*}
The excess risk is defined as the difference between the empirical risk of a
model returned by a learning algorithm and $f_*$. A learning is said to be
consistent when it is possible to bound the excess risk uniformly over all
the solutions returned by a learning algorithm. In other words we look for a
bound such that given a class of function $\mathcal{F}$ and a loss $L$,
\begin{dmath*}
    \risk{f_{\seq{s}}} \le \inf_{f\in\mathcal{F}} \risk{f} + C(\delta, \seq{s},
    L, \Set{f_{\seq{s}}}\subseteq\mathcal{F}),
\end{dmath*}
holds with probability $1 - \delta$, for all $\delta \in (0, 1)$  and
$C(\delta, \seq{s}, \mathcal{F}) \to 0$ when the number of training data $N$ in
$\seq{s}$ goes to infinity. 
\paragraph{}
To identify the best model in $\mathcal{F}$ an intuitive loss function would be
the $0-1$ loss defined as
\begin{dmath*}
    L(x, f, y) =
    \begin{cases}
        1 & \text{if } yf(x) \le 0 \\
        0 & \text{otherwise}.
    \end{cases}
\end{dmath*}
This loss returns $0$ if the model $f(x)$ and $y$ have the same sign and $1$
otherwise. In this simple setting \citet{hoffgen1995robust} showed that finding
an approximate to the empirical risk minimization with the $0-1$ loss is
NP-Hard. However by \say{relaxing} a loss such that it becomes a convex in
$f(x)$ functions yields a convex optimization problem which can then be solved
in polynomial time. For instance, a convex surrogate of the $0-1$ loss is the
Hinge loss
\begin{dmath*}
    L(x, f, y) =
    \begin{cases}
        f(x) & \text{if } (2y-1)f(x) \le 0 \\
        0 & \text{otherwise}.
    \end{cases}
\end{dmath*}
or the logistic loss
\begin{dmath*}
    L(x, f, y) = \frac{1}{\ln(2)} \ln(1 + \exp(-yf(x)))
\end{dmath*}.
For regression, a common choice is the least square loss
\begin{dmath*}
    L(x, f, y) = \frac{1}{2}(f(x) - y)^2
\end{dmath*}
In the next section we discuss the choice of the class of functions
$\mathcal{F}$.

%------------------------------------------------------------------------------
\subsection{Introduction to kernel methods}
A fair simple choice for $\mathcal{F}$ is the set of all linear functions where
we focus on defining learning algorithm picking up the \say{best} function in
the class
\begin{dmath*}
    \mathcal{F}_{lin.} = \Set{f | f(x) = \inner{w, x} + b, \enskip \forall w \in
    \mathbb{R}^d, \enskip \forall x \in \mathbb{R}^d, \enskip \forall b \in
    \mathbb{R}}.
\end{dmath*}
Although this class of function has been well studied and has good
generalization propertis (as long as the norm of $w$ is not too big), it has a
rather low capacity.  For instance in $\mathbb{R}^2$ it is impossible to
separate two nested circle with a line (see \cref{fig:nested_circle}). On the
other hand if one consider the class of function of all functions $\Set{f |
f:\mathcal{X}\to\mathbb{R}}$, this space contains too many functions for any
algorithm to be able to find a solution to the minimization of the empirical
risk.
\begin{figure}
    \centering
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./gfx/nested_circle.pgf}}')}
    \caption[Separation of nested circles with linear classifier]{It is
    impossible to find a linear classifier that split perfectly two nested
    circles.}
    \label{fig:nested_circle}
\end{figure}
The idea of kernel methods \citep{Aronszajn1950} is to consider a clever
subset of the set of all functions, namely the set of all function belonging to
a Hilbert space such that the evaluation of the function at $x$ is bounded for
all
$x\in\mathcal{X}$.
\begin{dmath*}
    \mathcal{F}_{\text{b.~eval.}} = \Set{f | \abs{f(x)} \le
    C(x)\norm{f}_{\mathcal{H}} \le \infty, \enskip \forall f\in\mathcal{H},
    \forall x \in\mathcal{X}, \enskip C(x)\in\mathbb{R}_+ }.
\end{dmath*}
With this hypothesis it is legitimate to construct an evaluation operator for
all $x\in\mathcal{X}$ which is bounded.
\begin{dmath*}
    \text{ev}_x : 
    \begin{cases}
        \mathcal{F}_{\text{b.~eval.}}  & \to \mathbb{R} \\
        f & \mapsto f(x).
    \end{cases}
\end{dmath*}
From this evaluation operator, since $\mathcal{F}_{\text{b.~eval.}}$ is a
subspace of a Hilbert space by Riesz representation theorem, it is possible to
find a function $k_x$ of $\mathcal{F}$ such that
\begin{dmath*}
    f(x) = \text{ev}_x f \hiderel{=} \inner{k_x,
    f}_{\mathcal{F}_{\text{b.~eval.}}}.
\end{dmath*}
Then we can define a function $k(x, z) \colonequals \inner{k_x,
k_z}_{\mathcal{F}_{\text{b.~eval.}}}=\conj{k_x(z)}$ and we have $f(x) =
\inner{k(\cdot, x), f}_{\mathcal{F}_{\text{b.~eval.}}}$. We call such functions
$k$ reproducing kernel because they reproduces the value of any function of the
space $\mathcal{F}_{\text{b.~eval.}}$ at any point $x\in\mathcal{X}$. We call
the Hilbert space of function with locally bounded evaluation maps a \acf{RKHS}
and we note $\mathcal{F}_{\text{b.~eval.}} =\mathcal{H}_k$. Moreover notice
that the function $k$ is by construction symmetric and positive definite.
\paragraph{}
One of the most important result of the theory of the reproducing kernels is
the fact that there exists a bijection between the set of positive
semi-definite functions and and the set of \acl{RKHS}. In other words a
symmetric positive definite function defines a unique \acs{RKHS} and
\emph{vice-versa}. Thus we identify positive definite kernels with reproducing
kernels and \acsp{RKHS}.
\begin{theorem}[\citet{Aronszajn1950}]
    Suppose $k$ is a symmetric, positive definite kernel on a set
    $\mathcal{X}$. Then there is a unique Hilbert space of functions on
    $\mathcal{X}$ for which $k$ is a reproducing kernel.
\end{theorem}
Now, back to learning and minimizing the empirical risk, a fair question is
how do I find a function in an infinite dimensional set $\mathcal{H}_K$ in 
polynomial time? The answer comes from the regularization and interpolation
theory. To limit the size of the space in which we search of the function
minimizing the empirical risk we add a regularization term to the empirical
risk.
\begin{dmath*}
    \mathcal{J}_{\lambda}(f) = \frac{1}{N} \sum_{i=1}^N L\left(x_i, f,
    y_i\right) + \frac{\lambda}{2}\norm{f}_{\mathcal{H}_K}^2
\end{dmath*}
and we minimize $J_{\lambda}$ instead of $\mathfrak{R}_{\text{emp}}$. Then
the representer theorem (also called minimal norm interpolation theorem) states
the following.
\begin{theorem}[Representer theorem, \citet{Wahba90}]
    If $f_{\seq{s}}$ is a solution of
    \begin{dmath*}
        \argmin_{f\in\mathcal{H}_K} J_{\lambda}(f),
    \end{dmath*}
    where $\lambda > 0$ then $f_{\seq{s}}=\sum_{i=1}^N k(\cdot, x_i) \alpha_i$.
\end{theorem}
We note the vector $\alpha = (\alpha_i)_{i=1}^N$ and the matrix
$\mathbf{K}=(k(x_i, x_k))_{i, k = 1}^N$. Then we can rewrite
\begin{dmath*}
    \mathcal{J}_{\lambda}(\alpha) = \frac{1}{N} \sum_{i=1}^N L(x_i, \alpha,
    y_i) + \lambda \inner{\alpha, \mathbf{K}\alpha}_2 / 2
\end{dmath*}
where $f(x_i) = (K\alpha)_i$ for any $x_i \in \seq{s}$. If we suppose that $L$
is convex in $f(x)$, then it is possible to derive a polynomial time (in $N$)
algorithm minimizing $\mathcal{J}_{\lambda}$. For intance if we choose
$L$ to be the least square loss, then
\begin{dmath}
    \label{eq:ridge_regression}
    \mathcal{J}_{\lambda}(\alpha) = \frac{1}{2N}\norm{\mathbf{K}\alpha -
    (y_i)_{i=1}^N}_2^2 + \lambda \inner{\alpha, \mathbf{K}\alpha}_2 / 2.
\end{dmath}
This problem is called \emph{Ridge regression}.  By strict convexity and
coercivity of $\mathcal{J}_{\lambda}$, and because $K + \lambda I_N$ is
invertible for any $\lambda > 0$ the unique solution is $\alpha_{\seq{s}} =
\argmin_{\alpha\in\mathbb{R}^N} J_{\lambda}(\alpha) = (\mathbf{K}/N + \lambda
I_N)^{-1}(y_i)_{i=1}^N$. This is an $O\left(N^3\right)$ algorithm.
\paragraph{}
Another way of describing positive definite kernels and \acs{RKHS} consists in
defining an approriate feature map.
\begin{theorem}[\citet{Aronszajn1950}]
    Given $\phi: \mathcal{X} \to \mathcal{H}$ a function that maps a space
    $\mathcal{X}$ to a Hilbert space $\mathcal{H}$. Then
    $k(x,z)=\inner{\phi(x), \phi(z)}_{\mathcal{H}}$ is a positive definite
    kernel.
    \paragraph{}
    Given $k$ a positive definite kernel then there exist infinitely many
    functions $\phi: \mathcal{X} \to \mathcal{H}$ that map a space
    $\mathcal{X}$ to some Hilbert space $\mathcal{H}$ such that
    $\inner{\phi(x), \phi(z)}_{\mathcal{H}}=k(x,z)$. We call the function
    $\phi$ a feature map.
\end{theorem}
Then any functions in $\mathcal{H}_K$ can be written $f(x)=\inner{\phi(x),
\theta}_{\mathcal{H}}$ In a nutshell the function $\phi$ is called feature map
because it \say{extracts charateristic elements from a vector}. Usually a
feature map takes a vector in an input space with low dimension and maps it to
a higher dimensional space. Put it differently, any function in $\mathcal{H}_K$
is the composition of linear functional $\theta^\transpose$ with a non linear
feature map $\phi$. Thus is the feature map $\phi$ is fixed (which is
equivalent to fixing the kernel), it is possible to \say{learn} with a linear
class of function $\theta\in\mathcal{H}$ (see \cref{fig:feature_map}). 
\begin{figure}
    %\centering\resizebox{\textwidth}{!}{%
    %\begin{tikzpicture}
        %\node[inner sep=0pt] (input) at (0,0)
            %{\includegraphics[width=.35\textwidth]{./gfx/input.eps}};
        %\node[inner sep=0pt] (feature) at (5,-6)
            %{\includegraphics[width=.35\textwidth]{./gfx/feature.eps}};
        %\draw[->,thick] (input.east) -- (feature.west)
            %node[midway,fill=white] {$\phi:\mathcal{X} \to \mathcal{H}$};
    %\end{tikzpicture}}
    \centering
    \begin{tabular}{c}
        \includegraphics[valign=m, width=.5\textheight]{./gfx/input.eps} \\
        $\xdownarrow{2cm} \phi: \enskip \mathcal{X} = \mathbb{R}^2 \to
        \mathcal{H} = \mathbb{R}^3$ \\
        \includegraphics[valign=m, width=.5\textheight]{./gfx/feature.eps}
    \end{tabular}
    \caption[A scalar-valued feature map]{We map the two circles in
    $\mathbb{R}^2$ to $\mathbb{R}^3$. In $\mathbb{R}^3$ it is now possible to
    separate the circles with a linear functional: a plane. We used the feature
    map \\ $\phi(x) = 3.46 \begin{pmatrix} \cos(1.76 x_1 + 2.24 x_2 + 2.75) \\
    \cos(0.40 x_1 + 1.87 x_2 + 5.6) \\ \cos(0.98 x_1 - 0.98 x_2 + 6.05)
    \end{pmatrix}$. \label{fig:feature_map}}
\end{figure}
If we note
\begin{dmath*}
    \boldsymbol{\phi} =
    \begin{pmatrix}
        \phi(x_1) & \dots & \phi(x_N)
    \end{pmatrix}
\end{dmath*}
the \say{matrix} where each column represents the feature map evaluated at the
point $x_i$ with $1 \le i \le N$, the the regularized risk minimization with
the least square loss reads
\begin{dmath*}
    \mathcal{J}_{\lambda} = \frac{1}{2N}\norm{\boldsymbol{\phi}^\transpose
    \theta - (y_i)_{i=1}^N }_2^2 + \frac{\lambda}{2}\norm{\theta}_2^2.
\end{dmath*}
and the unique solution is $\theta_{\seq{s}} =
\left(\boldsymbol{\phi}\boldsymbol{\phi}^\transpose/N + \lambda
I_{\mathcal{H}}\right)^{-1}\boldsymbol{\phi} (y_i)_{i=1}^N$. This is an
$O\left( \dim(\mathcal{H})^2(N + \dim{\mathcal{H}}) \right)$. This algorithm
seems more appealing than its kernel counterpart when many data are given since
one the space $\mathcal{H}$ has been fixed, the algorithm is linear in the
number of training points. However many questions remains. First although it is
possible to design a feature map \emph{ex nihilo}, can we design systematically
a feature map from a kernel? For some kernels (\acs{eg} the gaussian kernel) it
is well known that the Hilbert space corresponding to it has dimension
$\dim(\mathcal{H}) = \infty$. Is it possible to find an approximation of the
kernel such that $\dim(\mathcal{H}) < \infty$? If such a construction is
possible and we know that $N$ data are present in the training set, is it
possible to have a sufficiently good approximation with $\dim(\mathcal{H}) \ll
N$\footnote{When $\dim(\mathcal{H}) \ge N$ then is it is better to use the
kernel algorithm than the feature algorithm. This is called the kernel trick.}?

\subsection{Random Fourier Features, Mercer Theorem, Nystr\"om method and
others}
In this subsection we answer the question whether it is possible to construct
feature maps from a given kernel, that approximate a given kernel $k$, such the
the dimension of the space $\mathcal{H} < \infty$. We start with the seminal
work of \citet{Rahimi2007} who show that given a continuous shift-invariant
kernel ($\forall x, z, t \in \mathcal{X}$, $k(x + t, z + t) = k(x, z)$), it is
possible to obtain a feature map called \acs{RFF} that approximate the given
kernel.
\subsubsection{Random Fourier Feature maps}
Random Fourier Feature methodology introduced  by Rahimi and Recht
\cite{Rahimi2007} provides a way to scale up kernel methods when kernels are
Mercer and \emph{translation-invariant}.  We view the input space $\mathcal{X}$
as a group endowed with the addition. Extensions to other group laws such as
\cite{li2010random} are described in \cref{subsubsec:skewedchi2} within the
general framework of operator-valued kernels.
\paragraph{}
Denote $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ a positive
definite kernel on $\mathbb{R}^d$. A kernel $k$ is said to be
\emph{shift-invariant} or \emph{translation-invariant} for the addition if for
any $a \in \mathbb{R}^d$, and for all $(x,z,t) \in \left(\mathbb{R}^d\right)^3$
we have $k(x+t,z+t) = k(x,z)$.  Then, we define $k_0: \mathbb{R}^d \to
\mathbb{R}$ the function such that $k(x,z)= k_0(x-z)$. $k_0$ is called the
\emph{signature} of kernel $k$. Bochner's theorem \cite{folland1994course} is
the theoretical result that leads to the Random Fourier Features.
\begin{theorem}[Bochner's theorem]\label{th:bochner-scalar}
    Any continuous positive definite complex function is the \acl{FT} of a
    non-negative measure.
\end{theorem}
It implies that any positive definite, continuous and shift-invariant kernel
$k$, have a continuous and positive definite signature $k_0$, which is the
\acl{FT} $\mathcal{F}$ of a non-negative measure $\mu$. We therefore have the
following corollary.
\begin{corollary}\label{c:bochner-app}
    With the previous notations and assumptions on $k$,
    \begin{dmath}\label{bochner-scalar}
        k(x,z)=k_0(x-z) \hiderel{=} \int_{\mathbb{R}^d} e^{-\iu \inner{\omega,x
        - z}} d\mu(\omega)
        =\FT{k_0}(\omega).
    \end{dmath}
\end{corollary}
Moreover $\mu = \IFT{k_0}$.  Without loss of generality, we assume that $\mu$
is a probability measure, \acs{ie} $\int_{\mathbb{R}^d} d\mu(\omega)=1$ by
renormalizing the kernel since
\begin{dmath*}
    \int_{\mathbb{R}^d}d\mu(\omega)= \int_{\mathbb{R}^d}\exp{-\iu
    \inner{\omega, 0}}d\mu(\omega)\hiderel{=}k_0(0). 
\end{dmath*}
and we can write \cref{bochner-scalar} as an expectation over $\mu$. For all
$x$,
$z\in\mathbb{R}^d$
\begin{dmath*}
    k_0(x-z) = \expectation_{\mu}\left[e^{-\iu \inner{\omega,x - z}}\right].
\end{dmath*}
Eventuallt, if $k$ is real valued we only write the real part, 
\begin{dmath*}
    k(x,z) = \expectation_{\mu}[\cos \inner{\omega,x - z}] =
    \expectation_{\mu}[ \cos \inner{\omega,z} \cos \inner{\omega,x} + \sin
    \inner{\omega,z} \sin \inner{\omega,x}].
\end{dmath*}
Let $\Vect_{j=1}^D x_j$ denote the $Dd$-length column
vector obtained by stacking vectors $x_j \in \mathbb{R}^d$.  The feature map
$\tilde{\phi}: \mathbb{R}^d \rightarrow \mathbb{R}^{2D}$ defined as
\begin{dmath}
\label{eq:rff}
    \tilde{\phi}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix} 
        \cos{\inner{x,\omega_j}} \\
        \sin{\inner{x,\omega_j}}
    \end{pmatrix}\condition{$\omega_j \hiderel{\sim} \IFT{k_0}$ \acs{iid}}
\end{dmath}
is called a \emph{Random Fourier Feature} (map). Each $\omega_{j}, j=1, \ldots,
D$ is independently and identically sampled from the inverse Fourier transform
$\mu$ of $k_0$. This Random Fourier Feature map provides the following
Monte-Carlo estimator of the kernel: $\tilde{k}(x, z) = \tilde{\phi}(x)^*
\tilde{\phi}(z)$. Using trigonometric identities, \citet{Rahimi2007} showed
that the same feature map can also be written
\begin{dmath}
    \label{eq:rff2}
    \tilde{\phi}(x)=\frac{2}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix} 
        \cos{\inner{x,\omega_j + b_j}}
    \end{pmatrix},
\end{dmath}
where $\omega_j \hiderel{\sim} \IFT{k_0}$, $b_j \sim \mathcal{U}(0, 2\pi)$
\acs{iid}.  The feature map defined by \cref{eq:rff} and \cref{eq:rff2} have
been compared in \citet{sutherland2015} where they give the condition under
wich \cref{eq:rff} has lower variance than \cref{eq:rff2}. For instance for the
gaussian kernel, \cref{eq:rff} has always lower variance. In practice,
\cref{eq:rff2} is easier to program. In this manuscript we focus on random
Fourier feature of the form \cref{eq:rff}.

\paragraph{}
The dimension $D$ governs the precision of this
approximation, whose uniform convergence towards the target kernel (as defined
in \cref{bochner-scalar}) can be found in \citet{Rahimi2007} and in more recent
papers with some refinements proposed in \citet{sutherland2015} and
\citet{sriper2015}.  Finally, it is important to notice that Random Fourier
Feature approach \emph{only} requires two steps before the application of a
learning algorithm: (1) define the inverse Fourier transform of the given
shift-invariant kernel, (2) compute the randomized feature map using the
spectral distribution $\mu$.  \citet{Rahimi2007} show that fGor the Gaussian
kernel $k_0(x-z) = \exp(-\gamma \norm{x - z}_2^2)$, the spectral distribution
$\mu$ is a Gaussian distribution. For the Laplacian kernel $k_0(x-z) =
exp(-\gamma \norm{x - z}_1)$, the spectral distribution is a Cauchy
distribution.
\paragraph{}
We now focus on another famous way of obtaining feature maps for any scalar
valued kernel called the Nystr\"om method.

\subsubsection{Nystr\"om approximation}
To overcome the bottleneck of Gram matrix computations in kernel methods,
Williams and Seeger \cite{Williams2000-nystrom} have proposed to generate a
low-rank matrix approximation of the Gram matrix using a subset of its columns.
Since this feature map is based on a decomposition of the Gram matrix, the
feature map resulting from the Nystr\"om method is data dependent. Let $k:
\mathcal{X}^2 \to \mathbb{R}$ be any scalar-valued kernel and let
\begin{dmath*}
    \seq{s} = (x_i)_{i=1}^N
\end{dmath*}
be the training data. We note a subsample fo the training data
\begin{dmath*}
    \seq{s}_M = (x_i)_{i=1}^M
\end{dmath*}
where $M \le N$ and $\seq{s}_M$ is a subsequence of $\seq{s}$. Then construct 
the gram matrix $\mathbf{K}_M$ on the subsequence $\seq{s}_M$. Namely
\begin{dmath*}
    \mathbf{K}_M =
    \begin{pmatrix}
        k(x_i, x_j)
    \end{pmatrix}_{i,j=1}^M.
\end{dmath*}
Then perform the singular-valued decomposition $\mathbf{K}_M = U \Lambda
U^\transpose$. The Nystr\"om feature map is given by
\begin{dmath*}
    \tilde{\phi}(x) = \Lambda^{-1/2} U^\transpose\left( \vect_{i=1}^M k(x, x_i)
    \right). 
\end{dmath*}
Here $M$ plays the same role than $D$ in the \acs{RFF} case: it controls the
quality of the approximation. Let $\mathbf{K}$ be the full Gram matrix on the
traing data $\seq{s}$, let 
\begin{dmath*}
    \mathbf{K}_b =
    \begin{pmatrix}
        k(x_i, x_j)
    \end{pmatrix}_{i=1, j=1}^{i=N, j=M}.
\end{dmath*}
Then it is easy to verify that $\boldsymbol{\phi}^\transpose \boldsymbol{\phi} =
\mathbf{K}_b \mathbf{K}_M^\dagger \mathbf{K}_b^\transpose \approx \mathbf{K}$,
where $\mathbf{K}_M^\dagger$ is the pseudo-inverse of $\mathbf{K}_M$ and the
quantity $\mathbf{K}_b \mathbf{K}_M^\dagger \mathbf{K}_b^\transpose$ is a low
rank approximation of the Gram matrix $\mathbf{K}$.
\paragraph{}
The main conceptual difference between the Nystr\"om features and the \acl{RFF}
is that the Nystr\"om construction is data dependent, while the \acs{RFF} is
not. The advantage of random fourier feature lies in their fast construction.
For $N$ data in $\mathbb{R}^d$, it costs $O(NDd)$ to featurize all the data.
For the Nystr\"om features it costs $O(M^2(M + d)$. Moreover if one desire to
add a new feature, the \acs{RFF} methodology is as simple as drawing a new
random vector $\omega\sim\IFT{k_0}$, compute $\cos(\inner{\omega, x} + b)$,
where $b\sim \mathcal{U}(0, 2\pi)$ and concatenate it the the existing feature.
For the Nystr\"om features one needs to recompute the singular value
decomposition of the new augmented Gram matrix $\mathbf{K}_{M+1}$.
\paragraph{}
To analyse the \acs{RFF} and Nystr\"om features authors usually study the
approximation error of the approximate Gram matrix and the targer kernel
$\norm{\boldsymbol{\phi}^\transpose\boldsymbol{\phi} - \mathbf{K}}$ (see
\citep{Yang2012, drineas2005nystrom, rosasco2010learning}) or
the supremum of the error between the approximated kernel and the true kernel
over a compact subset $\mathcal{X}$ of the support if $k$: $\sup_{(x, z)
\in\mathcal{C} \subseteq \mathcal{X}^2} \abs{\tilde{\phi}(x)^\transpose
\tilde{\phi}(z) - k(x, z)}$ (see \cite{Rahimi2007, sutherland2015, Bach2015,
rudi2016generalization}).  Because~\citet{bartlett2002rademacher} showed that
for generalization error to be below $\epsilon \in \mathbb{R}_{>0}$ for kernel
methods is $O(N^{-1/2})$, the number of samples $M$ or $D$ require to reach
somme approximation error below $\epsilon$ should be not grow faster than
$O(M^{-1/2})$ for the Nystr\"om method or $O(D^{-1/2})$ for the \acs{RFF}
method to match kernel learning.  Concerning the Nystr\"om method,
\citet{Yang2012} suggest that the number of samples $M$ is reduced to
$O(M^{-1})$ to reach an error below $\epsilon$ when the gap between the
eigenvalues of $\mathbf{K}$ is large enough. As a result in this specific case,
one should sample $M=O(\sqrt{N})$ Nystr\"om features to ensure good
generalization. On the other hand \citet{rahimi2009weighted} reported that the
generalization performance of \acs{RFF} learning is $O(N^{-1/2} + D^{-1/2})$,
which indicates that $D=O(N)$ features should be sampled to generalize well.
As a result the complexity of learning with the \acs{RFF} seems not to
deacrease. However the bounds of \citet{rahimi2009weighted} are suboptimal and
very recently (end of 2016) \citet{rudi2016generalization} proved that in the
case of ridge regression (\cref{eq:ridge_regression}), the generalization error
is $O(N^{-1/2} + D^{-1})$ meaning that $D=O(\sqrt{N})$ random features are
required for good generalization with \acsp{RFF}. We refer the interrested
reader to \citet{Yang2012} for an empirical comparison between the Nystr\"om
method and the \acs{RFF} method.


\subsubsection{Recent extensions}

\section{On large-scale learning}
\label{sec:on_large-scale_learning}

\section{History and state of the art of large scale learning with kernels}
\label{sec:history}

\chapterend
