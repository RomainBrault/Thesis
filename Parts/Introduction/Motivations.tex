%!TEX root = ../../ThesisRomainbrault.tex

\section{About statistical learning}
\label{sec:about_statistical_learning}
We focus on the context of supervised learning. Supervised learning aims at building a function that predicts an output from a given input, by exploiting a training set composed of pairs of observed inputs/outputs. Denote $\mathcal{X}$, an {\it input space} and $\mathcal{Y}$, the {\it output space}. In this chapter, $\mathcal{Y} \subset \mathbb{R}$. When $\mathcal{Y}=\{1, \ldots, C:\}$, we talk about {\it supervised classification}. When $\mathcal{Y}=\mathbb{R}$, supervised learning  corresponds to  usual {\it regression}. We are given an i.i.d. sample of size $N$, $\seq{s}=\{(x_i,
y_i)_{i=1}^N\}$, drawn from a unknown but fixed joint probability law $\probability(X,Y)$. We call {\emph learning algorithm}, a function $\mathcal{A}$ that takes a class of functions
$\mathcal{F}$, a training sample $\seq{s}$ and returns a function in $\mathcal{F}$. The learning algorithm can be studied through many angles, from a computational pint of view to a statistical point of view.
%function $f_{\seq{s}}$ in $\mathcal{F}$. 
\paragraph{}
Although we have only a limited number of observations, we wish to build a function that captures the relationship between the two random variables $X$ and $Y$. More specifically, we search for a function $f$ in some class of functions, denoted $\mathcal{F}$ and called the {\it hypotheses class} such that $f$ make good predictions for the pair $(X,Y)$ distributed according $\probability(X,Y)$. To convert this abstract goal into a mathematical definition, we define a local
loss function $L:\mathcal{X}\times\mathcal{F}\times\mathcal{Y} \to
\mathbb{R}_+$ that evaluates the capacity of a function $f$ to predict the
outcome $y$ from an input $x$.

The goal of supervised learning is to find a function $f \in \mathcal{F}$ that minimizes the following criteria, called the true risk associated to $L$:
\begin{equation}\label{eq:true-risk}
\mathcal{R}(f)=\expectation_{\probability}[L(X,f,Y)],
\end{equation}
using the training dataset.

However, this definition comes with an important issue: we do not know $\probability(X,Y)$ and thus we cannot compute this risk nor minimize it.
A first proposition is to replace this true risk by its empirical counterpart, the {\it empirical risk}, i.e. the empirical mean of the loss computed on the training set:
\begin{dmath*}
    \riskemp{f, \seq{s}} \frac{1}{N}\sum_{i=1}^N L(x_i, f, y_i)
\end{dmath*}
Since the training data are \acs{iid}, the celebrated strong law of large
numbers tells us that for any given function $f$ in $\mathcal{F}$, the {\it empirical risk} converges
almost surely to the true risk .\\
Intuitively the empirical risk measure the performance of a model on the
training data, while the risk measure the performances of a model with respect
to all the possible experiments (event the one not present in the training
set). Although the convergence of
the empirical risk to the true risk is guaranteed by the strong law of large
number, for a given value of $N$, the function produced by minimization of the empirical risk may suffer from overfitting, .e. being too much adapted to the training set and having a poor behavior on new data.




\paragraph{}

Generalization error bounds, first introduced by the seminal work of Vapnik \cite{Vapnik1992principles} in the contexte os supervised binary classification and then largely studied in wider contexts (see for instance, \cite{Mohri2012}) , provide a tool to understand how the difference between the true risk and the empirical risk behave given $N$ the size of the sample used to compute the empirical risk and $d$, a measure of the capacity the hypothesis class. These bounds usually take the following form:\\
For any $\delta > 0$, with probability $1 - \delta$, the following holds for any function $f$ \in \mathcal{F}$ of capacity $d$:
\begin{dmath*}
    \risk{f} \le \riskemp{f,\seq{s}} + C(\delta, N,d)
\end{dmath*}
Especially for the function of interest $f_{\seq{s}}$, we have 
\begin{dmath*}
    \risk{f} \le \riskemp{f_{\seq{s},\seq{s}} + C(\delta, N,d), 
\end{dmath*}
which suggest to control the complexity of the hypothesis class while minimizing the empirical risk.\\

Most of the approches in machine learning, and specifically in supervised learning, are based on regularizing approaches: in this case, learning algorithms minimize the empirical loss while controlling a penality term on the model $f$. In \cr{subsec:kernels}, we will chose an hypothesis class as an Hilbert space where the penalty can be expresses as the $\ell_2$ norm in this Hilbert space.


\paragraph{}
There is a crucial difference between the strong law of large numbers and the
generalization property of a learning algorithm. The strong law of large number
holds \emph{after} a model $f$ has been selected and fixed in $\mathcal{F}$.
Thus minimizing the empirical risk does not yield \emph{ipso facto} a model that
minimize the true risk (which measure the adequation of the model on unseen
data). This can be illustrated by an intuitive example adapted from
\citet[page 64]{cornuejols2011apprentissage} and the infinite monkey theorem.
\begin{figure}
    \centering\includegraphics[width=\textwidth]{./gfx/infinite_monkey.jpg}
    \caption{Borel's strong law of large numbers.}
\end{figure}
\begin{example}
    Suppose we have a recruiter (a learning algorithm) whose task is to select 
    the best students from a pool of candidates (the class of functions).
    Given ten students the recruiter make them pass a test with $N$ questions.
    If the exam is well constructed and there are enough questions the
    recruiter should be able to retrieve the best student.
    \paragraph{}
    Now suppose that ten millions monkeys $\gg N$ take the test and answer
    randomly to the questions. Then with high probability a monkey will score
    better or as well as the best student (strong law of large number). Can we
    say then that the recruiter has identified the best
    student?
    \paragraph{}
    Intuitively we see that when the capacity of the class of function grows
    (the number of students and random monkeys), the performance of the best
    element \emph{a posteriori} (minimizing the empirical risk) is not linked
    to the future performance (minimizing the true risk).
    \paragraph{}
    On the contrary the generalization property ensure that the difference
    between the empirical risk and the true risk is controlled because the bound
    does not depend on a single fixed model, but on the whole class of
    functions. In this case if there are too many random monkey, $C(\delta,
    \seq{s}, L, \mathcal{F})$ will blow-up, giving a poor generalization
    property.
\end{example}
\paragraph{}
A slightly stronger requirement is the {\emph consistency} of learning algorithm.
Given a loss function $L$ and a class of function $\mathcal{F}$ there exists an
optimal solution that minimizes the true risk.
\begin{dmath*}
    f_* = \argmin_{f\in\mathcal{F}} \risk{f}.
\end{dmath*}
The excess risk is defined as the difference between the empirical risk of a
model returned by a learning algorithm and $f_*$. A learning algorithm is said to be
consistent when it is possible to bound the excess risk uniformly over all
the solutions returned by a learning algorithm. 

%\paragraph{}
%To identify the best model in $\mathcal{F}$ an intuitive loss function would be
%the $0-1$ loss defined as
%\begin{dmath*}
  %  L(x, f, y) =
%    \begin{cases}
 %       1 & \text{if } yf(x) \le 0 \\
  %      0 & \text{otherwise}.
%    \end{cases}
%\end{dmath*}
%This loss returns $0$ if the model $f(x)$ and $y$ have the same sign and $1$
%otherwise. In this simple setting \citet{hoffgen1995robust} showed that finding
%an approximate to the empirical risk minimization with the $0-1$ loss is
%NP-Hard. However by \say{relaxing} a loss such that it becomes a convex in
%$f(x)$ functions yields a convex optimization problem which can then be solved
%in polynomial time. For instance, a convex surrogate of the $0-1$ loss is the
%Hinge loss
%\begin{dmath*}
 %   L(x, f, y) =
%    \begin{cases}
%        f(x) & \text{if } (2y-1)f(x) \le 0 \\
 %       0 & \text{otherwise}.
 %   \end{cases}
%\end{dmath*}
%or the logistic loss
%\begin{dmath*}
 %   L(x, f, y) = \frac{1}{\ln(2)} \ln(1 + \exp(-yf(x)))
%\end{dmath*}.
%For regression, a common choice is the least square loss
%\begin{dmath*}
%    L(x, f, y) = \frac{1}{2}(f(x) - y)^2
%\end{dmath*}
%In the next section we discuss the choice of the class of functions
%$\mathcal{F}$.

%------------------------------------------------------------------------------
\subsection{Introduction to kernel methods}\label{subsec:kernels}
\subsubsection{Kernels and Reproducing Kernel Hilbert Spaces}
A fair simple choice for $\mathcal{F}$ is the set of all linear functions where
we focus on defining learning algorithm picking up the \say{best} function in
the class
\begin{dmath*}
    \mathcal{F}_{lin.} = \Set{f | f(x) = \inner{w, x} + b, \enskip \forall w \in
    \mathbb{R}^d, \enskip \forall x \in \mathbb{R}^d, \enskip \forall b \in
    \mathbb{R}}.
\end{dmath*}
Although this class of function has been well studied and has good
generalization properties (as long as the norm of $w$ is not too big), it has a
rather low capacity.  For instance in $\mathbb{R}^2$ it is impossible to
separate two nested circle with a line (see \cref{fig:nested_circle}). On the
other hand if one consider the class of function of all functions $\Set{f |
f:\mathcal{X}\to\mathbb{R}}$, this space contains too many functions for any
algorithm to be able to find a solution to the minimization of the empirical
risk.
\begin{figure}
    \centering
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./gfx/nested_circle.pgf}}')}
    \caption[Separation of nested circles with linear classifier]{It is
    impossible to find a linear classifier that split perfectly two nested
    circles.}
    \label{fig:nested_circle}
\end{figure}
The idea of kernel methods \citep{Aronszajn1950,KIMELDORF1971,Boser1992, Berlinet2003,Shawe-TaylorBook} is to work in a 
subset of the set of all functions, namely a Reproducing Kernel Hilbert Space (\acl{RKHS}), associated to a well chosen positive definite and symmetric function ({\it a kernel}).

\paragraph{}
\begin{definition}[Positive Definite Symmetric kernels}
Let $\bmX$ be a Polish space. A kernel $k:\bmX \times \bmX \to \mathbb{R}$ is said to be  positive definite symmetric (PDS) if for any $\{x_1, \ldots, x_m\} \subseteq \bmX $, the matrix $K=[K(x_i,x_j)]_{ij} \in \mathbb{R}^{m \times m} is symmetric positive semi-definite (SPDS).
\end{definition}
The following proposition gives sufficient conditions to get a SPSD matrix:
\begin{Proposition}[SPSD matrix}
$K$ is SPSD if it is symmetric and one of the following conditions holds:
\begin{itemize}
\item The eigenvalues of $K$ are non-negative
\item for any colunn vector $c= (c_1, \ldots, c_m)^T \in \mathbb{R}^{m \times 1}$, $c^TKc=\sum_{i,j=1}^m c_ic_jK(x_i,x_i) \geq 0.
\end{itemize}
\end{proposition}
One of the most important property of PDS kernels \cite{Mohri2012} is that a PDS kernel defines a unique \acs{RKHS}. Note that the converse is also true. 
\begin{theorem}[\citet{Aronszajn1950}]
    Suppose $k$ is a symmetric, positive definite kernel on a set
    $\mathcal{X}$. Then there is a unique Hilbert space of functions $\mathcal{H}$ on
    $\mathcal{X}$ for which $k$ is a reproducing kernel, i.e.
    \begin{eqnarray}\label{eq:reproducing-prop}
    \forall x \in \mathcal{X}, k(\cdot, x) \in \mathcal{H} \\
    \forall h \in \mathcal{H}, \forall x\in \mathcal{X}, h(x)=\inner{h,k(\cdot,x)}_{\mathcal{H}}.
    \end{eqnarray}
    $\mathcal{H}$ is called a reproducing kernel Hilbert space (\acl{RKHS}) associated to $k$, and will be denoted, $\mathcal{H}_k$.
\end{theorem}
Another way to use Aronszajn's results is to state the feature map property for the PDS kernels.
\begin{proposition}[Feature map]
 Suppose $k$ is a symmetric, positive definite kernel on a set
    $\mathcal{X}$. Then, there exists a Hilbert space $\mathcal{F} and a mapping $\phi$ from \mathcal{X} to $\mathcal{F}$ such that:
    \begin{equation}
    \forall x, x' \in \mathcal{X}, k(x,x')=\inner{\phi(x),\phi(x')}.
    \end{equation}
    The mapping $\phi$ is called a {\it feature map} and $\mathcal{F}$, a feature space.
    \end{proposition}
    \begin{remark}
    Aronszajn's theorem tells us that there always exists at least one feature map, the so-called {\it canonical feature map} and the feature space associate, the \acl{RKHS} $\mathcal{H}_k$:
    \begin{eqnarray*}
    \phi_{canonical}(x)= k(\cdot, x)\\
    \mathcal{F}= \mathcal{H}_k
    \end{eqnarray*}
    However there might exist several pairs of feature maps and features spaces for a given kernel $k$.
    \end{remark}
    
 \subsubsection{Learning in Reproducing Kernel Hilbert Spaces}

Now, back to learning and minimizing the empirical risk, a fair question is
how do I find a function in an infinite dimensional set $\mathcal{H}_k$ in 
polynomial time? The answer comes from the regularization and interpolation
theory. To limit the size of the space in which we search of the function
minimizing the empirical risk we add a regularization term to the empirical
risk.
\begin{dmath*}
    \mathcal{J}_{\lambda}(f) = \frac{1}{N} \sum_{i=1}^N L\left(x_i, f,
    y_i\right) + \frac{\lambda}{2}\norm{f}_{\mathcal{H}_k}^2
\end{dmath*}
and we minimize $J_{\lambda}$ instead of $\mathfrak{R}_{\text{emp}}$. Then
the representer theorem (also called minimal norm interpolation theorem) states
the following.
\begin{theorem}[Representer theorem, \citet{Wahba90}]
    If $f_{\seq{s}}$ is a solution of
    \begin{dmath*}
        \argmin_{f\in\mathcal{H}_k} J_{\lambda}(f),
    \end{dmath*}
    where $\lambda > 0$ then $f_{\seq{s}}=\sum_{i=1}^N k(\cdot, x_i) \alpha_i$.
\end{theorem}
We note the vector $\alpha = (\alpha_i)_{i=1}^N$ and the matrix
$\mathbf{K}=(k(x_i, x_k))_{i, k = 1}^N$. Then we can rewrite
\begin{dmath*}
    \mathcal{J}_{\lambda}(\alpha) = \frac{1}{N} \sum_{i=1}^N L(x_i, \alpha,
    y_i) + \lambda \inner{\alpha, \mathbf{K}\alpha}_2 / 2
\end{dmath*}
where $f(x_i) = (\mathbf{K}\alpha)_i$ for any $x_i \in \seq{s}$. If we suppose that $L$
is convex in $f(x)$, then it is possible to derive a polynomial time (in $N$)
algorithm minimizing $\mathcal{J}_{\lambda}$. For instance if we choose
$L$ to be the least square loss, then
\begin{dmath}
    \label{eq:ridge_regression}
    \mathcal{J}_{\lambda}(\alpha) = \frac{1}{2N}\norm{\mathbf{K}\alpha -
    (y_i)_{i=1}^N}_2^2 + \lambda \inner{\alpha, \mathbf{K}\alpha}_2 / 2.
\end{dmath}
This problem is called \emph{Ridge regression}.  By strict convexity and
coercivity of $\mathcal{J}_{\lambda}$, and because $K + \lambda I_N$ is
invertible for any $\lambda > 0$ the unique solution is $\alpha_{\seq{s}} =
\argmin_{\alpha\in\mathbb{R}^N} J_{\lambda}(\alpha) = (\mathbf{K}/N + \lambda
I_N)^{-1}(y_i)_{i=1}^N$. This is an $O\left(N^3\right)$ algorithm.
\paragraph{}
Another way of describing positive definite kernels and \acs{RKHS} consists in
Then any functions in $\mathcal{H}_k$ can be written $f(x)=\inner{\phi(x),
\theta}_{\mathcal{H}}$ In a nutshell the function $\phi$ is called feature map
because it \say{extracts charateristic elements from a vector}. Usually a
feature map takes a vector in an input space with low dimension and maps it to
a higher dimensional space. Put it differently, any function in $\mathcal{H}_k$
is the composition of linear functional $\theta^\transpose$ with a non linear
feature map $\phi$. Thus is the feature map $\phi$ is fixed (which is
equivalent to fixing the kernel), it is possible to \say{learn} with a linear
class of function $\theta\in\mathcal{H}$ (see \cref{fig:feature_map}). 
\begin{figure}
    %\centering\resizebox{\textwidth}{!}{%
    %\begin{tikzpicture}
        %\node[inner sep=0pt] (input) at (0,0)
            %{\includegraphics[width=.35\textwidth]{./gfx/input.eps}};
        %\node[inner sep=0pt] (feature) at (5,-6)
            %{\includegraphics[width=.35\textwidth]{./gfx/feature.eps}};
        %\draw[->,thick] (input.east) -- (feature.west)
            %node[midway,fill=white] {$\phi:\mathcal{X} \to \mathcal{H}$};
    %\end{tikzpicture}}
    \centering
    \begin{tabular}{c}
        \includegraphics[valign=m, width=.5\textheight]{./gfx/input.eps} \\
        $\xdownarrow{2cm} \phi: \enskip \mathcal{X} = \mathbb{R}^2 \to
        \mathcal{H} = \mathbb{R}^3$ \\
        \includegraphics[valign=m, width=.5\textheight]{./gfx/feature.eps}
    \end{tabular}
    \caption[A scalar-valued feature map]{We map the two circles in
    $\mathbb{R}^2$ to $\mathbb{R}^3$. In $\mathbb{R}^3$ it is now possible to
    separate the circles with a linear functional: a plane. We used the feature
    map \\ $\phi(x) = 3.46 \begin{pmatrix} \cos(1.76 x_1 + 2.24 x_2 + 2.75) \\
    \cos(0.40 x_1 + 1.87 x_2 + 5.6) \\ \cos(0.98 x_1 - 0.98 x_2 + 6.05)
    \end{pmatrix}$. \label{fig:feature_map}}
\end{figure}
If we note
\begin{dmath*}
    \boldsymbol{\phi} =
    \begin{pmatrix}
        \phi(x_1) & \dots & \phi(x_N)
    \end{pmatrix}
\end{dmath*}
the \say{matrix} where each column represents the feature map evaluated at the
point $x_i$ with $1 \le i \le N$, the the regularized risk minimization with
the least square loss reads
\begin{dmath*}
    \mathcal{J}_{\lambda} = \frac{1}{2N}\norm{\boldsymbol{\phi}^\transpose
    \theta - (y_i)_{i=1}^N }_2^2 + \frac{\lambda}{2}\norm{\theta}_2^2.
\end{dmath*}
and the unique solution is $\theta_{\seq{s}} =
\left(\boldsymbol{\phi}\boldsymbol{\phi}^\transpose/N + \lambda
I_{\mathcal{H}}\right)^{-1}\boldsymbol{\phi} (y_i)_{i=1}^N$. This is an
$O\left( \dim(\mathcal{H})^2(N + \dim{\mathcal{H}}) \right)$. This algorithm
seems more appealing than its kernel counterpart when many data are given since
one the space $\mathcal{H}$ has been fixed, the algorithm is linear in the
number of training points. However many questions remains. First although it is
possible to design a feature map \emph{ex nihilo}, can we design systematically
a feature map from a kernel? For some kernels (\acs{eg} the gaussian kernel) it
is well known that the Hilbert space corresponding to it has dimension
$\dim(\mathcal{H}) = \infty$. Is it possible to find an approximation of the
kernel such that $\dim(\mathcal{H}) < \infty$? If such a construction is
possible and we know that $N$ data are present in the training set, is it
possible to have a sufficiently good approximation with $\dim(\mathcal{H}) \ll
N$\footnote{When $\dim(\mathcal{H}) \ge N$ then is it is better to use the
kernel algorithm than the feature algorithm. This is called the kernel trick.}?

\subsection{Towards large scale learning with kernels}
%Random Fourier Features, Mercer Theorem, Nystr\"om method and
others}
Motivated by large scale applications, different methodologies have been proposed to approximate kernels and feature maps. This subsection briefly reminds the main approaches based on  Random Fourier Features and Nystr\"om techniques. Notice that another line of research concerns online learning method such as NORMA developed in \cite{kivinen2004online}, later extended to the operator-valued kernel case by \cite{audiffren2013online}.
We start with the seminal
work of \citet{Rahimi2007} who show that given a continuous shift-invariant
kernel ($\forall x, z, t \in \mathcal{X}$, $k(x + t, z + t) = k(x, z)$), it is
possible to obtain a feature map called \acs{RFF} that approximate the given
kernel.
\subsubsection{Random Fourier Feature maps}
Random Fourier Feature methodology introduced  by Rahimi and Recht
\cite{Rahimi2007} provides a way to scale up kernel methods when kernels are
Mercer and \emph{translation-invariant}.  We view the input space $\mathcal{X}$
as a group endowed with the addition. Extensions to other group laws such as
\cite{li2010random} are described in \cref{subsubsec:skewedchi2} within the
general framework of operator-valued kernels.
\paragraph{}
Denote $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ a positive
definite kernel on $\mathbb{R}^d$. A kernel $k$ is said to be
\emph{shift-invariant} or \emph{translation-invariant} for the addition if for
any $a \in \mathbb{R}^d$, and for all $(x,z,t) \in \left(\mathbb{R}^d\right)^3$
we have $k(x+t,z+t) = k(x,z)$.  Then, we define $k_0: \mathbb{R}^d \to
\mathbb{R}$ the function such that $k(x,z)= k_0(x-z)$. $k_0$ is called the
\emph{signature} of kernel $k$. Bochner's theorem \cite{folland1994course} is
the theoretical result that leads to the Random Fourier Features.
\begin{theorem}[Bochner's theorem]\label{th:bochner-scalar}
    Any continuous positive definite complex function is the \acl{FT} of a
    non-negative measure.
\end{theorem}
It implies that any positive definite, continuous and shift-invariant kernel
$k$, have a continuous and positive definite signature $k_0$, which is the
\acl{FT} $\mathcal{F}$ of a non-negative measure $\mu$. We therefore have the
following corollary.
\begin{corollary}\label{c:bochner-app}
    With the previous notations and assumptions on $k$,
    \begin{dmath}\label{bochner-scalar}
        k(x,z)=k_0(x-z) \hiderel{=} \int_{\mathbb{R}^d} e^{-\iu \inner{\omega,x
        - z}} d\mu(\omega)
        =\FT{k_0}(\omega).
    \end{dmath}
\end{corollary}
Moreover $\mu = \IFT{k_0}$.  Without loss of generality, we assume that $\mu$
is a probability measure, \acs{ie} $\int_{\mathbb{R}^d} d\mu(\omega)=1$ by
renormalizing the kernel since
\begin{dmath*}
    \int_{\mathbb{R}^d}d\mu(\omega)= \int_{\mathbb{R}^d}\exp{-\iu
    \inner{\omega, 0}}d\mu(\omega)\hiderel{=}k_0(0). 
\end{dmath*}
and we can write \cref{bochner-scalar} as an expectation over $\mu$. For all
$x$,
$z\in\mathbb{R}^d$
\begin{dmath*}
    k_0(x-z) = \expectation_{\mu}\left[e^{-\iu \inner{\omega,x - z}}\right].
\end{dmath*}
Eventuallt, if $k$ is real valued we only write the real part, 
\begin{dmath*}
    k(x,z) = \expectation_{\mu}[\cos \inner{\omega,x - z}] =
    \expectation_{\mu}[ \cos \inner{\omega,z} \cos \inner{\omega,x} + \sin
    \inner{\omega,z} \sin \inner{\omega,x}].
\end{dmath*}
Let $\Vect_{j=1}^D x_j$ denote the $Dd$-length column
vector obtained by stacking vectors $x_j \in \mathbb{R}^d$.  The feature map
$\tilde{\phi}: \mathbb{R}^d \rightarrow \mathbb{R}^{2D}$ defined as
\begin{dmath}
\label{eq:rff}
    \tilde{\phi}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix} 
        \cos{\inner{x,\omega_j}} \\
        \sin{\inner{x,\omega_j}}
    \end{pmatrix}\condition{$\omega_j \hiderel{\sim} \IFT{k_0}$ \acs{iid}}
\end{dmath}
is called a \emph{Random Fourier Feature} (map). Each $\omega_{j}, j=1, \ldots,
D$ is independently and identically sampled from the inverse Fourier transform
$\mu$ of $k_0$. This Random Fourier Feature map provides the following
Monte-Carlo estimator of the kernel: $\tilde{k}(x, z) = \tilde{\phi}(x)^*
\tilde{\phi}(z)$. Using trigonometric identities, \citet{Rahimi2007} showed
that the same feature map can also be written
\begin{dmath}
    \label{eq:rff2}
    \tilde{\phi}(x)=\frac{2}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix} 
        \cos{\inner{x,\omega_j + b_j}}
    \end{pmatrix},
\end{dmath}
where $\omega_j \hiderel{\sim} \IFT{k_0}$, $b_j \sim \mathcal{U}(0, 2\pi)$
\acs{iid}.  The feature map defined by \cref{eq:rff} and \cref{eq:rff2} have
been compared in \citet{sutherland2015} where they give the condition under
wich \cref{eq:rff} has lower variance than \cref{eq:rff2}. For instance for the
gaussian kernel, \cref{eq:rff} has always lower variance. In practice,
\cref{eq:rff2} is easier to program. In this manuscript we focus on random
Fourier feature of the form \cref{eq:rff}.

\paragraph{}
The dimension $D$ governs the precision of this
approximation, whose uniform convergence towards the target kernel (as defined
in \cref{bochner-scalar}) can be found in \citet{Rahimi2007} and in more recent
papers with some refinements proposed in \citet{sutherland2015} and
\citet{sriper2015}.  Finally, it is important to notice that Random Fourier
Feature approach \emph{only} requires two steps before the application of a
learning algorithm: (1) define the inverse Fourier transform of the given
shift-invariant kernel, (2) compute the randomized feature map using the
spectral distribution $\mu$.  \citet{Rahimi2007} show that fGor the Gaussian
kernel $k_0(x-z) = \exp(-\gamma \norm{x - z}_2^2)$, the spectral distribution
$\mu$ is a Gaussian distribution. For the Laplacian kernel $k_0(x-z) =
exp(-\gamma \norm{x - z}_1)$, the spectral distribution is a Cauchy
distribution.
\paragraph{}
We now focus on another famous way of obtaining feature maps for any scalar
valued kernel called the Nystr\"om method.

\subsubsection{Nystr\"om approximation}
To overcome the bottleneck of Gram matrix computations in kernel methods,
Williams and Seeger \cite{Williams2000-nystrom} have proposed to generate a
low-rank matrix approximation of the Gram matrix using a subset of its columns.
Since this feature map is based on a decomposition of the Gram matrix, the
feature map resulting from the Nystr\"om method is data dependent. Let $k:
\mathcal{X}^2 \to \mathbb{R}$ be any scalar-valued kernel and let
\begin{dmath*}
    \seq{s} = (x_i)_{i=1}^N
\end{dmath*}
be the training data. We note a subsample fo the training data
\begin{dmath*}
    \seq{s}_M = (x_i)_{i=1}^M
\end{dmath*}
where $M \le N$ and $\seq{s}_M$ is a subsequence of $\seq{s}$. Then construct 
the gram matrix $\mathbf{K}_M$ on the subsequence $\seq{s}_M$. Namely
\begin{dmath*}
    \mathbf{K}_M =
    \begin{pmatrix}
        k(x_i, x_j)
    \end{pmatrix}_{i,j=1}^M.
\end{dmath*}
Then perform the singular-valued decomposition $\mathbf{K}_M = U \Lambda
U^\transpose$. The Nystr\"om feature map is given by
\begin{dmath*}
    \tilde{\phi}(x) = \Lambda^{-1/2} U^\transpose\left( \vect_{i=1}^M k(x, x_i)
    \right). 
\end{dmath*}
Here $M$ plays the same role than $D$ in the \acs{RFF} case: it controls the
quality of the approximation. Let $\mathbf{K}$ be the full Gram matrix on the
traing data $\seq{s}$, let 
\begin{dmath*}
    \mathbf{K}_b =
    \begin{pmatrix}
        k(x_i, x_j)
    \end{pmatrix}_{i=1, j=1}^{i=N, j=M}.
\end{dmath*}
Then it is easy to verify that $\boldsymbol{\phi}^\transpose \boldsymbol{\phi} =
\mathbf{K}_b \mathbf{K}_M^\dagger \mathbf{K}_b^\transpose \approx \mathbf{K}$,
where $\mathbf{K}_M^\dagger$ is the pseudo-inverse of $\mathbf{K}_M$ and the
quantity $\mathbf{K}_b \mathbf{K}_M^\dagger \mathbf{K}_b^\transpose$ is a low
rank approximation of the Gram matrix $\mathbf{K}$.
\subsubsection{Random features vs Nystr\"om method}
The main conceptual difference between the Nystr\"om features and the \acl{RFF}
is that the Nystr\"om construction is data dependent, while the \acs{RFF} is
not. The advantage of random Fourier feature lies in their fast construction.
For $N$ data in $\mathbb{R}^d$, it costs $O(NDd)$ to featurize all the data.
For the Nystr\"om features it costs $O(M^2(M + d)$. Moreover if one desire to
add a new feature, the \acs{RFF} methodology is as simple as drawing a new
random vector $\omega\sim\IFT{k_0}$, compute $\cos(\inner{\omega, x} + b)$,
where $b\sim \mathcal{U}(0, 2\pi)$ and concatenate it the the existing feature.
For the Nystr\"om features one needs to recompute the singular value
decomposition of the new augmented Gram matrix $\mathbf{K}_{M+1}$.
\paragraph{}
To analyse the \acs{RFF} and Nystr\"om features authors usually study the
approximation error of the approximate Gram matrix and the targer kernel
$\norm{\boldsymbol{\phi}^\transpose\boldsymbol{\phi} - \mathbf{K}}$ (see
\citep{Yang2012, drineas2005nystrom, rosasco2010learning}) or
the supremum of the error between the approximated kernel and the true kernel
over a compact subset $\mathcal{X}$ of the support if $k$: $\sup_{(x, z)
\in\mathcal{C} \subseteq \mathcal{X}^2} \abs{\tilde{\phi}(x)^\transpose
\tilde{\phi}(z) - k(x, z)}$ (see \cite{Rahimi2007, sutherland2015, Bach2015,
rudi2016generalization}).  Because~\citet{bartlett2002rademacher} showed that
for generalization error to be below $\epsilon \in \mathbb{R}_{>0}$ for kernel
methods is $O(N^{-1/2})$, the number of samples $M$ or $D$ require to reach
somme approximation error below $\epsilon$ should be not grow faster than
$O(M^{-1/2})$ for the Nystr\"om method or $O(D^{-1/2})$ for the \acs{RFF}
method to match kernel learning.  Concerning the Nystr\"om method,
\citet{Yang2012} suggest that the number of samples $M$ is reduced to
$O(M^{-1})$ to reach an error below $\epsilon$ when the gap between the
eigenvalues of $\mathbf{K}$ is large enough. As a result in this specific case,
one should sample $M=O(\sqrt{N})$ Nystr\"om features to ensure good
generalization. On the other hand \citet{rahimi2009weighted} reported that the
generalization performance of \acs{RFF} learning is $O(N^{-1/2} + D^{-1/2})$,
which indicates that $D=O(N)$ features should be sampled to generalize well.
As a result the complexity of learning with the \acs{RFF} seems not to
decrease. However the bounds of \citet{rahimi2009weighted} are suboptimal and
very recently (end of 2016) \citet{rudi2016generalization} proved that in the
case of ridge regression (\cref{eq:ridge_regression}), the generalization error
is $O(N^{-1/2} + D^{-1})$ meaning that $D=O(\sqrt{N})$ random features are
required for good generalization with \acsp{RFF}. We refer the interrested
reader to \citet{Yang2012} for an empirical comparison between the Nystr\"om
method and the \acs{RFF} method.

\subsubsection{Extensions of the RFF method}
\paragraph{}
The seminal idea of \citet{Rahimi2007} has open a large litterature on random
features. Nowaday, many classes of kernels other than translation invariant are
now proved to have an efficient random feature representation.
\citet{kar2012random} proposed random feature maps for dot product kernels
(rotation invariant) and \citet{hamid2014compact} improved the rate of
convergence of the approximation error for such kernels by noticing that
feature maps for dot product kernels are usually low ranki and may not utilize
the capacity of the projected feature  space  effectively. \Citet{pham2013fast}
proposed fast random feature maps for polynomial kernels.
\paragraph{}
To speed-up the convergence rate of the random features approximation,
\citet{yang2014quasi} proposed to sample the random variable from a quasi
Monte-Carlo sequence instead of \acs{iid}~random variables. \Citet{Le2013}
proposed the \say{Fastfood} algorithm to reduce the complexity of computing a
\acs{RFF}, using structured matrices and a fast Walsh-Hadarmard transform, from
$O(Dd)$ to $O(D\log(d))$. More recently \citet{felix2016orthogonal} proposed
also an algorithm \say{SORF} to compute Gaussian \acs{RFF} in $O(D\log(d))$ but
with better convergence rates than \say{Fastfood} \citep{Le2013}.
\Citet{mukuta2016kernel} proposed data dependent features (comparable to the
Nystro\"m method) by estimating the distribution of the input data, and then
finding the eigenfunction decomposition of Mercer's integral operator
associated to the kernel.
\paragraph{}
\Citet{li2010random} generalized the original \acs{RFF} of \citet{Rahimi2007}.
Instead of computing feature maps for shift-invariant kernels on the additive
group $(\mathbb{R}^d, +)$, they used the generalized Fourier transform on any
locally compact abelian group to derive random features on the multiplicative
group $(\mathbb{R}^d, *)$. In the same spirit \citet{yang2014random} noticed
that an theorem equivalent to Bochner's theorem exists on the semi-group
$(\mathbb{R}_+^d, +)$. From this they derived \say{Random Laplace} features and
used them to approximate kernels adapted to learn on histograms.
\paragraph{}
In the context of large scale learning and deep learning, \citet{lu2014scale}
showed that \acsp{RFF} can achieve performances comparable to deep-learning
methods by combining multiple kernel learning and composition of kernels along
with a scalable parallel implementation. \Citet{dai2014scalable} and
\citet{xie2015scale} combined \acsp{RFF} and stochastic gradient descent to
defined an online learning algorithm called \say{Doubly stochastic gradient
descent} adapted to large scale learning. \Citet{yang2015deep} proposed an
studied the idea of replacing the last fully interconnected layer of a deep
convolutional neural network \citep{lecun1995convolutional} by the
\say{Fastfood} implementation of \acsp{RFF}.
\paragraph{}
Eventually \citet{Yang2015} introduced the algorithm \say{\`A la Carte}, based
on \say{Fastfood} which is able to learn the spectral distribution
corresponding to a kernel rather than defining it from the kernel.


% A la carte
%reported
%performance comparable with deep learn- ing by combining multiple kernel
%learning and the compo- sition of kernels.

%A important subject of research is on how to compute random features
%as fast as possible. \Citet{hamid2014compacti, } proposed feature maps for
%polynomial kernels rank deficient, and there-
%fore may not utilize the capacity of the projected
%feature  space  effectively.

%\section{On large-scale learning}
%\label{sec:on_large-scale_learning}

%\section{History and state of the art of large scale learning with kernels}
%\label{sec:history}

\chapterend
