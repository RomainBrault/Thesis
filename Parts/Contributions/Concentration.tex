%!TEX root = ../../ThesisRomainbrault.tex

\section{Consistency of the ORFF estimator}
\label{sec:consistency_of_the_ORFF_estimator}
We are now interested on a non-asymptotic analysis of the ORFF approximation of
shift-invariant $\mathcal{Y}$-Mercer kernels on \acs{LCA} group $\mathcal{X}$
endowed with the operation group $\groupop$ with $\mathcal{X} \subset
\mathbb{R}^d$. For a given $N$, we study how close is the approximation
$\tilde{K}(x,z)=\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)$ to the target kernel
$K(x,z)$ for any $x,z$ in $\mathcal{X}$.
\paragraph{}
If $A\in\mathcal{L}_+(\mathcal{Y})$ we denote
$\norm{A}_{\mathcal{Y},\mathcal{Y}}$ its operator norm, which amounts the
square root of the largest eigenvalue of $A$ when $\mathcal{Y}$ is finite
dimensional. For $x$ and $z$ in some non-empty compact $\mathcal{C} \subset
\mathbb{R}^d$, we consider: $F(x \groupop \inv{z}) =\tilde{K}(x,z)-K(x,z)$ and
study how the uniform norm
\begin{dmath}\label{eq:norm_inf}
    \norm{\tilde{K}-K}_{\mathcal{C}\times\mathcal{C}}=\norm{F}_{\infty}
    \hiderel{=}\sup_{(x,z)\in\mathcal{C}\times\mathcal{C}}
    \norm{\tilde{K}(x,z)-K(x,z)}_{\mathcal{Y},\mathcal{Y}}
\end{dmath}
behaves according to $D$. All along this document we denote $\delta=x\groupop
z^{-1}$ for all $x$ and $z\in\mathcal{X}$. \Cref{fig:approximation_error}
empirically shows convergence of three different \acs{OVK} approximations for
$x,z$ sampled from the compact $[-1,1]^4$ and using an increasing number of
sample points $D$. The log-log plot shows that all three kernels have the same
convergence rate, up to a multiplicative factor.
\begin{pycode}[approximation]
sys.path.append('./src/')
import approximation

err = approximation.main()
\end{pycode}

\begin{figure}[!ht]
    \centering
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./approximation.pgf}}')}
    \caption[\acs{ORFF} reconstruction error]{Error reconstructing the target
    operator-valued kernel $K$ with \acs{ORFF}
    approximation $\tilde{K}$ for the decomposable, curl-free and
    divergence-free kernel.}
    \label{fig:approximation_error}
\end{figure}
\paragraph{}
In order to bound the error with high probability, we turn to concentration
inequalities devoted to random matrices~\citep{Boucheron}. Prior to the
presentation of general results, we briefly recall the uniform convergence of
\acs{RFF} approximation for a scalar shift invariant kernel on the additive
\acs{LCA} group $\mathbb{R}^d$ and introduce a direct corollary about
decomposable shift-invariant \acs{OVK} on the \acs{LCA} group $(\mathbb{R}^d,
+)$.

\subsection{Random Fourier Features in the scalar case and decomposable OVK}
\citet{Rahimi2007} proved the uniform convergence of \acf{RFF} approximation
for a scalar shift-invariant kernel on the \acs{LCA} group $\mathbb{R}^d$
endowed with the group operation $\groupop=+$. In the case of the
shift-invariant decomposable \acs{OVK}, an upper bound on the error can be
obtained as a direct consequence of the result in the scalar case obtained
by~\citet{Rahimi2007} and other authors~\citep{sutherland2015, sriper2015}.

\begin{theorem}[Uniform error bound for \ac{RFF},~\citet{Rahimi2007}]
    \label{rff-scalar-bound}
    Let $\mathcal{C}$ be a compact of subset of $\mathbb{R}^d$ of diameter
    $\abs{\mathcal{C}}$. Let $k$ be a shift invariant kernel, differentiable
    with a bounded second derivative and $\probability_{\rho}$ its normalized
    \acl{IFT} such that it defines a probability measure. Let 
    \begin{dmath*}
        \widetilde{k}=\sum_{j=1}^D\cos{\inner{\cdot, \omega_j}}
        \hiderel{\approx} k(x,z) \enskip\text{and}\enskip
        \sigma^2\hiderel{=}\expectation_{\rho}
        \norm{\omega}^2_2.
    \end{dmath*}
    Then we have
    \begin{dmath*}
        \probability_{\rho}\Set{(\omega_j)_{j=1}^D |
        \norm{\tilde{k}-k}_{\mathcal {C}\times\mathcal{C}}\ge \epsilon } \le
        2^8\left( \frac{\sigma \abs{\mathcal{C}}}{\epsilon} \right)^2\exp\left(
        -\frac{\epsilon^2D}{4(d+2)} \right)
    \end{dmath*}
\end{theorem}
From \cref{rff-scalar-bound}, we can deduce the following corollary about the
uniform convergence of the ORFF approximation of the decomposable kernel. We
recall that for a given pair $x$, $z$ in $\mathcal{C}$, $\tilde{K}(x,z)=
\tildePhi{\omega}(x)^* \tildePhi{\omega}(z)=A\tilde{k}(x,z)$ and $K_0(x-z)=A
\expectation_{\dual{\Haar},\rho}[\tilde{k}(x,z)]$.
\begin{corollary}[Uniform error bound for decomposable \acs{ORFF}]
    \label{c:dec-bound}
    Let $\mathcal{C}$ be a compact of subset of $\mathbb{R}^d$ of diameter
    $\abs{\mathcal{C}}$. Let $K$ be a decomposable kernel built from a positive
    operator self-adjoint $A$, and $k$ a shift invariant kernel with bounded
    second derivative such that
    \begin{dmath*}
        \widetilde{K}=\sum_{j=1}^D\cos{\inner{\cdot, \omega_j}}A
        \hiderel{\approx} K \enskip\text{and}\enskip
        \sigma^2\hiderel{=}\expectation_{\rho}
        \norm{\omega}^2_2.
    \end{dmath*}
    Then
    \begin{dmath*}
        \probability_{%
        \rho}\Set{(\omega_j)_{j=1}^D|\norm{\widetilde{K}-K}_{\mathcal{C} \times
        \mathcal{C}}\ge \epsilon } \le 2^8\left( \frac{\sigma
        \norm{A}_{\mathcal{Y},\mathcal{Y}} \abs{\mathcal{C}}}{\epsilon}
        \right)^2\exp\left( -\frac{\epsilon^2D}{4\norm{A}_2^2(d+2)} \right)
    \end{dmath*}
\end{corollary}
\begin{proof}
    The proof directly extends \cref{rff-scalar-bound} given
    by~\cite{Rahimi2007}. Let $\tilde{k}$ the Random Fourier approximation for
    the scalar-valued kernel $k$. Since
    \begin{dmath*}
        \sup_{(x,z)\in\mathcal{C}\times\mathcal{C}}\norm{\widetilde{K}(x,z) -
        K(x,z)}_{\mathcal{Y},\mathcal{Y}} =
        \sup_{(x,z)\in\mathcal{C}\times\mathcal{C}}
        \norm{A}_{\mathcal{Y},\mathcal{Y}} \abs{\widetilde{K}(x,z) - k(x,z)},
    \end{dmath*}
    taking $\epsilon' = \norm{A}_{\mathcal{Y},\mathcal{Y}} \epsilon$ gives the
    following result for all positive $\epsilon'$:
    \begin{dmath*}
        \probability_{%
        \rho}\Set{(\omega_j)_{j=1}^D|\sup_{x,z\in\mathcal{C}}\norm{A
        \left(\widetilde{k}(x,z)-k(x,z)\right)}_{\mathcal{Y},\mathcal{Y}}\ge
        \epsilon' } \le 2^8\left( \frac{\sigma
        \norm{A}_{\mathcal{Y},\mathcal{Y}} \abs{\mathcal{C}}}{\epsilon'}
        \right)^2\exp\left( -\frac{\left(\epsilon'\right)^2D}{4
        \norm{A}_{\mathcal{Y}, \mathcal{Y}}^2(d + 2)} \right)
    \end{dmath*}
    which concludes the proof.
\end{proof}
Please note that a similar corollary could have been obtained for the recent
result of~\citet{sutherland2015} who refined the bound proposed by Rahimi and
Recht by using a Bernstein concentration inequality instead of the Hoeffding
inequality. More recently~\citet{sriper2015} showed an optimal bound for
\acl{RFF}. The improvement of~\citet{sriper2015} is mainly in the constant
factors where the bound does not depend linearly on the diameter
$\abs{\mathcal{C}}$ of $\mathcal{C}$ but exhibit a logarithmic dependency
$\log\left(\abs{\mathcal{C}}\right)$, hence requiring significantly less
random features to reach a desired uniform error with high probability. Moreover,
\citet{sutherland2015} also considered a bound on the expected max error
$\expectation_{\dual{\Haar}, \rho} \norm{\widetilde{K}-K}_{\infty}$, which is
obtained using Dudley's entropy integral~\citep{dudley1967sizes, Boucheron} as
a bound on the supremum of an empirical process by the covering number of the
indexing set. This usefull theorem is also part of the proof of
\citet{sriper2015}.

\subsection{Uniform convergence of ORFF approximation on LCA groups}
In this analysis, we assume that $\mathcal{Y}$ is finite dimensional, in
\cref{remark:infinite_dimension}, we discuss how the proof could be extended to
infinite dimensional output Hilbert spaces. We propose a bound for \acl{ORFF}
approximation in the general case. It relies on two main ideas:
\begin{enumerate}
    \item q matrix-Bernstein concentration inequality for random matrices need
    to be used instead of concentration inequality for scalar random variables,
    \item a general theorem valid for random matrices with bounded norms such
    as decomposable kernel \acs{ORFF} approximation as well as un\-bound\-ed
    norms such as the \acs{ORFF} aproximation we proposed for curl and
    divergence-free kernels that behave as subexponential random variables.
\end{enumerate}
Before introducing the new theorem, we give the definition of the Orlicz norm
which gives a proxy-bound on the norm of subexponential random variables.
\begin{definition}[Orlicz norm~\citep{van1996weak}]
    Let $\psi:\mathbb{R}_+\to\mathbb{R}_+$ be a non-decreasing convex function
    with $\psi(0)=0$. For a random variable $X$ on a measured space
    $(\Omega,\mathcal{T} (\Omega),\mu)$, the quantity
    \begin{dmath*}
        \norm{X}_{\psi} \hiderel{=} \inf \Set{C > 0  |
        \expectation_{\mu}[\psi\left( \abs{X}/C \right)]\le 1}.
    \end{dmath*}
    is called the Orlicz norm of $X$.
\end{definition}
Here, the function $\psi$ is chosen as $\psi(u)=\psi_{\alpha}(u)$ where
$\psi_{\alpha}(u) \colonequals e^{u^{\alpha}}-1$. When $\alpha=1$, a random
variable with finite Orlicz norm is called a \emph{subexponential variable}
because its tails decrease at an exponential rate. Let $X$ be a self-adjoint 
random operator.  We call \emph{variance} of $X$ the quantity
$\variance_{\mu}[X]=\expectation_ {\mu}[X-\expectation_{\mu}[X]]^2$.  With this
convention if $X$ is a $p\times p$ Hermitian matrix, $\variance_{\mu}[X]_{\ell
m}=\sum_{r=1}^p\covariance{X_{\ell r}, X_{rm}}$. Among the possible
concentration inequalities adapted to random operators
\citep{tropp2015introduction, minsker2011some, ledoux2013probability,
pinelis1994optimum}. In this proposition we use the results of
\citet{minsker2011some}.

\begin{corollary}
    Let $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ be a
    shift-invariant $\mathcal{Y}$-Mercer kernel, where $\mathcal{Y}$ is a
    finite dimensional Hilbert space of dimension $p$ and $\mathcal{X}$ a
    finite dimensional Banach space of dimension $d$. Moreover, let
    $\mathcal{C}$ be a closed ball of $\mathcal{X}$ centered at the origin of
    diameter $\abs{\mathcal{C}}$,
    $A:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y})$ and
    $\probability_{\dual{\Haar},\rho}$ a pair such that
    \begin{dmath*}
        \tilde{K}_e = \sum_{j=1}^D \cos{\pairing{\cdot,\omega_j}}A(\omega_j)
        \hiderel{\approx}
        K_e\condition{$\omega_j\sim\probability_{\dual{\Haar}, \rho}$
        \acs{iid}.}.
    \end{dmath*}
    Let $\mathcal{D}_{\mathcal{C}}=\mathcal{C}\groupop\mathcal{C}^{-1}$ and
    \begin{dmath*}
        V(\delta) \succcurlyeq \variance_{\dual{\Haar},\rho}
        \tilde{K}_e(\delta) \condition{for all
        $\delta\in\mathcal{D}_{\mathcal{C}}$}
    \end{dmath*}
    and $H_\omega$ be the Lipschitz constant of the function $h: x\mapsto
    \pairing{x,\omega}$. If the three following constants exist
    \begin{dmath*}
        m \ge \int_{\dual{\mathcal{X}}} H_\omega
        \norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}} d\probability_{\dual{\Haar},
        \rho} \hiderel{<} \infty
    \end{dmath*}
    and
    \begin{dmath*}
        u \ge 4\left(\norm{\norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}
        + \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
        \norm{K_e(\delta)}_{\mathcal{Y},\mathcal{Y}}\right) \hiderel{<} \infty
    \end{dmath*}
    and
    \begin{dmath*}
        v \ge \sup_{\delta\in\mathcal{D}_{\mathcal{C}}} D
        \norm{V(\delta)}_{\mathcal{Y}, \mathcal{Y}} \hiderel{<} \infty.
    \end{dmath*}
    Define $p_{int}\ge \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
    \intdim(V(\delta))$, then for all $0 < \epsilon \le m \abs{C}$,
    \begin{dmath*}
        \probability_{\dual{\Haar,\rho}}\Set{(\omega_j)_{j=1}^D |
        \norm{\tilde{K}-K}_{\mathcal{C}\times\mathcal{C}} \ge \epsilon} 
        \le 8\sqrt{2} \left( \frac{m\abs{\mathcal{C}}}{\epsilon}
        \right)
        {\left(p_{int}r_{v/D}(\epsilon)\right)}^{\frac{1}{d + 1}}
        \begin{cases}
            \exp\left(-D\frac{\epsilon^2}{8
            v(d+1)\left(1 + \frac{1}{p}\right)}
            \right) \condition{$\epsilon \le
            \frac{v}{u}\frac{1+1/p}{K(v,
            p)}$} \\
            \exp\left(-D\frac{\epsilon}{8u(d+1)K(v,
            p)}\right)\condition{otherwise,}
        \end{cases}
    \end{dmath*}
    where $K(v, p)=\log\left(16 \sqrt{2}
    p\right)+\log\left(\frac{u^2}{v}\right) $ and $r_{v/D}(\epsilon)=1 +
    \frac{3}{\epsilon^2\log^2(1 + D \epsilon / v)}$.
 \end{corollary}
\begin{sproof}
In the following, let $F(\delta)=F(x \groupop \inv{z})=\tilde{K}(x,z)-K(x,z)$.
Let $\mathcal{D}_{\mathcal{C}}=\mathcal{C}^{-1}\groupop \mathcal{C} =
\Set{x\groupop \inv{z} | x,z\in\mathcal{C}}$. Since $\mathcal{C}$ is supposed
compact, so is $\mathcal{C}_{\Delta}$. Its diameter is at most
$2\abs{\mathcal{C}}$ where $\abs{\mathcal{C}}$ is the diameter of
$\mathcal{C}$. Since $\mathcal{C}$ is supposed to be a closed ball of a Bochner
space tt is then possible to find an $\epsilon$-net covering $\mathcal
{C}_{\Delta}$ with at most $T=(4\abs{\mathcal{C}}/r)^d$ balls of radius $r$
\citep{cucker2001mathematical}. We call $\delta_i$ for $i\inrange{1}{T}$ the
center of the $i$-th ball, called \emph{anchors} of the $\epsilon$-net. Denote
$L_{F}$ the Lipschitz constant of $F$. We introduce the following lemma proved
in~\cite{Rahimi2007}.
\begin{lemma}
    \label{lm:error_decomposition_main}
    For all $\delta \in \mathcal{D}_{\mathcal{C}}$, if
    \begin{dmath}
        L_{F}\le\frac{\epsilon}{2r}
        \label{condition1_main}
    \end{dmath}
    and
    \begin{dmath}
        \norm{F(\delta_i)}_{\mathcal{Y},\mathcal{Y}}
        \le\frac{\epsilon}{2}\condition{for all $i\in\mathbb{N}^*_T$}
        \label{condition2_main}
    \end{dmath}
    then $\norm{F(\delta)}_{\mathcal{Y},\mathcal{Y}} \leq \epsilon$.
\end{lemma}
To apply the lemma, we must check assumptions \cref{condition1_main} and
\cref{condition2_main}.
\begin{sproof}[\cref{condition1_main}]
We bound the Lipschitz constant by noticing that $F$ is differentiable, so
\begin{dmath*}
    L_{F}=\norm{\frac{\partial F}{\partial
    \delta}(\delta^\adjoint)}_{\mathcal{Y}, \mathcal{Y}} \text{~where }
    \delta^\adjoint \hiderel{=}
    \argmax_{\delta\in\mathcal{C}_{\Delta}}\norm{\frac{\partial F}{\partial
    \delta}(\delta)}_{\mathcal{Y},\mathcal{Y}}.
\end{dmath*}
Using Jensen's inequality and applying Markov's inequality yields
\begin{dmath*}
\probability_{\mu} \Set{\omega | L_{F} \ge \frac{\epsilon}{2r}} = \probability_{\mu} \Set{\omega | L_{F}^2 \hiderel{\ge} \left(\frac{\epsilon}{2r}\right)^2 } \le \expectation_{\mu}\left[\frac{\partial}{\partial \delta}h_{\omega}(\delta)\norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}}^2\right]\left(\frac{2r}{\epsilon} \right)^2.
\label{eq:Lipschitz_bound_sproof}
\end{dmath*}
We set $\sigma_p^2=\expectation_{\mu}\left[H_\omega^2\norm{A(\omega)}_2^2\right]$ where $H_\omega$ is the Lipschitz-constant of $h_\omega$ and suppose its existence.
\end{sproof}
\begin{sproof}[\cref{condition2_main}]
To obtain a bound on the anchors we apply theorem 4 of~\citet{koltchinskii2013remark}.
We suppose the existence of the two constants
\begin{dmath*}
b= \sup_{\delta\in\mathcal{C}_{\Delta}}D\norm{\variance_\mu \left[ \tilde{K}_e(\delta) \right]}_{\mathcal{Y},\mathcal{Y}}
\end{dmath*}
and
\begin{dmath*}
\bar{u}=\log \left( 2\left(\frac{m}{b}\right)^2+1\right),
\end{dmath*}
where $m=4\left(\norm{\norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}} }_{\psi_{1}}+\sup_{\delta\in \mathcal{C}_{\Delta}}\norm{K_e(\delta)}_{\mathcal{Y},\mathcal{Y}}\right)$ and $\omega \sim \mu$. Then $\forall i\hiderel{\inrange{1}{T}}$,
\begin{dmath*}
\probability_{\mu}\Set{ (\omega_j)_{j=1}^D | \norm{F(\delta_i)}_{\mathcal{Y},\mathcal{Y}} \hiderel{\ge} \epsilon } \hiderel{\le} 2p
\begin{cases}\exp\left( -\frac{D\epsilon^2}{4b+2\epsilon\bar{u}/3}\right) & \text{if $\epsilon\bar{u}\le2(e-1)b$} \\ \exp\left( -\frac{D\epsilon}{(e-1)\bar{u}} \right) & \text{otherwise.} \end{cases}
\label{eq:anchor_bound_sproof}
\end{dmath*}
\end{sproof}
\emph{Combining (H1) and (H2)}.\
Now applying the lemma and taking the union bound over the centers of the $\epsilon$-net yields
\begin{dmath*}
\probability_{\mu} \Set{ (\omega_j)_{j=1}^D | \sup_{\delta\in\mathcal{C}_{\Delta}} \norm{F(\delta)}_{\mathcal{Y},\mathcal{Y}} \le \epsilon } \ge 1 - \kappa_1 r^{-d} - \kappa_2 r^2,
\end{dmath*}
with $\kappa_2=4\sigma_p^2\epsilon^{-2}$ and
\begin{dmath*}
\kappa_1 = 2p(4 \abs{\mathcal{C}})^d\begin{cases} \exp\left( -\frac{\epsilon^2 D}{16\left(b+\frac{\epsilon}{6}\bar{u}\right)}\right) & \text{if}\enskip \epsilon\bar{u}\le 2(e-1)b \\\exp\left( -\frac{\epsilon D}{2(e-1)\bar{u}} \right) & \text{otherwise}.\end{cases}
\end{dmath*}
We choose $r$ such that $d\kappa_1r^{-d-1}-2\kappa_2r=0$, \acs{ie}~$r=\left(\frac{d\kappa_1}{2\kappa_2}\right)^{\frac{1}{d+2}}$.
The bound becomes
\begin{dmath*}
\probability_{\mu} \Set{(\omega_j)_{j=1}^D | \sup_{\delta\in\mathcal{C}_{\Delta}} \norm{F(\delta)}\textbf{} \ge \epsilon }
\le p C'_d 2^{\frac{6d+2}{d+2}}\left( \frac{\sigma_p \abs{\mathcal{C}}}{\epsilon} \right)^{\frac{2}{1+2/d}} \begin{cases} \exp\left( -\frac{\epsilon^2}{8(d+2)\left(b+\frac{\epsilon}{6}\bar{u}\right)}\right) & \text{if}\enskip \epsilon\bar{u}\le 2(e-1)b \\ \exp\left( -\frac{\epsilon}{(d+2)(e-1)\bar{u}} \right) & \text{otherwise}. \end{cases}
\end{dmath*}
where $C'_d=\left(\left(\frac{d}{2}\right)^{\frac{-d}{d+2}}+\left(\frac{d}{2}\right)^{\frac{2}{d+2}}\right)$.
Conclude by taking $C_{d,p}=pC'_d 2^{\frac{6d+2}{d+2}}$.
\end{sproof}
We give a comprehensive full proof of the theorem in the appendix. It follows the usual scheme derived in~\citet{Rahimi2007} and~\citet{sutherland2015} and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem 3.3 in the supplements).
\begin{remark}[dealing with infinite dimensional operators]\label{remark:infinite_dimension}
We studied the concentration of ORFFs under the assumption that $\mathcal{Y}$ is finite dimensional. This assumption is required in the proof when bounding the Lipschitz constant. The proof require the quantity $\tilde{K}$ to be smooth enough to be able to interchange integral and derivative such that $\expectation\frac{\partial}{\partial \delta}\tilde{K}=\frac{\partial}{\partial \delta}\expectation\tilde{K}$. In the proof we show, component-wise, that that is $K$ is twice differentiable, then the interchange is possible. To do the same for infinite dimensional operator-valued kernels, one could use Hille's theorem\mpar{See \url{http://fa.its.tudelft.nl/~neerven/publications/papers/ISEM.pdf}.} and could deduce the necessary condition on $K$ to allow the interchange. One could also deduced a bound on the Lipchitz constant without computing the derivative of $\tilde{K}-K$. Additionally one should study whether~\citet[theorem 4]{koltchinskii2013remark} could apply to infinite dimensional operators and under which assumption.
\end{remark}
\subsection{Variance of the ORFF approximation}
We now provide a bound on the norm of the variance of $\tilde{K}$, required to apply \cref{pr:orff_concentration}.
\begin{proposition}[Bounding the \emph{variance} of $\tilde{K}$]
    Let $K$ be a shift invariant $\mathcal{Y}$-Mercer
    kernel on a second countable \ac{LCA} topological space $\mathcal{X}$. Let
    $A:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y})$ and
    $\probability_{\dual{\Haar},\rho}$ a pair such that
    \begin{dmath*}
        \tilde{K}_e = \sum_{j=1}^D \cos{\pairing{\cdot,\omega_j}}A (\omega_j)
        \hiderel{\approx}
        K_e\condition{$\omega_j\sim\probability_{\dual{\Haar}, \rho}$
        \acs{iid}.}
    \end{dmath*}
    Then,
    \begin{dmath*}
        \variance_{\dual{\Haar}, \rho} \left[ \tilde{K}_e(\delta) \right]
        \preccurlyeq \frac{1}{2D} \left( \left( K_e(2\delta) + K_e(e) \right)
        \expectation_{\dual{\Haar}, \rho}\left[ A(\omega) \right] -
        2 K_e(\delta)^2 + \variance_{\dual{\Haar}, \rho}\left[
        A(\omega) \right]\right)
    \end{dmath*}
\end{proposition}
\begin{proof}
    It relies on the \ac{iid}~property of the random vectors $\omega_j$ and
    trigonometric identities (see the proof in \cref{sec:variance_bound} of the
    supplementary material).
\end{proof}
% \begin{figure}[!ht]
%   \centering
%   \begin{tabular}{cc}
%   \includegraphics[width=0.47\textwidth]{gfx/variance_dec.tikz} & \includegraphics[width=0.47\textwidth]{gfx/variance_curl.tikz}
%   \end{tabular}
%   \caption{Upper bound on the variance of the decomposable and curl-free kernel obtained via ORFF.}
%   \label{fig:variance_error}
% \end{figure}
An empirical illustration of these bounds is shown in \cref{fig:variance_error}. We generated a random point in $[-1,1]^4$ and computed the empirical variance of the estimator (blue line). We also plotted (red line) the theoretical bound in \cref{pr:variance_bound}.
\subsection{Application on decomposable, curl-free and div-free OVKs}
First, the two following example discuss the form of $H_\omega$ for the additive group and the skewed-multiplicative group.
\begin{example}[Additive group]
On the additive group, $h_\omega(\delta)=\inner{\omega, \delta}$. Hence $H_\omega=\norm{\omega}_2$.
\end{example}
\begin{example}[Skewed-multiplicative group]
On the skewed multiplicative group, $h_\omega(\delta)=\inner{\omega, \log(\delta+c)}$. Therefore
\begin{dmath*}
\sup_{\delta\in\mathcal{C}}\norm{\nabla h_\omega(\delta)}=\sup_{\delta\in\mathcal{C}}\norm{\omega/(\delta + c)}_2.
\end{dmath*}
Eventually $\mathcal{C}$ is compact and finite dimensional thus $\mathcal{C}$ is closed and bounded. Thus $H_\omega=\norm{\omega}_2/(\min_{\delta\in\mathcal{C}} \norm{\delta}_2+c)$.
\end{example}
Now we compute upper bounds on the norm of the variance and Orlicz norm of the three ORFFs we took as examples.
\paragraph{Decomposable kernel:}
notice that in the case of the Gaussian decomposable kernel, i.e. $A(\omega)=A$, $e=0$, $K_0(\delta)= Ak_0(\delta)$, $k_0(\delta) \geq 0$ and $k_0(\delta)=1$, then we have \begin{equation*}
D\norm{\variance_\mu \left[ \tilde{K}_0(\delta) \right]}_{\mathcal{Y},\mathcal{Y}}\leq (1+k_0(2\delta))\norm{A}_{\mathcal{Y},\mathcal{Y}}/2 + k_0(\delta)^2.
\end{equation*}
\paragraph{Curl-free and div-free kernels:} recall that in this case $p=d$. For the (Gaussian) curl-free kernel, $A(\omega)=\omega\omega^*$ where $\omega\in\mathbb{R}^d\sim\mathcal{N}(0, \sigma^{-2}I_d)$ thus $\expectation_\mu [A(\omega)] = I_d/\sigma^2$ and $\variance_{\mu}[A(\omega)]=(d+1)I_d/\sigma^4$. Hence,
\begin{equation*}
D\norm{\variance_\mu \left[ \tilde{K}_0(\delta) \right]}_{\mathcal{Y},\mathcal{Y}} \leq \frac{1}{2}\norm{\frac{1}{\sigma^2}K_0(2\delta)-2 K_0(\delta)^2}_{\mathcal{Y},\mathcal{Y}} + \frac{(d+1)}{\sigma^4}.
\end{equation*}
This bound is illustrated by \cref{fig:approximation_error} B, for a given datapoint. Eventually for the Gaussian divergence-free kernel, $A(\omega)=I\norm{\omega}_2^2-\omega\omega^*$, thus $\expectation_\mu [A(\omega)] = I_d(d-1)/\sigma^2$ and $ \variance_{\mu}[A(\omega)]=d(4d-3)I_d/\sigma^4$. Hence,
\begin{equation*}
D\norm{\variance_\mu \left[ \tilde{K}_0(\delta) \right]}_{\mathcal{Y},\mathcal{Y}} \leq \frac{1}{2}\norm{\frac{(d-1)}{\sigma^2}K_0(2\delta)-2K_0(\delta)^2}_{\mathcal{Y},\mathcal{Y}}+ \frac{d(4d-3)}{\sigma^4}.
\end{equation*}
Eventually, we ensure that the random variable $\norm{A(\omega)}$ has a finite Orlicz norm with $\psi=\psi_1$ in these three cases.
\paragraph{Computing the Orlicz norm:}
for a random variable with strictly monotonic moment generating function (MGF),
one can characterize its inverse $\psi_1$ Orlicz norm by taking the functional inverse of the MGF evaluated at 2 (see \cref{lm:orlicz_mgf} of the supplementary material).
In other words $\norm{X}_{\psi_1}^{-1}=\MGF(x)^{-1}_X(2)$.
For the Gaussian curl-free and divergence-free kernel,
\begin{dmath*}
\norm{A^{div}(\omega)}_{\mathcal{Y},\mathcal{Y}}=\norm{A^{curl}(\omega)}_{\mathcal{Y},\mathcal{Y}}\hiderel{=}\norm{\omega}_{2}^2,
\end{dmath*}
where$\omega\sim\mathcal{N}(0,I_d/\sigma^2)$, hence $\norm{A(\omega)}_2\sim \Gamma(p/2,2/\sigma^2)$. The MGF of this gamma distribution is $\MGF(x)(t)=(1-2t/\sigma^2)^{-(p/2)}$. Eventually
\begin{equation*}
 \norm{\norm{A^{div}(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}^{-1}=\norm{\norm{A^{curl}(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}^{-1}=\frac{\sigma^2}{2}\left(1-4^{-\frac{1}{p}}\right).
\end{equation*}

% \clearpage
\section{Generalization bound}
% In this section are interested in finding a minimizer $f_*:\mathcal{X}\to\mathcal{Y}$, where $\mathcal{X}$ is a Polish space and $\mathcal{Y}$ a separable Hilbert space such that for all $x_i$ in $\mathcal{X}$ and all $y_i$ in $\mathcal{Y}$. We consider a training set $Z=\Set{(x_i,y_i)|i\inrange{1}{N}}$ drawn \iid~from an unknown probability law $\probability$. We suppose we are given a cost function $c:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$, such that $c(f(x_i),y_i)$ returns the error of the prediction $f(x_i)$ \wrt~the ground truth $y_i$. A loss function of a model $f$ with respect to an example $(x_i,y_i)\in\mathcal{X}\times\mathcal{Y}$ is defined as $L(x_i,f,y_i)=c(f(x_i),y_i)$. Conceptually the function $c$ evaluate the quality of the prediction versus its ground truth $y_i\in\mathcal{Y}$ while the loss function $L$ evaluate the quality of the model $f$ at a training point $(x_i, y_i)\in\mathcal{X}\times\mathcal{Y}$. Then the generalization error of a model $f$ reads
% \begin{dmath*}
% \risk{f}=\int_{\mathcal{X}\times\mathcal{Y}}L(x,f,y)d\probability(x,y)
% \end{dmath*}
% Since in practice we do not have access to the probability $\probability$ of the points in the dataset, we define its empirical counterpart as the empirical mean estimate, where the $(x_i,y_i)$'s are drawn \iid~from a uniform law. Namely
% \begin{dmath*}
% \riskemp{f, Z}=\frac{1}{N}\sum_{i=1}^NL(x_i,f,y_i)
% \end{dmath*}

% \begin{proposition} Suppose that $f\in\mathcal{H}_K$ a \acs{vv-RKHS} where $\sup_{x\in\mathcal{X}} \Tr[K(x,x)] < T$ and $\norm{f}_{\mathcal{H}_K}<B$. Moreover let $L:\mathcal{Y}\times \mathcal{Y}\to[0, C]$ be a $L$-Lipschitz cost function. Then if we are given $N$ training points, we have with at least probability $1-2\delta$ that
% \begin{dmath}
% \mathcal{R}_{true}(f, L) \le \mathcal{R}_{emp}(f, L)  + 2\sqrt{\frac{2}{N}}\left( LBT^{1/2} + \frac{3C}{4}\sqrt{\ln(1/\delta)}\right).
% \end{dmath}
% \label{pr:ovk_gen}
% \end{proposition}

% \begin{dmath}
% f_* = \argmin_{f\in\mathcal{H}_K} \sum_{i=1}^NL(x_i, f, y _i) \hiderel{=} \argmin_{f\in\mathcal{H}_K} \mathcal{R}_{emp}(f, L),
% \label{eq:pbOVK}
% \end{dmath}
% where $\mathcal{H}_K$ is a \acs{vv-RKHS}. The empirical risk is the empirical counterpart of the generalization risk defined as



% How does a function trained as in \cref{eq:pbOVK} generalizes on a test set?

 % and $\forall (x_i,y_i) \in \mathcal{X}\times\mathcal{Y}$, $L(x_i,f,y_i)$ a $L$-Lipschitz cost function.
