%!TEX root = ../ThesisRomainbrault.tex
\chapter{Outline and motivations}
\label{ch:motivations}
\include{Parts/Introduction/Outline}

\chapter{On learning efficently scalar-valued functions}
\label{ch:motivations}
\bigskip
\begin{flushright}
    ``For such a model there is no need to ask the question \say{Is the
    model true?}. \\
    If \say{truth} is to be the \say{whole truth} the answer must
    be \say{No}. \\
    The only question of interest is \say{Is the model
    illuminating and useful?}''. \\
    --- \defcitealias{box1979robustness}{George
    Box}\citetalias{box1979robustness}
    \citep{box1979robustness}
\end{flushright}
\bigskip
\begin{justify}
    In this chapter we recall some elements of the statistical learning theory
    started by \citet{Vapnik1998}.  Then we recall kernel methods
    \citep{Aronszajn1950} which are used to construct spaces of real-valued
    function (called RKHS) that are used model and learn non linear
    dependencies from the data. We finish by a litterature review on
    large-scale implementation of kernel methods based on random Fourier
    features \citep{Rahimi2007} and the Nystr\"om method
    \citep{Williams2000-nystrom}.
\end{justify}
\minitoc
\include{Parts/Introduction/Motivations}

\chapter{Background}
\label{ch:background}
\bigskip
\begin{justify}
    In this chapter we introduce briefly the mathematical tools used throughout
    this manuscript. We give a full table of notations, and present elements of
    functional analysis \citep{kurdila2006convex} and abstract hamonic analysis
    \citep{folland1994course}. Then we turn our attention to the case when the
    functions we want to learn are not real-valued, but vector-valued.  To
    learn vector-valued functions we define \aclp{OVK} that generalize the
    scalar-valued kernel presented in \cref{ch:motivations}. We conclude by
    giving a non-exhaustive list of \aclp{OVK} and in which context they have
    been used.
\end{justify}
\minitoc
\include{Parts/Introduction/Background}
