%!TEX root = ../../ThesisRomainbrault.tex

\section{About statistical learning}
\label{sec:about_statistical_learning}
We focus on the context of supervised learning. Supervised learning aims at
building a function that predicts an output from a given input, by exploiting a
\say{training set} composed of pairs of observed inputs/outputs. Denote
$\mathcal{X}$, an \emph{input space} and $\mathcal{Y}$, the \emph{output
space}.  In this chapter, $\mathcal{Y} \subseteq \mathbb{R}$. When
$\mathcal{Y}=\Set{1, \ldots, C}$, we talk about \emph{supervised
classification}.  When $\mathcal{Y}=\mathbb{R}$, supervised learning
corresponds to  usual \emph{regression}. We are given an \acf{iid} sample of
size $N$ of traning data $\seq{s}=(x_i, y_i)_{i=1}^N$, drawn from an unknown
but fixed joint probability law $\probability$. We call \emph{learning
algorithm}, a function $\mathcal{A}$ that takes a class of functions
$\mathcal{F}$, a training sample $\seq{s}$ and returns a function in
$\mathcal{F}$. The learning algorithm can be studied through many angles, from
a computational point of view to a statistical point of view.
%function $f_{\seq{s}}$ in $\mathcal{F}$.
\paragraph{}
From a limited number of observations, we wish to build a function that
captures the relationship between the two random variables $X$ and $Y$. More
specifically, we search for a function $f$ in some class of functions, denoted
$\mathcal{F}$ and called the \emph{hypothesis class} such that the
$f\in\mathcal{F}$ makes good predictions for the pair $(X,Y)$ distributed
according $\probability$. To convert this abstract goal into a
mathematical definition, we define a local loss function
$L:\mathcal{X}\times\mathcal{F}\times\mathcal{Y} \to \mathbb{R}_+$ that
evaluates the capacity of a function $f$ to predict the outcome $y$ from an
input $x$.
\paragraph{}
Hence, the goal of supervised learning is to find a function $f \in
\mathcal{F}$ that minimizes the following criterion, called the true risk
associated to $L$:
\begin{equation}\label{eq:true-risk}
    \risk{f}=\expectation_{\probability}[L(X,f,Y)],
\end{equation}
using the training dataset. However, this definition comes with an important
issue: we do not know $\probability(X,Y)$ and thus we cannot compute this
risk nor minimize it.  A first proposition is to replace this true risk by its
empirical counterpart, the \emph{empirical risk}, \acs{ie} the empirical mean
of the loss computed on the training data:
\begin{dmath*}
    \riskemp{f, \seq{s}} = \frac{1}{N}\sum_{i=1}^N L(x_i, f, y_i).
\end{dmath*}
Since the training data are usually supposed to be \acs{iid}, the celebrated
strong law of large numbers tells us that for any given function $f$ in
$\mathcal{F}$, the \emph{empirical risk} converges almost surely to the true
risk.
\paragraph{}
Intuitively the empirical risk measures the performance of a model on the
training data, while the true risk measures the performance of a model with
respect to all the possible experiments (even the ones that are not present in
the training set). Although the convergence of the empirical risk to the true
risk is guaranteed by the strong law of large numbers, for a given value of
$N$, the function produced by minimization of the empirical risk may suffer
from overfitting, \acs{ie} being too much adapted to the training data and
having a poor behavior on new unseen data.
\paragraph{}
Generalization error bounds, first introduced by the seminal work of
\citet{vapnik1992principles} in the context of supervised binary
classification and then largely studied in wider contexts (see for instance,
\citet{Mohri2012}), provide a tool to understand how the difference between the
true risk and the empirical risk behaves given $N$, the size of the sample used
to compute the empirical risk, and $d$, a measure of the capacity the hypothesis
class. These bounds usually take the following form. For any $\delta \in (0,
1)$, with probability $1 - \delta$, the following holds for any function $f \in
\mathcal{F}$ of capacity $\abs{\mathcal{F}}\in\mathbb{R}$:
\begin{dmath*}
    \risk{f} \le \riskemp{f,\seq{s}} + C(\delta, N, \abs{\mathcal{F}})
\end{dmath*}
Especially for the functions of interest $f_{\seq{s}}$ returned by a learning
algorithm, we have
\begin{dmath*}
    \risk{f_{\seq{s}}} \le \riskemp{f_{\seq{s}},\seq{s}} + C(\delta, N,
    \abs{\mathcal{F}}).
\end{dmath*}
Usually it is expected from the quantity $C(\delta, N, \abs{\mathcal{F}})$ to
increase with the capacity of the class of functions $\abs{\mathcal{F}}$, and
to decrease when the number of points $N$ increases. This suggests to control
the complexity of the hypothesis class while minimizing the empirical risk. In
other words, is the class of functions $\mathcal{F}$ is not too big, we expect
that a low empirical risk implies a low true risk in particular when the number
of training points $N$ is large. Also when $\delta$ goes to zero, it is
expected for $C(\delta, N, \abs{\mathcal{F}})$ to go to infinity since
$1-\delta$ is the probability of the bound to be valid\footnote{We give
examples of such bounds in \cref{ch:generalization_for_ORFF}.}.
\paragraph{}
Most of the approaches in machine learning, and specifically in supervised
learning, are based on regularizing approaches: in this case, learning
algorithms minimize the empirical loss while controlling a penality term on the
model $f$. In \cref{subsec:kernels}, we will choose an hypothesis class as an
Hilbert space where the penalty can be expressed as the $\ell_2$ norm in this
Hilbert space.
\paragraph{}
There is a crucial difference between the strong law of large numbers and the
generalization property of a learning algorithm. The strong law of large
numbers holds \emph{after} a model $f$ has been selected and fixed in
$\mathcal{F}$.  Thus minimizing the empirical risk does not yield \emph{ipso
facto} a model that minimizes the true risk (which measures the adequation of
the model on unseen data). This can be illustrated by an intuitive example
adapted from \citet[page 64]{cornuejols2011apprentissage} and the infinite
monkey theorem.
\begin{figure}
    \centering\includegraphics[width=\textwidth]{./gfx/infinite_monkey.jpg}
    \caption{Borel's strong law of large numbers.}
\end{figure}
\begin{example}
    Suppose we have a recruiter (a learning algorithm) whose task is to select
    the best students from a pool of candidates (the class of functions).
    Given ten students the recruiter makes them pass a test with $N$ questions.
    If the exam is well constructed and there are enough questions the
    recruiter should be able to retrieve the best student.
    \paragraph{}
    Now suppose that ten million monkeys $\gg N$ take the test and answer
    randomly to the questions. Then with high probability a monkey will score
    better or as well as the best student (strong law of large numbers). Can we
    say then that the recruiter has identified the best
    student?
    \paragraph{}
    Intuitively we see that when the capacity of the class of function grows
    (the number of students and random monkeys), the performance of the best
    element \emph{a posteriori} (minimizing the empirical risk) is not linked
    to the future performance (minimizing the true risk). In the present
    example we see that the capacity of the class of function is too large with
    respect to the number of data and thus presents a risk of overfitting.
    \paragraph{}
    On the contrary the generalization property ensures that the difference
    between the empirical risk and the true risk is controlled because the
    bound does not depend on a single fixed model, but on the whole class of
    functions. In this case if there are too many random monkeys, $C(\delta, N,
    \abs{\mathcal{F}})$ will blow-up, resulting in a poor generalization
    property.
\end{example}
\paragraph{}
A slightly stronger requirement is the {\emph consistency} of learning
algorithm.  Given a loss function $L$ and a class of function $\mathcal{F}$
there exists a optimal solutions that minimize the true risk.
\begin{dmath*}
    f_* \in \argmin_{f\in\mathcal{F}} \risk{f}.
\end{dmath*}
The excess risk is defined as the difference between the empirical risk of a
model returned by a learning algorithm and $f_*$. A learning algorithm is said
to be consistent when it is possible to bound the excess risk uniformly over
all
%<<<<<<< HEAD
%the solutions returned by a learning algorithm. In other words we look for a
%bound such that given a class of function $\mathcal{F}$ and a loss $L$,
%\begin{dmath*}
    %\risk{f_{\seq{s}}} \le \inf_{f\in\mathcal{F}} \risk{f} + C(\delta, \seq{s},
    %L, \Set{f_{\seq{s}}}\hiderel{\subseteq}\mathcal{F}),
%\end{dmath*}
%holds with probability $1 - \delta$, for all $\delta \in (0, 1)$  and
%$C(\delta, \seq{s}, \mathcal{F}) \to 0$ when the number of training data $N$ in
%$\seq{s}$ goes to infinity.
%\paragraph{}
%To identify the best model in $\mathcal{F}$ an intuitive loss function would be
%the $0-1$ loss defined as
%\begin{dmath*}
    %L(x, f, y) =
    %\begin{cases}
        %1 & \text{if } yf(x) \le 0 \\
        %0 & \text{otherwise}.
    %\end{cases}
%\end{dmath*}
%This loss returns $0$ if the model $f(x)$ and $y$ have the same sign and $1$
%otherwise. In this simple setting \citet{hoffgen1995robust} showed that finding
%an approximate to the empirical risk minimization with the $0-1$ loss is
%NP-Hard. However by \say{relaxing} a loss such that it becomes a convex in
%$f(x)$ functions yields a convex optimization problem which can then be solved
%in polynomial time. For instance, a convex surrogate of the $0-1$ loss is the
%Hinge loss
%\begin{dmath*}
    %L(x, f, y) =
    %\begin{cases}
        %f(x) & \text{if } (2y-1)f(x) \le 0 \\
        %0 & \text{otherwise}.
    %\end{cases}
%\end{dmath*}
%or the logistic loss
%\begin{dmath*}
    %L(x, f, y) = \frac{1}{\ln(2)} \ln(1 + \exp(-yf(x)))
%\end{dmath*}.
%For regression, a common choice is the least square loss
%\begin{dmath*}
    %L(x, f, y) = \frac{1}{2}(f(x) - y)^2
%\end{dmath*}
%In the next section we discuss the choice of the class of functions
%$\mathcal{F}$.
%=======
the solutions returned by a learning algorithm.

%\paragraph{}
%To identify the best model in $\mathcal{F}$ an intuitive loss function would be
%the $0-1$ loss defined as
%\begin{dmath*}
  %  L(x, f, y) =
%    \begin{cases}
 %       1 & \text{if } yf(x) \le 0 \\
  %      0 & \text{otherwise}.
%    \end{cases}
%\end{dmath*}
%This loss returns $0$ if the model $f(x)$ and $y$ have the same sign and $1$
%otherwise. In this simple setting \citet{hoffgen1995robust} showed that finding
%an approximate to the empirical risk minimization with the $0-1$ loss is
%NP-Hard. However by \say{relaxing} a loss such that it becomes a convex in
%$f(x)$ functions yields a convex optimization problem which can then be solved
%in polynomial time. For instance, a convex surrogate of the $0-1$ loss is the
%Hinge loss
%\begin{dmath*}
 %   L(x, f, y) =
%    \begin{cases}
%        f(x) & \text{if } (2y-1)f(x) \le 0 \\
 %       0 & \text{otherwise}.
 %   \end{cases}
%\end{dmath*}
%or the logistic loss
%\begin{dmath*}
 %   L(x, f, y) = \frac{1}{\ln(2)} \ln(1 + \exp(-yf(x)))
%\end{dmath*}.
%For regression, a common choice is the least square loss
%\begin{dmath*}
%    L(x, f, y) = \frac{1}{2}(f(x) - y)^2
%\end{dmath*}
%In the next section we discuss the choice of the class of functions
%$\mathcal{F}$.
%>>>>>>> 68a8c8187614fe541360c89c36b8192178611905

%------------------------------------------------------------------------------
\subsection{Introduction to kernel methods}\label{subsec:kernels}
\subsubsection{Kernels and Reproducing Kernel Hilbert Spaces}
A fair simple choice for $\mathcal{F}$ is the set of all linear functions. In
this case we focus on defining learning algorithm picking up the \say{best}
function(s) in the class
\begin{dmath*}
    \mathcal{F}_{lin.} = \Set{f | f(x) = \inner{w, x} + b, \enskip \forall w \in
    \mathbb{R}^d, \enskip \forall x \in \mathbb{R}^d, \enskip \forall b \in
    \mathbb{R}}.
\end{dmath*}
Although this class of functions has been well studied and has good
generalization properties (as long as the norm of $w$ is not too big), it has a
rather low capacity.  For instance in $\mathbb{R}^2$ it is impossible to
separate two nested circles with a line (see \cref{fig:nested_circle}). On the
other hand if one considers the class of functions of all functions $\Set{f |
f:\mathcal{X}\to\mathbb{R}}$, this space contains too many functions for any
algorithm to be able to find a solution to the minimization of the empirical
risk.
\begin{figure}
    \centering
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./gfx/nested_circle.pgf}}')}
    \caption[Separation of nested circles with linear classifier]{It is
    impossible to find a linear classifier that splits perfectly two nested
    circles.}
    \label{fig:nested_circle}
\end{figure}
The idea of kernel methods
\citep{Aronszajn1950,KIMELDORF1971,boser1992training,
Berlinet2003,Shawe-TaylorBook} is to work in a subset of the set of all
functions, namely a \acf{RKHS}, associated
to a well chosen positive semi-definite and symmetric function (\emph{a
kernel}).
\paragraph{}
\begin{definition}[Positive Definite kernels]
    Let $\mathcal{X}$ be a locally compact second countable topological space.
    A kernel $k:\mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is said to be
    \acf{PSD} if for any $(x_1, \ldots, x_N) \in \mathcal{X}^N$, the (Gram)
    matrix
    \begin{dmath*}
        \mathbf{K} =
        \begin{pmatrix}
            k(x_i,x_j)
        \end{pmatrix}_{i = 1, j = 1}^{i = N, j = N} \hiderel{\in}
        \mathcal{M}_{N, N}(\mathbb{R})
    \end{dmath*}
    is \acf{SPSD}\footnote{Note that for historical reasons valid kernels are
    called \say{Positive Definite kernels}, although for any sequences of
    points the corresponding Gram matrix needs only to be (symmetric) Positive
    Semi-Definite \citep{fukumizu2008elements}.}.
\end{definition}
The following proposition gives necessary and sufficient conditions to obtain a
\acs{SPSD} matrix:
\begin{proposition}[\acs{SPSD} matrix]
    $\mathbf{K}$ is \acs{SPSD} if and only if it is symmetric and one of the
    following assertions holds:
    \begin{itemize}
        \item The eigenvalues of $\mathbf{K}$ are non-negative
        \item for any column vector $c= (c_1, \ldots, c_N)^\transpose \in
        \mathcal{M}_{N, 1}(\mathbb{R})$,
        \begin{dmath*}
            c^\transpose\mathbf{K}c=\sum_{i,j=1}^N c_i\mathbf{K}_{ij}c_j
            \hiderel{\geq} 0
        \end{dmath*}
    \end{itemize}
\end{proposition}
One of the most important property of \acs{PD} kernels \citep{Mohri2012} is
that a \acs{PD} kernel defines a unique \acs{RKHS}. Note that the converse is
also true.
\begin{theorem}[\citet{Aronszajn1950}]
    Suppose $k$ is a symmetric, positive definite kernel on a set
    $\mathcal{X}$. Then there is a unique Hilbert space of functions
    $\mathcal{H}$ on $\mathcal{X}$ for which $k$ is a reproducing kernel,
    \acs{ie}
    \begin{dgroup}
        \begin{dmath}\label{eq:reproducing-prop}
            \forall x \in \mathcal{X}, k(\cdot, x) \hiderel{\in} \mathcal{H}
        \end{dmath}
        \begin{dmath}
            \forall h \in \mathcal{H}, \forall x \hiderel{\in} \mathcal{X},
            h(x) \hiderel{=} \inner{h,k(\cdot,x)}_{\mathcal{H}}.
        \end{dmath}
    \end{dgroup}
    $\mathcal{H}$ is called a reproducing kernel Hilbert space (\acl{RKHS})
    associated to $k$, and will be denoted, $\mathcal{H}_k$.
\end{theorem}
Another way to use Aronszajn's results is to state the feature map property for
the \acs{PSD} kernels.
\begin{proposition}[Feature map]
     Suppose $k$ is a symmetric, positive definite kernel on a set
     $\mathcal{X}$. Then, there exists a Hilbert space $\mathcal{H}$ and a
     mapping $\phi$ from $\mathcal{X}$ to $\mathcal{H}$ such that:
    \begin{dmath*}
        \forall x, x' \hiderel{\in} \mathcal{X},
        k(x,x')\hiderel{=}\inner{\phi(x),\phi(x')}_{\mathcal{H}}.
    \end{dmath*}
    The mapping $\phi$ is called a \emph{feature map} and $\mathcal{H}$, a
    feature space.
\end{proposition}
\begin{remark}
    Aronszajn's theorem tells us that there always exists at least one feature
    map, the so-called \emph{canonical feature map} and the feature space
    associate, the \acl{RKHS} $\mathcal{H}_k$
    \begin{dmath*}
        \phi(x)= k(\cdot, x)
    \end{dmath*}
    and $\mathcal{H}= \mathcal{H}_k$.  However there exists several pairs of
    feature maps and features spaces for a given kernel $k$.
\end{remark}
\subsubsection{Learning in Reproducing Kernel Hilbert Spaces}
Back to learning and minimizing the empirical risk, a fair question is how to
pick-up functions that minimize the empirical risk, in a space $\mathcal{H}_k$
with infinite cardinality in polynomial time? The answer comes from the
regularization and interpolation theory. To limit the size of the space in
which we search for the function minimizing the empirical risk we add a
regularization term to the empirical risk.
\begin{dmath*}
    \mathfrak{R}_{\lambda}(f, \seq{s}) = \mathfrak{R}_{\text{emp}}(f, \seq{s})
    + \frac{\lambda}{2} \norm{f}_{\mathcal{H}_k}^2
    = \frac{1}{N} \sum_{i=1}^N L\left(x_i, f,
    y_i\right) + \frac{\lambda}{2}\norm{f}_{\mathcal{H}_k}^2
\end{dmath*}
and we minimize $\mathfrak{R}_{\lambda}$ instead of
$\mathfrak{R}_{\text{emp}}$. Then the representer theorem (also called minimal
norm interpolation theorem) states the following.
\begin{theorem}[Representer theorem, \citet{Wahba90}]
    If $f_{\seq{s}}$ is a solution of $\argmin_{f\in\mathcal{H}_k}
    \mathfrak{R}_{\lambda}(f, \seq{s})$, where $\lambda > 0$ then
    $f_{\seq{s}}=\sum_{i=1}^N k(\cdot, x_i) \alpha_i$.
\end{theorem}
We note the vector $\boldsymbol{\alpha} = (\alpha_i)_{i=1}^N$ and the matrix
$\mathbf{K}=(k(x_i, x_k))_{i, k = 1}^N$. Because of the representer theorem,
stating that a solution of the empirical risk minimization is a linear
combination of kernel evaluations weighted by a vector $\boldsymbol{\alpha}$,
with mild abuse of notation we identify the function $f\in\mathcal{H}_k$ with
the vector $\boldsymbol{\alpha}$.  Thus we rewrite the loss $L(x, f, y)$ as
$L(x, \boldsymbol{\alpha}, y)$. Then we can rewrite
\begin{dmath*}
    \mathfrak{R}_{\lambda}(\boldsymbol{\alpha}, \seq{s}) =
    %\frac{1}{2N}
    %\sum_{i=1}^N ((\mathbf{K}\boldsymbol{\alpha})_i - y_i)^2 +
    %\frac{\lambda}{2} \inner{\boldsymbol{\alpha},
    %\mathbf{K}\boldsymbol{\alpha}}_2
    \frac{1}{N}
    \sum_{i=1}^NL(x_i, \boldsymbol{\alpha}, y_i) +
    \frac{\lambda}{2} \inner{\boldsymbol{\alpha},
    \mathbf{K}\boldsymbol{\alpha}}_2,
\end{dmath*}
and $f(x_i) = (\mathbf{K}\boldsymbol{\alpha})_i$ for any $x_i$ in the
training set. For instance if we choose $L(x, f,
y)=\frac{1}{2}\abs{f(x)-y}^2$ to be the least square loss, then
\begin{dmath*}
    L(x_i, \boldsymbol{\alpha}, y_i) =
    \frac{1}{2}\abs{(\mathbf{K}\boldsymbol{\alpha})_i-y_i}^2.
\end{dmath*}
In this case $L$ is convex in $\boldsymbol{\alpha}$, thus it is possible to
derive a polynomial time (in $N$) algorithm minimizing $\mathfrak{R}_{\lambda}$
for the least square loss, which is called \emph{kernel Ridge regression}:
\begin{dmath}
    \label{eq:ridge_regression}
    \mathfrak{R}_{\lambda}(\boldsymbol{\alpha}, \seq{s}) =
    \frac{1}{2N}\norm{\mathbf{K}\boldsymbol{\alpha} - (y_i)_{i=1}^N}_2^2 +
    \frac{\lambda}{2} \inner{\boldsymbol{\alpha},
    \mathbf{K}\boldsymbol{\alpha}}_2.
\end{dmath}
As a result of the representer theorem we see that we search a minimizer over
$\boldsymbol{\alpha}\in\mathbb{R}^N$ instead of $f\in\mathcal{H}_k$. By strict
convexity and coercivity of $\mathfrak{R}_{\lambda}$, and because $\mathbf{K} +
\lambda I_N$ is invertible\footnote{Note that although $\mathbf{K} + \lambda
I_N$ is always invertible if $\lambda>0$, choosing a too small value of
$\lambda$ can leads to an ill-conditioned system if the eigenvalues of
$\mathbf{K}+\lambda I_N$ are too small.} for any $\lambda > 0$, a solution is
$\alpha_{\seq{s}} = \argmin_{\boldsymbol{\alpha}\in\mathbb{R}^N}
\mathfrak{R}_{\lambda}(\boldsymbol{\alpha}, \seq{s}) = (\mathbf{K}/N + \lambda
I_N)^{-1}(y_i)_{i=1}^N$. This is an $O\left(N^3\right)$ algorithm.
\paragraph{}
Another way of describing positive definite kernels and \acs{RKHS} consists in
defining a \emph{feature map} $\phi:\mathcal{X}\to\mathcal{H}$ where
$\mathcal{H}$ is a Hilbert space.  Then any function in $\mathcal{H}_k$ can be
written $f(x)=\inner{\phi(x), \theta}_{\mathcal{H}}$ In a nutshell the function
$\phi$ is called feature map because it \say{extracts characteristic elements
from a vector}. Usually a feature map takes a vector in an input space with low
dimension and maps it to a potentially infinite dimensional Hilbert space. Put
it differently, any function in $\mathcal{H}_k$ is the composition of linear
functional $\theta^\adjoint$ with a non linear feature map $\phi$. Thus if the
feature map $\phi$ is fixed (which is equivalent to fixing the kernel), it is
possible to \say{learn} with a linear class of functions $\theta\in\mathcal{H}$
(see \cref{fig:feature_map}).
\begin{figure}
    %\centering\resizebox{\textwidth}{!}{%
    %\begin{tikzpicture}
        %\node[inner sep=0pt] (input) at (0,0)
            %{\includegraphics[width=.35\textwidth]{./gfx/input.eps}};
        %\node[inner sep=0pt] (feature) at (5,-6)
            %{\includegraphics[width=.35\textwidth]{./gfx/feature.eps}};
        %\draw[->,thick] (input.east) -- (feature.west)
            %node[midway,fill=white] {$\phi:\mathcal{X} \to \mathcal{H}$};
    %\end{tikzpicture}}
    \centering
    \begin{tabular}{c}
        \includegraphics[valign=m, width=.5\textheight]{./gfx/input.eps} \\
        $\xdownarrow{1cm} \phi: \enskip \mathcal{X} = \mathbb{R}^2 \to
        \mathcal{H} = \mathbb{R}^3$ \\
        \includegraphics[valign=m, width=.5\textheight]{./gfx/feature.eps}
    \end{tabular}
    \caption[A scalar-valued feature map]{We map the two circles in
    $\mathbb{R}^2$ to $\mathbb{R}^3$. In $\mathbb{R}^3$ it is now possible to
    separate the circles with a linear functional: a plane. We used the feature
    map \\ $\phi(x) = 0.82 \begin{pmatrix} \cos(1.76 x_1 + 2.24 x_2 + 2.75) \\
    \cos(0.40 x_1 + 1.87 x_2 + 5.6) \\ \cos(0.98 x_1 - 0.98 x_2 + 6.05)
    \end{pmatrix}$. \\
    Here $\phi:\mathbb{R}^2\to\mathbb{R}^3$ has been chosen
    as a realization of an \acs{RFF} map (see \cref{eq:rff2}). A \say{cleaner}
    feature map adapted to this problem could have been \\
    $\phi(x)=\begin{pmatrix} x_1 \\ x_2 \\ x_1^2 + x_2^2 \end{pmatrix}$.
    \label{fig:feature_map}}
\end{figure}
If we note
\begin{dmath*}
    \boldsymbol{\phi} =
    \begin{pmatrix}
        \phi(x_1) & \dots & \phi(x_N)
    \end{pmatrix}
\end{dmath*}
the \say{matrix} where each column represents the feature map evaluated at the
point $x_i$ with $1 \le i \le N$, the regularized risk minimization with the
least square loss reads
\begin{dmath*}
    \mathfrak{R}_{\lambda}(\theta, \seq{s}) =
    \frac{1}{2N}\norm{\boldsymbol{\phi}^\transpose \theta - (y_i)_{i=1}^N
    }_{2}^2 + \frac{\lambda}{2}\norm{\theta}_2^2.
\end{dmath*}
and if $\lambda > 0$ the unique minimizer is $\theta_{\seq{s}} =
\left(\boldsymbol{\phi}\boldsymbol{\phi}^\transpose/N + \lambda
I_{\mathcal{H}}\right)^{-1}\boldsymbol{\phi}$. This is an
\begin{dmath*}
    O_t\left( \dim(\mathcal{H})^2(N + \dim{\mathcal{H}}) \right).
\end{dmath*}
time complexity algorithm.  This algorithm seems more appealing than its kernel
counterpart when many data are given since once the space $\mathcal{H}$ has
been fixed, the algorithm is linear in the number of training points. However
many questions remains. First although it is possible to design a feature map
\emph{ex nihilo}, can we design systematically a feature map from a kernel? For
some kernels (\acs{eg} the Gaussian kernel) it is well known that the Hilbert
space corresponding to it has dimension $\dim(\mathcal{H}) = \infty$. Is it
possible to find an approximation of the kernel such that $\dim(\mathcal{H}) <
\infty$? If such a construction is possible and we know that $N$ training data
are available, is it possible to have a sufficiently good approximation
\footnote{When $\dim(\mathcal{H}) \ge N$ then is it is better to use the kernel
algorithm than the feature algorithm. This is called the kernel trick.} with
$\dim(\mathcal{H}) \ll N$?

\subsection{Towards large scale learning with kernels}
Motivated by large scale applications, different methodologies have been
proposed to approximate kernels and feature maps. This subsection briefly
reminds the main approaches based on  Random Fourier Features and Nystr\"om
techniques. Notice that another line of research concerns online learning
method such as \acs{NORMA} developed in \cite{kivinen2004online}, later
extended to the operator-valued kernel case by \citet{audiffren2013online}.  We
start with the seminal work of \citet{Rahimi2007} who show that given a
continuous shift-invariant kernel ($\forall x, z, t \in \mathcal{X}$, $k(x + t,
z + t) = k(x, z)$), it is possible to obtain a feature map called \acs{RFF}
that approximate the given kernel.
\subsubsection{Random Fourier Features map}
The Random Fourier Features methodology introduced  by \citet{Rahimi2007}
provides a way to scale up kernel methods when kernels are Mercer and
\emph{translation-invariant}.  We view the input space $\mathcal{X}$ as a group
endowed with the addition. Extensions to other group laws such as
\citet{li2010random} are described in \cref{subsubsec:skewedchi2} within the
general framework of operator-valued kernels.
\paragraph{}
Denote $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ a positive
definite kernel on $\mathcal{X}=\mathbb{R}^d$. A kernel $k$ is said to be
\emph{shift-invariant} or \emph{translation-invariant} for the addition if for
for all $(x,z,t) \in \left(\mathbb{R}^d\right)^3$ we have $k(x+t,z+t) =
k(x,z)$.  Then, we define $k_0: \mathbb{R}^d \to \mathbb{R}$ the function such
that $k(x,z)= k_0(x-z)$. $k_0$ is called the \emph{signature} of kernel $k$.
Bochner's theorem \citep{folland1994course} is the theoretical result that
leads to the Random Fourier Features.
\begin{theorem}[Bochner's theorem]\label{th:bochner-scalar}
    Any continuous positive definite function is the \acl{FT} of a
    bounded non-negative Borel measure.
\end{theorem}
It implies that any positive definite, continuous and shift-invariant kernel
$k$, has a continuous and positive semi-definite signature $k_0$, which is the
\acl{FT} $\mathcal{F}$ of a non-negative measure $\mu$. Hence we have
$k(x,z)=k_0(x-z) \hiderel{=} \int_{\mathbb{R}^d} \exp\left(-\iu \inner{\omega,x
- z}\right) d\mu(\omega) =\FT{k_0}(\omega)$.  Moreover $\mu = \IFT{k_0}$.
Without loss of generality, we assume that $\mu$ is a probability measure,
\acs{ie} $\int_{\mathbb{R}^d} d\mu(\omega)=1$ by renormalizing the kernel since
\begin{dmath*}
    \int_{\mathbb{R}^d}d\mu(\omega)= \int_{\mathbb{R}^d}\exp\left(-\iu
    \inner{\omega, 0}\right)d\mu(\omega)\hiderel{=}k_0(0).
\end{dmath*}
and we can write the above equation as an expectation over $\mu$. For all
$x$,
$z\in\mathbb{R}^d$
\begin{dmath*}
    k_0(x-z) = \expectation_{\mu}\left[\exp(-\iu \inner{\omega,x - z})\right].
\end{dmath*}
Eventually, if $k$ is real valued we only write the real part,
\begin{dmath*}
    k(x,z) = \expectation_{\mu}[\cos \inner{\omega,x - z}] =
    \expectation_{\mu}[ \cos \inner{\omega,z} \cos \inner{\omega,x} + \sin
    \inner{\omega,z} \sin \inner{\omega,x}].
\end{dmath*}
Let $\Vect_{j=1}^D x_j$ denote the $Dd$-length column
vector obtained by stacking vectors $x_j \in \mathbb{R}^d$.  The feature map
$\widetilde{\phi}: \mathbb{R}^d \rightarrow \mathbb{R}^{2D}$ defined as
\begin{dmath}
\label{eq:rff}
    \widetilde{\phi}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos{\inner{x,\omega_j}} \\
        \sin{\inner{x,\omega_j}}
    \end{pmatrix}\condition{$\omega_j \hiderel{\sim} \IFT{k_0}$ \acs{iid}}
\end{dmath}
is called a \emph{Random Fourier Features} (map). Each $\omega_{j}, j=1, \ldots,
D$ is independently and identically sampled from the inverse Fourier transform
$\mu$ of $k_0$. This Random Fourier Features map provides the following
Monte-Carlo estimator of the kernel: $\widetilde{k}(x, z) =
\widetilde{\phi}(x)^* \widetilde{\phi}(z)$. Using trigonometric identities,
\citet{Rahimi2007} showed that the same feature map can also be written
\begin{dmath}
    \label{eq:rff2}
    \tilde{\phi}(x)=\sqrt{\frac{2}{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos(\inner{x,\omega_j} + b_j)
    \end{pmatrix},
\end{dmath}
where $\omega_j \hiderel{\sim} \IFT{k_0}$, $b_j \sim \mathcal{U}(0, 2\pi)$
\acs{iid}.  The feature map defined by \cref{eq:rff} and \cref{eq:rff2} have
been compared in \citet{sutherland2015} where they give the condition under
wich \cref{eq:rff} has lower variance than \cref{eq:rff2}. For instance for the
Gaussian kernel, \cref{eq:rff} has always lower variance. In practice,
\cref{eq:rff2} is easier to program. In this manuscript we focus on random
Fourier feature of the form given in \cref{eq:rff}.

\paragraph{}
The dimension $D$ governs the precision of this
approximation, whose uniform convergence towards the target kernel (as defined
in \cref{bochner-scalar}) can be found in \citet{Rahimi2007} and in more recent
papers with some refinements proposed in \citet{sutherland2015} and
\citet{sriper2015}.  Finally, it is important to notice that Random Fourier
Features approach \emph{only} requires two steps before the application of a
learning algorithm: (1) define the inverse Fourier transform of the given
shift-invariant kernel, (2) compute the randomized feature map using the
spectral distribution $\mu$.  \citet{Rahimi2007} show that for the Gaussian
kernel $k_0(x-z) = \exp(-\gamma \norm{x - z}_2^2)$, the spectral distribution
$\mu$ is a Gaussian distribution. For the Laplacian kernel $k_0(x-z) =
\exp(-\gamma \norm{x - z}_1)$, the spectral distribution is a Cauchy
distribution.
\paragraph{}
We now focus on another famous way of obtaining feature maps for any scalar
valued kernel called the Nystr\"om method.

\subsubsection{Nystr\"om approximation}
To overcome the bottleneck of Gram matrix computations in kernel methods,
\citet{Williams2000-nystrom} have proposed to generate a low-rank matrix
approximation of the Gram matrix using a subset of its columns.  Since this
feature map is based on a decomposition of the Gram matrix, the feature map
resulting from the Nystr\"om method is data dependent. Let $k: \mathcal{X}^2
\to \mathbb{R}$ be any scalar-valued kernel and let
\begin{dmath*}
    \seq{s} = (x_i)_{i=1}^N
\end{dmath*}
be the training data. We note a subsample of the training data
\begin{dmath*}
    \seq{s}_M = (x_i)_{i=1}^M
\end{dmath*}
where $M \le N$ and $\seq{s}_M$ is a subsequence of $\seq{s}$. Then construct
the Gram matrix $\mathbf{K}_M$ on the subsequence $\seq{s}_M$. Namely
\begin{dmath*}
    \mathbf{K}_M =
    \begin{pmatrix}
        k(x_i, x_j)
    \end{pmatrix}_{i,j=1}^M.
\end{dmath*}
Then perform the singular-valued decomposition $\mathbf{K}_M = U \Lambda
U^\transpose$. The Nystr\"om feature map is given by
\begin{dmath*}
    \tilde{\phi}(x) = \Lambda^{-1/2} U^\transpose\left( \vect_{i=1}^M k(x, x_i)
    \right).
\end{dmath*}
Here $M$ plays the same role as $D$ in the \acs{RFF} case: it controls the
quality of the approximation. Let $\mathbf{K}$ be the full Gram matrix on the
training data $\seq{s}$, let
\begin{dmath*}
    \mathbf{K}_b =
    \begin{pmatrix}
        k(x_i, x_j)
    \end{pmatrix}_{i=1, j=1}^{i=N, j=M}.
\end{dmath*}
Then it is easy to verify that $\boldsymbol{\phi}^\transpose \boldsymbol{\phi} =
\mathbf{K}_b \mathbf{K}_M^\dagger \mathbf{K}_b^\transpose \approx \mathbf{K}$,
where $\mathbf{K}_M^\dagger$ is the pseudo-inverse of $\mathbf{K}_M$ and the
quantity $\mathbf{K}_b \mathbf{K}_M^\dagger \mathbf{K}_b^\transpose$ is a low
rank approximation of the Gram matrix $\mathbf{K}$.
\subsubsection{Random features vs Nystr\"om method}
The main conceptual difference between the Nystr\"om features and the \acl{RFF}
is that the Nystr\"om construction is data dependent, while the \acs{RFF} is
not. The advantage of random Fourier feature lies in their fast construction.
For $N$ data in $\mathbb{R}^d$, it costs $O(NDd)$ to featurize all the data.
For the Nystr\"om features it costs $O\left(M^2(M + d)\right)$. Moreover if one
desires to add a new feature, the \acs{RFF} methodology is as simple as drawing
a new random vector $\omega\sim\IFT{k_0}$, compute $\cos(\inner{\omega, x} +
b)$, where $b\sim \mathcal{U}(0, 2\pi)$ and concatenate it the existing
feature.  For the Nystr\"om features one needs to recompute the singular value
decomposition of the new augmented Gram matrix $\mathbf{K}_{M+1}$.
\paragraph{}
To analyse the \acs{RFF} and Nystr\"om features authors usually study the
approximation error of the approximate Gram matrix and the targer kernel
$\norm{\boldsymbol{\phi}^\transpose\boldsymbol{\phi} - \mathbf{K}}$ (see
\citep{Yang2012, drineas2005nystrom, rosasco2010learning}) or
the supremum of the error between the approximated kernel and the true kernel
over a compact subset $\mathcal{X}$ of the support if $k$: $\sup_{(x, z)
\in\mathcal{C} \subseteq \mathcal{X}^2} \abs{\tilde{\phi}(x)^\transpose
\tilde{\phi}(z) - k(x, z)}$ (see \citet{Rahimi2007, sutherland2015, Bach2015,
rudi2016generalization}).  Because~\citet{bartlett2002rademacher} showed that
for generalization error to be below $\epsilon \in \mathbb{R}_{>0}$ for kernel
methods is $O(N^{-1/2})$, the number of samples $M$ or $D$ required to reach
some approximation error below $\epsilon$ should not grow faster than
$O(M^{-1/2})$ for the Nystr\"om method or $O(D^{-1/2})$ for the \acs{RFF}
method to match kernel learning.  Concerning the Nystr\"om method,
\citet{Yang2012} suggest that the number of samples $M$ is reduced to
$O(M^{-1})$ to reach an error below $\epsilon$ when the gap between the
eigenvalues of $\mathbf{K}$ is large enough. As a result in this specific case,
one should sample $M=O(\sqrt{N})$ Nystr\"om features to ensure good
generalization. On the other hand \citet{rahimi2009weighted} reported that the
generalization performance of \acs{RFF} learning is $O(N^{-1/2} + D^{-1/2})$,
which indicates that $D=O(N)$ features should be sampled to generalize well.
As a result the complexity of learning with the \acs{RFF} seems not to
decrease. However the bounds of \citet{rahimi2009weighted} are suboptimal and
very recently (end of 2016) \citet{rudi2016generalization} proved that in the
case of ridge regression (\cref{eq:ridge_regression}), the generalization error
is $O(N^{-1/2} + D^{-1})$ meaning that $D=O(\sqrt{N})$ random features are
required for good generalization with \acsp{RFF}. We refer the interested
reader to \citet{Yang2012} for an empirical comparison between the Nystr\"om
method and the \acs{RFF} method.

\subsubsection{Extensions of the RFF method}
\paragraph{}
The seminal idea of \citet{Rahimi2007} has opened a large literature on random
features. Nowadays, many classes of kernels other than translation invariant are
now proved to have an efficient random feature representation.
\citet{kar2012random} proposed random feature maps for dot product kernels
(rotation invariant) and \citet{hamid2014compact} improved the rate of
convergence of the approximation error for such kernels by noticing that
feature maps for dot product kernels are usually low rank and may not utilize
the capacity of the projected feature  space  efficiently. \Citet{pham2013fast}
proposed fast random feature maps for polynomial kernels.
\paragraph{}
\Citet{li2010random} generalized the original \acs{RFF} of \citet{Rahimi2007}.
Instead of computing feature maps for shift-in\-va\-riant kernels on the
additive group $(\mathbb{R}^d, +)$, they used the generalized Fourier transform
on any locally compact abelian group to derive random features on the
multiplicative group $(\mathbb{R}^d_{>0}, *)$. In the same spirit
\citet{yang2014random} noticed that an theorem equivalent to Bochner's theorem
exists on the semi-group $(\mathbb{R}_+^d, +)$. From this they derived
\say{Random Laplace} features and used them to approximate kernels adapted to
learn on histograms.
\paragraph{}
To speed-up the convergence rate of the random features approximation,
\citet{yang2014quasi} proposed to sample the random variable from a quasi
Monte-Carlo sequence instead of \acs{iid}~random variables. \Citet{Le2013}
proposed the \say{Fastfood} algorithm to reduce the complexity of computing a
\acs{RFF} --using structured matrices and a fast Walsh-Hadarmard transform--
from $O_t(Dd)$ to $O_t(D\log(d))$. More recently \citet{felix2016orthogonal}
proposed also an algorithm \say{SORF} to compute Gaussian \acs{RFF} in
$O_t(D\log(d))$ but with better convergence rates than \say{Fastfood}
\citep{Le2013}.  \Citet{mukuta2016kernel} proposed a data dependent feature
map (comparable to the Nystro\"m method) by estimating the distribution of the
input data, and then finding the eigenfunction decomposition of Mercer's
integral operator associated to the kernel.
\paragraph{}
In the context of large scale learning and deep learning, \citet{lu2014scale}
showed that \acsp{RFF} can achieve performances comparable to deep-learning
methods by combining multiple kernel learning and composition of kernels along
with a scalable parallel implementation. \Citet{dai2014scalable} and
\citet{xie2015scale} combined \acsp{RFF} and stochastic gradient descent to
define an online learning algorithm called \say{Doubly stochastic gradient
descent} adapted to large scale learning. \Citet{yang2015deep} proposed and
studied the idea of replacing the last fully interconnected layer of a deep
convolutional neural network \citep{lecun1995convolutional} by the
\say{Fastfood} implementation of \acsp{RFF}.
\paragraph{}
Eventually \citet{Yang2015} introduced the algorithm \say{\`A la Carte}, based
on \say{Fastfood} which is able to learn the spectral distribution
%<<<<<<< HEAD
%corresponding to a kernel rather than defining it from the kernel. Among the
%large-scale learning methods with kernel, but not using random features we
%mention the algorithm \say{NORMA} by \citet{kivinen2004online} (to be compared
%to the \say{Doubly stochastic gradient descent}) and \say{Pegasos} by
%\citet{shalev2007pegasos} and \say{SMO} by \citet{platt199912} for kernel
%\acs{SVM} learning.
%=======
corresponding to a kernel rather than defining it from the kernel. Very
recently \citet{kawaguchi2017deep} proposed to use semi-random features which
are a tradeoff between the random features based on kernel methods (\acs{eg}
\acsp{RFF}) and the trainable layer in deep learning.
%>>>>>>> 9b5b5ea96fbb45de714e68edd2b96f14a62f3e01
% A la carte
%reported
%performance comparable with deep learn- ing by combining multiple kernel
%learning and the compo- sition of kernels.

%A important subject of research is on how to compute random features
%as fast as possible. \Citet{hamid2014compacti, } proposed feature maps for
%polynomial kernels rank deficient, and there-
%fore may not utilize the capacity of the projected
%feature  space  effectively.

%\section{On large-scale learning}
%\label{sec:on_large-scale_learning}

%\section{History and state of the art of large scale learning with kernels}
%\label{sec:history}

\chapterend
